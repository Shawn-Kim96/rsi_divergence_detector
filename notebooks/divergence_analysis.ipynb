{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSI Divergence Analysis\n",
    "\n",
    "This notebook analyzes how the RSI divergence detector identifies trend waves and calculates TP/SL levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Add project path to system path\n",
    "path_splited = os.path.abspath('.').split('rsi_divergence_detector')[0]\n",
    "PROJECT_PATH = os.path.join(path_splited, 'rsi_divergence_detector')\n",
    "sys.path.append(PROJECT_PATH)\n",
    "\n",
    "# Import project modules\n",
    "from src.data_process.divergence import DivergenceDetector\n",
    "from src.data_process.data_preprocess import DataPreprocess\n",
    "from src.data_process.data_fetcher import DataFetcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "First, let's load the price data and calculate RSI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved training data found. Loading from CSV files...\n",
      "No CSV file found. Please run data_fetcher.py first to download data.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "data/BTC_USDT_5m.csv does not exist. Please fetch the data first.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 3\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m----> 3\u001b[0m     training_data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_pickle\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mPROJECT_PATH\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/data/training_data.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded training data from pickle file\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/final-project-CEzg4y1O-py3.10/lib/python3.10/site-packages/pandas/io/pickle.py:185\u001b[0m, in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    184\u001b[0m excs_to_catch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;167;01mAttributeError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m, \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;66;03m# 1) try standard library Pickle\u001b[39;00m\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;66;03m# 2) try pickle_compat (older pandas version) to handle subclass changes\u001b[39;00m\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;66;03m# 3) try pickle_compat with latin-1 encoding upon a UnicodeDecodeError\u001b[39;00m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# TypeError for Cython complaints about object.__new__ vs Tick.__new__\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/final-project-CEzg4y1O-py3.10/lib/python3.10/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m     handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m handles\u001b[38;5;241m.\u001b[39mappend(handle)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/shawnksh/Documents/personal/rsi_divergence_detector/data/training_data.pickle'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m timeframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m5m\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# We'll focus on 5-minute timeframe for this analysis\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mdata_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m data from CSV\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/personal/rsi_divergence_detector/src/data_process/data_fetcher.py:117\u001b[0m, in \u001b[0;36mDataFetcher.load_data\u001b[0;34m(symbol, timeframe, input_dir)\u001b[0m\n\u001b[1;32m    115\u001b[0m file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(input_dir, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msymbol\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtimeframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(file_path):\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist. Please fetch the data first.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    118\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pd\u001b[38;5;241m.\u001b[39mread_csv(file_path, index_col\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m, parse_dates\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdatetime\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: data/BTC_USDT_5m.csv does not exist. Please fetch the data first."
     ]
    }
   ],
   "source": [
    "# Check if we have saved data\n",
    "try:\n",
    "    training_data = pd.read_pickle(f\"{PROJECT_PATH}/data/training_data.pickle\")\n",
    "    print(\"Loaded training data from pickle file\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No saved training data found. Loading from CSV files...\")\n",
    "    # Load data from CSV files\n",
    "    data_fetcher = DataFetcher()\n",
    "    symbol = 'BTC/USDT'\n",
    "    timeframe = '5m'  # We'll focus on 5-minute timeframe for this analysis\n",
    "    \n",
    "    try:\n",
    "        df = data_fetcher.load_data(symbol, timeframe)\n",
    "        print(f\"Loaded {timeframe} data from CSV\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No CSV file found. Please run data_fetcher.py first to download data.\")\n",
    "        # You can uncomment the following line to fetch data directly, but it might take time\n",
    "        # df = data_fetcher.fetch_ohlcv(symbol, timeframe)\n",
    "        # df.to_csv(f\"{PROJECT_PATH}/data/{symbol.replace('/', '_')}_{timeframe}.csv\")\n",
    "        raise\n",
    "    \n",
    "    # Preprocess data\n",
    "    data_preprocessor = DataPreprocess()\n",
    "    df = data_preprocessor.calculate_rsi(df)\n",
    "    df['timeframe'] = timeframe\n",
    "    training_data = df\n",
    "\n",
    "# Filter to a specific timeframe for analysis\n",
    "timeframe = '5m'\n",
    "df = training_data[training_data['timeframe'] == timeframe].copy()\n",
    "\n",
    "# Display the first few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Divergence Detection\n",
    "\n",
    "Let's create a function to visualize how the divergence detector identifies trend waves and calculates TP/SL levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_divergence(df, start_date=None, end_date=None, max_lookback_bars=50):\n",
    "    \"\"\"Visualize divergence detection and TP/SL calculation.\"\"\"\n",
    "    # Filter data by date range if provided\n",
    "    if start_date and end_date:\n",
    "        df_subset = df[(df.index >= start_date) & (df.index <= end_date)].copy()\n",
    "    else:\n",
    "        # Use the last 200 bars if no date range is provided\n",
    "        df_subset = df.iloc[-200:].copy()\n",
    "    \n",
    "    # Create divergence detector\n",
    "    detector = DivergenceDetector()\n",
    "    \n",
    "    # Find divergences\n",
    "    divergence_df = detector.find_divergences(\n",
    "        df_subset, \n",
    "        bullish_rsi_threshold=35, \n",
    "        bearish_rsi_threshold=65,\n",
    "        min_bars_lookback=5,\n",
    "        max_bars_lookback=180\n",
    "    )\n",
    "    \n",
    "    if divergence_df.empty:\n",
    "        print(\"No divergences found in the selected time range.\")\n",
    "        return\n",
    "    \n",
    "    # Create figure with subplots\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), gridspec_kw={'height_ratios': [3, 1]})\n",
    "    \n",
    "    # Plot price\n",
    "    ax1.plot(df_subset.index, df_subset['close'], label='Close Price', color='black', alpha=0.7)\n",
    "    \n",
    "    # Plot RSI\n",
    "    ax2.plot(df_subset.index, df_subset['rsi'], label='RSI', color='blue')\n",
    "    ax2.axhline(y=30, color='green', linestyle='--', alpha=0.5)\n",
    "    ax2.axhline(y=70, color='red', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Format x-axis\n",
    "    ax1.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d %H:%M'))\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Analyze each divergence\n",
    "    for i, (start_dt, row) in enumerate(divergence_df.iterrows()):\n",
    "        end_dt = row['end_datetime']\n",
    "        divergence_type = row['divergence']\n",
    "        is_bullish = divergence_type == 'Bullish Divergence'\n",
    "        color = 'green' if is_bullish else 'red'\n",
    "        \n",
    "        # Find previous peak/valley with both old and new methods\n",
    "        prev_peak_old = detector.find_previous_peak(\n",
    "            df_subset, start_dt, is_bullish, \n",
    "            bullish_peak_rsi_threshold=55, \n",
    "            bearish_peak_rsi_threshold=45\n",
    "        )\n",
    "        \n",
    "        prev_peak_new = detector.find_previous_peak(\n",
    "            df_subset, start_dt, is_bullish, \n",
    "            bullish_peak_rsi_threshold=55, \n",
    "            bearish_peak_rsi_threshold=45,\n",
    "            max_lookback_bars=max_lookback_bars\n",
    "        )\n",
    "        \n",
    "        if prev_peak_old is None or prev_peak_new is None:\n",
    "            continue\n",
    "        \n",
    "        # Calculate TP/SL with old method\n",
    "        tp_old, sl_old = detector.calculate_tp_sl(prev_peak_old, end_dt, df_subset, is_bullish)\n",
    "        \n",
    "        # Calculate TP/SL with new method (using the modified function)\n",
    "        tp_new, sl_new = detector.calculate_tp_sl(prev_peak_new, end_dt, df_subset, is_bullish)\n",
    "        \n",
    "        # Plot divergence points\n",
    "        ax1.scatter(start_dt, df_subset.loc[start_dt, 'close'], color=color, marker='o', s=100)\n",
    "        ax1.scatter(end_dt, df_subset.loc[end_dt, 'close'], color=color, marker='o', s=100)\n",
    "        ax1.plot([start_dt, end_dt], [df_subset.loc[start_dt, 'close'], df_subset.loc[end_dt, 'close']], \n",
    "                 color=color, linestyle='--')\n",
    "        \n",
    "        # Plot RSI divergence\n",
    "        ax2.scatter(start_dt, df_subset.loc[start_dt, 'rsi'], color=color, marker='o', s=100)\n",
    "        ax2.scatter(end_dt, df_subset.loc[end_dt, 'rsi'], color=color, marker='o', s=100)\n",
    "        ax2.plot([start_dt, end_dt], [df_subset.loc[start_dt, 'rsi'], df_subset.loc[end_dt, 'rsi']], \n",
    "                 color=color, linestyle='--')\n",
    "        \n",
    "        # Plot previous peak/valley\n",
    "        ax1.scatter(prev_peak_old, df_subset.loc[prev_peak_old, 'close'], color='purple', marker='*', s=150)\n",
    "        ax1.scatter(prev_peak_new, df_subset.loc[prev_peak_new, 'close'], color='orange', marker='*', s=150)\n",
    "        \n",
    "        # Plot trend wave\n",
    "        if is_bullish:\n",
    "            ax1.plot([prev_peak_old, end_dt], \n",
    "                     [df_subset.loc[prev_peak_old, 'high'], df_subset.loc[end_dt, 'low']], \n",
    "                     color='purple', linestyle='-', alpha=0.5)\n",
    "            ax1.plot([prev_peak_new, end_dt], \n",
    "                     [df_subset.loc[prev_peak_new, 'high'], df_subset.loc[end_dt, 'low']], \n",
    "                     color='orange', linestyle='-', alpha=0.5)\n",
    "        else:\n",
    "            ax1.plot([prev_peak_old, end_dt], \n",
    "                     [df_subset.loc[prev_peak_old, 'low'], df_subset.loc[end_dt, 'high']], \n",
    "                     color='purple', linestyle='-', alpha=0.5)\n",
    "            ax1.plot([prev_peak_new, end_dt], \n",
    "                     [df_subset.loc[prev_peak_new, 'low'], df_subset.loc[end_dt, 'high']], \n",
    "                     color='orange', linestyle='-', alpha=0.5)\n",
    "        \n",
    "        # Plot TP/SL levels\n",
    "        entry_dt = row['entry_datetime']\n",
    "        ax1.axhline(y=tp_old, color='purple', linestyle='-.', alpha=0.7)\n",
    "        ax1.axhline(y=sl_old, color='purple', linestyle=':', alpha=0.7)\n",
    "        ax1.axhline(y=tp_new, color='orange', linestyle='-.', alpha=0.7)\n",
    "        ax1.axhline(y=sl_new, color='orange', linestyle=':', alpha=0.7)\n",
    "        \n",
    "        # Add annotations\n",
    "        ax1.annotate(f\"Old TP: {tp_old:.2f}\", xy=(entry_dt, tp_old), xytext=(10, 10), \n",
    "                    textcoords='offset points', color='purple')\n",
    "        ax1.annotate(f\"Old SL: {sl_old:.2f}\", xy=(entry_dt, sl_old), xytext=(10, -20), \n",
    "                    textcoords='offset points', color='purple')\n",
    "        ax1.annotate(f\"New TP: {tp_new:.2f}\", xy=(entry_dt, tp_new), xytext=(10, 30), \n",
    "                    textcoords='offset points', color='orange')\n",
    "        ax1.annotate(f\"New SL: {sl_new:.2f}\", xy=(entry_dt, sl_new), xytext=(10, -40), \n",
    "                    textcoords='offset points', color='orange')\n",
    "        \n",
    "        # Print analysis\n",
    "        print(f\"Divergence {i+1}: {divergence_type}\")\n",
    "        print(f\"  Start: {start_dt}, End: {end_dt}, Entry: {entry_dt}\")\n",
    "        print(f\"  Old method - Previous peak: {prev_peak_old}, TP: {tp_old:.2f}, SL: {sl_old:.2f}\")\n",
    "        print(f\"  New method - Previous peak: {prev_peak_new}, TP: {tp_new:.2f}, SL: {sl_new:.2f}\")\n",
    "        print(f\"  Old TP/SL ratio: {abs(tp_old - row['entry_price']) / abs(sl_old - row['entry_price']):.2f}\")\n",
    "        print(f\"  New TP/SL ratio: {abs(tp_new - row['entry_price']) / abs(sl_new - row['entry_price']):.2f}\")\n",
    "        print()\n",
    "    \n",
    "    # Set labels and title\n",
    "    ax1.set_title('Price Chart with Divergence Analysis')\n",
    "    ax1.set_ylabel('Price')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.set_title('RSI')\n",
    "    ax2.set_ylabel('RSI Value')\n",
    "    ax2.set_ylim(0, 100)\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return divergence_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Recent Divergences\n",
    "\n",
    "Let's analyze some recent divergences to see how the trend wave detection and TP/SL calculation work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the most recent data\n",
    "# You can adjust the date range to focus on specific periods\n",
    "divergence_df = visualize_divergence(df, max_lookback_bars=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Parameters\n",
    "\n",
    "Let's try different parameters to see how they affect the trend wave detection and TP/SL calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with a shorter lookback period\n",
    "divergence_df = visualize_divergence(df, max_lookback_bars=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try with a longer lookback period\n",
    "divergence_df = visualize_divergence(df, max_lookback_bars=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze Specific Time Periods\n",
    "\n",
    "You can also analyze specific time periods where you know there are interesting divergences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Analyze a specific time period\n",
    "# Replace these dates with periods you're interested in\n",
    "start_date = '2023-01-01'\n",
    "end_date = '2023-01-15'\n",
    "\n",
    "try:\n",
    "    divergence_df = visualize_divergence(df, start_date=start_date, end_date=end_date, max_lookback_bars=50)\n",
    "except KeyError:\n",
    "    print(f\"No data available for the period {start_date} to {end_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrates how the RSI divergence detector identifies trend waves and calculates TP/SL levels. The key improvements made are:\n",
    "\n",
    "1. **More Conservative Trend Wave Detection**: Limited the lookback period to avoid selecting peaks that are too far back.\n",
    "2. **Improved TP/SL Calculation**: Used more conservative Fibonacci levels for TP and added a small buffer to SL.\n",
    "\n",
    "You can experiment with different parameters to find the optimal settings for your trading strategy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-project-CEzg4y1O-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

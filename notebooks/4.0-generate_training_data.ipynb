{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "PROJECT_PATH = \"/Users/shawn/Documents/personal/rsi_divergence_detector\"\n",
    "sys.path.append(PROJECT_PATH)\n",
    "# Load the training data\n",
    "# Replace 'training_data.csv' with your actual data file or DataFrame\n",
    "df = pd.read_pickle(f'{PROJECT_PATH}/data/processed_data/training_data.pickle')\n",
    "divergence_data = pd.read_pickle(f\"{PROJECT_PATH}/data/processed_data/divergence_data.pickle\")\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test dataframes sliced\n",
    "df_5m = df[df.timeframe == '5m']\n",
    "df_5m_filter = df_5m.loc[df_5m.index > '2024-11-20 00:30:00']\n",
    "# df_5m_filter = df_5m_filter.loc[df_5m_filter.index < '2024-11-22 20:00:00']\n",
    "\n",
    "df_total = df_5m_filter\n",
    "df_divergence = divergence_data['15m']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>volume</th>\n",
       "      <th>timeframe</th>\n",
       "      <th>rsi</th>\n",
       "      <th>macd</th>\n",
       "      <th>macd_signal</th>\n",
       "      <th>...</th>\n",
       "      <th>willr</th>\n",
       "      <th>cci</th>\n",
       "      <th>atr</th>\n",
       "      <th>return_1</th>\n",
       "      <th>return_5</th>\n",
       "      <th>return_10</th>\n",
       "      <th>volatility_5</th>\n",
       "      <th>volatility_10</th>\n",
       "      <th>volume_change</th>\n",
       "      <th>volume_rolling_mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2024-11-20 00:35:00</th>\n",
       "      <td>1732062900000</td>\n",
       "      <td>92080.35</td>\n",
       "      <td>92260.0</td>\n",
       "      <td>92050.53</td>\n",
       "      <td>92260.00</td>\n",
       "      <td>50.34613</td>\n",
       "      <td>5m</td>\n",
       "      <td>54.149277</td>\n",
       "      <td>9.696354</td>\n",
       "      <td>-7.417105</td>\n",
       "      <td>...</td>\n",
       "      <td>-37.900067</td>\n",
       "      <td>13.364110</td>\n",
       "      <td>194.759931</td>\n",
       "      <td>0.001951</td>\n",
       "      <td>0.000873</td>\n",
       "      <td>0.001121</td>\n",
       "      <td>0.001268</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>-0.287841</td>\n",
       "      <td>58.752716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-20 00:40:00</th>\n",
       "      <td>1732063200000</td>\n",
       "      <td>92259.99</td>\n",
       "      <td>92320.0</td>\n",
       "      <td>92233.69</td>\n",
       "      <td>92233.70</td>\n",
       "      <td>69.42001</td>\n",
       "      <td>5m</td>\n",
       "      <td>53.172138</td>\n",
       "      <td>15.807042</td>\n",
       "      <td>-2.772276</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.181714</td>\n",
       "      <td>87.280919</td>\n",
       "      <td>187.013507</td>\n",
       "      <td>-0.000285</td>\n",
       "      <td>0.001365</td>\n",
       "      <td>0.001378</td>\n",
       "      <td>0.001193</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>0.378855</td>\n",
       "      <td>62.958378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-11-20 00:45:00</th>\n",
       "      <td>1732063500000</td>\n",
       "      <td>92233.69</td>\n",
       "      <td>92260.0</td>\n",
       "      <td>92124.51</td>\n",
       "      <td>92130.93</td>\n",
       "      <td>55.29224</td>\n",
       "      <td>5m</td>\n",
       "      <td>49.419335</td>\n",
       "      <td>12.216304</td>\n",
       "      <td>0.225440</td>\n",
       "      <td>...</td>\n",
       "      <td>-68.727907</td>\n",
       "      <td>-21.628155</td>\n",
       "      <td>183.333257</td>\n",
       "      <td>-0.001114</td>\n",
       "      <td>-0.000337</td>\n",
       "      <td>-0.001948</td>\n",
       "      <td>0.001317</td>\n",
       "      <td>0.001361</td>\n",
       "      <td>-0.203511</td>\n",
       "      <td>62.781558</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         timestamp      open     high       low     close  \\\n",
       "datetime                                                                    \n",
       "2024-11-20 00:35:00  1732062900000  92080.35  92260.0  92050.53  92260.00   \n",
       "2024-11-20 00:40:00  1732063200000  92259.99  92320.0  92233.69  92233.70   \n",
       "2024-11-20 00:45:00  1732063500000  92233.69  92260.0  92124.51  92130.93   \n",
       "\n",
       "                       volume timeframe        rsi       macd  macd_signal  \\\n",
       "datetime                                                                     \n",
       "2024-11-20 00:35:00  50.34613        5m  54.149277   9.696354    -7.417105   \n",
       "2024-11-20 00:40:00  69.42001        5m  53.172138  15.807042    -2.772276   \n",
       "2024-11-20 00:45:00  55.29224        5m  49.419335  12.216304     0.225440   \n",
       "\n",
       "                     ...      willr        cci         atr  return_1  \\\n",
       "datetime             ...                                               \n",
       "2024-11-20 00:35:00  ... -37.900067  13.364110  194.759931  0.001951   \n",
       "2024-11-20 00:40:00  ... -44.181714  87.280919  187.013507 -0.000285   \n",
       "2024-11-20 00:45:00  ... -68.727907 -21.628155  183.333257 -0.001114   \n",
       "\n",
       "                     return_5  return_10  volatility_5  volatility_10  \\\n",
       "datetime                                                                \n",
       "2024-11-20 00:35:00  0.000873   0.001121      0.001268       0.001520   \n",
       "2024-11-20 00:40:00  0.001365   0.001378      0.001193       0.001510   \n",
       "2024-11-20 00:45:00 -0.000337  -0.001948      0.001317       0.001361   \n",
       "\n",
       "                     volume_change  volume_rolling_mean  \n",
       "datetime                                                 \n",
       "2024-11-20 00:35:00      -0.287841            58.752716  \n",
       "2024-11-20 00:40:00       0.378855            62.958378  \n",
       "2024-11-20 00:45:00      -0.203511            62.781558  \n",
       "\n",
       "[3 rows x 27 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end_datetime</th>\n",
       "      <th>entry_datetime</th>\n",
       "      <th>entry_price</th>\n",
       "      <th>previous_peak_datetime</th>\n",
       "      <th>divergence</th>\n",
       "      <th>price_change</th>\n",
       "      <th>rsi_change</th>\n",
       "      <th>TP</th>\n",
       "      <th>SL</th>\n",
       "      <th>label</th>\n",
       "      <th>...</th>\n",
       "      <th>SL_percent</th>\n",
       "      <th>position</th>\n",
       "      <th>TP_vs_SL</th>\n",
       "      <th>TP_/_SL</th>\n",
       "      <th>profit</th>\n",
       "      <th>div_5m</th>\n",
       "      <th>div_15m</th>\n",
       "      <th>div_1h</th>\n",
       "      <th>div_4h</th>\n",
       "      <th>div_1d</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start_datetime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2017-08-17 16:15:00</th>\n",
       "      <td>2017-08-17 18:00:00</td>\n",
       "      <td>2017-08-17 18:30:00</td>\n",
       "      <td>4287.69</td>\n",
       "      <td>2017-08-17 14:30:00</td>\n",
       "      <td>Bullish Divergence</td>\n",
       "      <td>5.18</td>\n",
       "      <td>6.47</td>\n",
       "      <td>4320.56</td>\n",
       "      <td>4218.68</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>1.61</td>\n",
       "      <td>False</td>\n",
       "      <td>0.476354</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-69.01</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 18:30:00</th>\n",
       "      <td>2017-08-18 01:15:00</td>\n",
       "      <td>2017-08-18 01:45:00</td>\n",
       "      <td>4231.61</td>\n",
       "      <td>2017-08-17 14:30:00</td>\n",
       "      <td>Bullish Divergence</td>\n",
       "      <td>37.98</td>\n",
       "      <td>10.29</td>\n",
       "      <td>4268.61</td>\n",
       "      <td>4134.61</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.29</td>\n",
       "      <td>False</td>\n",
       "      <td>0.381422</td>\n",
       "      <td>0.38</td>\n",
       "      <td>37.00</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-17 18:30:00</th>\n",
       "      <td>2017-08-18 15:15:00</td>\n",
       "      <td>2017-08-18 15:45:00</td>\n",
       "      <td>4248.00</td>\n",
       "      <td>2017-08-17 14:30:00</td>\n",
       "      <td>Bullish Divergence</td>\n",
       "      <td>36.84</td>\n",
       "      <td>13.37</td>\n",
       "      <td>4314.89</td>\n",
       "      <td>4209.50</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.91</td>\n",
       "      <td>False</td>\n",
       "      <td>1.737402</td>\n",
       "      <td>1.74</td>\n",
       "      <td>-38.50</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-18 19:00:00</th>\n",
       "      <td>2017-08-18 20:15:00</td>\n",
       "      <td>2017-08-18 20:45:00</td>\n",
       "      <td>4015.40</td>\n",
       "      <td>2017-08-18 10:30:00</td>\n",
       "      <td>Bullish Divergence</td>\n",
       "      <td>-34.90</td>\n",
       "      <td>-1.32</td>\n",
       "      <td>4148.66</td>\n",
       "      <td>4010.90</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.11</td>\n",
       "      <td>True</td>\n",
       "      <td>29.612631</td>\n",
       "      <td>29.61</td>\n",
       "      <td>-4.50</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017-08-18 19:00:00</th>\n",
       "      <td>2017-08-18 21:45:00</td>\n",
       "      <td>2017-08-18 22:15:00</td>\n",
       "      <td>4061.00</td>\n",
       "      <td>2017-08-18 10:30:00</td>\n",
       "      <td>Bullish Divergence</td>\n",
       "      <td>125.71</td>\n",
       "      <td>21.95</td>\n",
       "      <td>4121.34</td>\n",
       "      <td>3966.69</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.32</td>\n",
       "      <td>False</td>\n",
       "      <td>0.639753</td>\n",
       "      <td>0.64</td>\n",
       "      <td>60.34</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-04 20:30:00</th>\n",
       "      <td>2024-12-06 14:15:00</td>\n",
       "      <td>2024-12-06 14:45:00</td>\n",
       "      <td>98827.76</td>\n",
       "      <td>2024-12-04 16:45:00</td>\n",
       "      <td>Bearish Divergence</td>\n",
       "      <td>-78.12</td>\n",
       "      <td>-7.39</td>\n",
       "      <td>97495.01</td>\n",
       "      <td>99292.00</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.47</td>\n",
       "      <td>False</td>\n",
       "      <td>2.870827</td>\n",
       "      <td>2.87</td>\n",
       "      <td>-464.24</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-04 20:30:00</th>\n",
       "      <td>2024-12-06 14:45:00</td>\n",
       "      <td>2024-12-06 15:15:00</td>\n",
       "      <td>98832.73</td>\n",
       "      <td>2024-12-04 16:45:00</td>\n",
       "      <td>Bearish Divergence</td>\n",
       "      <td>-145.69</td>\n",
       "      <td>-4.56</td>\n",
       "      <td>97613.55</td>\n",
       "      <td>99483.81</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.66</td>\n",
       "      <td>False</td>\n",
       "      <td>1.872557</td>\n",
       "      <td>1.87</td>\n",
       "      <td>-651.08</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-06 18:15:00</th>\n",
       "      <td>2024-12-06 19:30:00</td>\n",
       "      <td>2024-12-06 20:00:00</td>\n",
       "      <td>101439.99</td>\n",
       "      <td>2024-12-06 12:45:00</td>\n",
       "      <td>Bearish Divergence</td>\n",
       "      <td>-308.25</td>\n",
       "      <td>-9.06</td>\n",
       "      <td>100100.28</td>\n",
       "      <td>101700.00</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>0.26</td>\n",
       "      <td>True</td>\n",
       "      <td>5.152520</td>\n",
       "      <td>5.15</td>\n",
       "      <td>-260.01</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-06 18:15:00</th>\n",
       "      <td>2024-12-06 20:30:00</td>\n",
       "      <td>2024-12-06 21:00:00</td>\n",
       "      <td>101360.00</td>\n",
       "      <td>2024-12-06 12:45:00</td>\n",
       "      <td>Bearish Divergence</td>\n",
       "      <td>-1459.08</td>\n",
       "      <td>-22.72</td>\n",
       "      <td>100223.26</td>\n",
       "      <td>101898.99</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.53</td>\n",
       "      <td>False</td>\n",
       "      <td>2.109020</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1136.74</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-12-07 19:30:00</th>\n",
       "      <td>2024-12-07 20:45:00</td>\n",
       "      <td>2024-12-07 21:15:00</td>\n",
       "      <td>100256.00</td>\n",
       "      <td>2024-12-07 14:15:00</td>\n",
       "      <td>Bearish Divergence</td>\n",
       "      <td>-285.47</td>\n",
       "      <td>-14.41</td>\n",
       "      <td>99831.70</td>\n",
       "      <td>100323.53</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.07</td>\n",
       "      <td>True</td>\n",
       "      <td>6.283173</td>\n",
       "      <td>6.28</td>\n",
       "      <td>424.30</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8303 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           end_datetime      entry_datetime  entry_price  \\\n",
       "start_datetime                                                             \n",
       "2017-08-17 16:15:00 2017-08-17 18:00:00 2017-08-17 18:30:00      4287.69   \n",
       "2017-08-17 18:30:00 2017-08-18 01:15:00 2017-08-18 01:45:00      4231.61   \n",
       "2017-08-17 18:30:00 2017-08-18 15:15:00 2017-08-18 15:45:00      4248.00   \n",
       "2017-08-18 19:00:00 2017-08-18 20:15:00 2017-08-18 20:45:00      4015.40   \n",
       "2017-08-18 19:00:00 2017-08-18 21:45:00 2017-08-18 22:15:00      4061.00   \n",
       "...                                 ...                 ...          ...   \n",
       "2024-12-04 20:30:00 2024-12-06 14:15:00 2024-12-06 14:45:00     98827.76   \n",
       "2024-12-04 20:30:00 2024-12-06 14:45:00 2024-12-06 15:15:00     98832.73   \n",
       "2024-12-06 18:15:00 2024-12-06 19:30:00 2024-12-06 20:00:00    101439.99   \n",
       "2024-12-06 18:15:00 2024-12-06 20:30:00 2024-12-06 21:00:00    101360.00   \n",
       "2024-12-07 19:30:00 2024-12-07 20:45:00 2024-12-07 21:15:00    100256.00   \n",
       "\n",
       "                    previous_peak_datetime          divergence  price_change  \\\n",
       "start_datetime                                                                 \n",
       "2017-08-17 16:15:00    2017-08-17 14:30:00  Bullish Divergence          5.18   \n",
       "2017-08-17 18:30:00    2017-08-17 14:30:00  Bullish Divergence         37.98   \n",
       "2017-08-17 18:30:00    2017-08-17 14:30:00  Bullish Divergence         36.84   \n",
       "2017-08-18 19:00:00    2017-08-18 10:30:00  Bullish Divergence        -34.90   \n",
       "2017-08-18 19:00:00    2017-08-18 10:30:00  Bullish Divergence        125.71   \n",
       "...                                    ...                 ...           ...   \n",
       "2024-12-04 20:30:00    2024-12-04 16:45:00  Bearish Divergence        -78.12   \n",
       "2024-12-04 20:30:00    2024-12-04 16:45:00  Bearish Divergence       -145.69   \n",
       "2024-12-06 18:15:00    2024-12-06 12:45:00  Bearish Divergence       -308.25   \n",
       "2024-12-06 18:15:00    2024-12-06 12:45:00  Bearish Divergence      -1459.08   \n",
       "2024-12-07 19:30:00    2024-12-07 14:15:00  Bearish Divergence       -285.47   \n",
       "\n",
       "                     rsi_change         TP         SL  label  ...  SL_percent  \\\n",
       "start_datetime                                                ...               \n",
       "2017-08-17 16:15:00        6.47    4320.56    4218.68  False  ...        1.61   \n",
       "2017-08-17 18:30:00       10.29    4268.61    4134.61   True  ...        2.29   \n",
       "2017-08-17 18:30:00       13.37    4314.89    4209.50  False  ...        0.91   \n",
       "2017-08-18 19:00:00       -1.32    4148.66    4010.90  False  ...        0.11   \n",
       "2017-08-18 19:00:00       21.95    4121.34    3966.69   True  ...        2.32   \n",
       "...                         ...        ...        ...    ...  ...         ...   \n",
       "2024-12-04 20:30:00       -7.39   97495.01   99292.00  False  ...        0.47   \n",
       "2024-12-04 20:30:00       -4.56   97613.55   99483.81  False  ...        0.66   \n",
       "2024-12-06 18:15:00       -9.06  100100.28  101700.00  False  ...        0.26   \n",
       "2024-12-06 18:15:00      -22.72  100223.26  101898.99   True  ...        0.53   \n",
       "2024-12-07 19:30:00      -14.41   99831.70  100323.53   True  ...        0.07   \n",
       "\n",
       "                     position   TP_vs_SL  TP_/_SL   profit  div_5m  div_15m  \\\n",
       "start_datetime                                                                \n",
       "2017-08-17 16:15:00     False   0.476354     0.48   -69.01   False     True   \n",
       "2017-08-17 18:30:00     False   0.381422     0.38    37.00   False     True   \n",
       "2017-08-17 18:30:00     False   1.737402     1.74   -38.50   False     True   \n",
       "2017-08-18 19:00:00      True  29.612631    29.61    -4.50    True     True   \n",
       "2017-08-18 19:00:00     False   0.639753     0.64    60.34    True     True   \n",
       "...                       ...        ...      ...      ...     ...      ...   \n",
       "2024-12-04 20:30:00     False   2.870827     2.87  -464.24   False     True   \n",
       "2024-12-04 20:30:00     False   1.872557     1.87  -651.08   False     True   \n",
       "2024-12-06 18:15:00      True   5.152520     5.15  -260.01   False     True   \n",
       "2024-12-06 18:15:00     False   2.109020     2.11  1136.74   False     True   \n",
       "2024-12-07 19:30:00      True   6.283173     6.28   424.30   False     True   \n",
       "\n",
       "                     div_1h  div_4h  div_1d  \n",
       "start_datetime                               \n",
       "2017-08-17 16:15:00   False   False   False  \n",
       "2017-08-17 18:30:00   False   False   False  \n",
       "2017-08-17 18:30:00   False   False   False  \n",
       "2017-08-18 19:00:00   False   False   False  \n",
       "2017-08-18 19:00:00   False   False   False  \n",
       "...                     ...     ...     ...  \n",
       "2024-12-04 20:30:00   False   False   False  \n",
       "2024-12-04 20:30:00   False   False   False  \n",
       "2024-12-06 18:15:00   False   False   False  \n",
       "2024-12-06 18:15:00   False   False   False  \n",
       "2024-12-07 19:30:00   False   False   False  \n",
       "\n",
       "[8303 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining divergence data and price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_divergence_sequence(df5, divergence_data):\n",
    "    \"\"\"\n",
    "    Create a divergence feature sequence aligned with df5 timestamps for multiple timeframes.\n",
    "    divergence_data: { '5m': ddf_5m, '1h': ddf_1h, ... }\n",
    "    Each ddf_x: has divergence info with timestamps. For each timestamp in df5, \n",
    "    we check if there's a bullish divergence (1), bearish divergence (-1), else 0.\n",
    "    \n",
    "    This is just an example. You need to adapt it based on how you store divergence info.\n",
    "    \"\"\"\n",
    "    # Initialize a dict of arrays, one per timeframe\n",
    "    divergence_arrays = {}\n",
    "    for tf, ddf in divergence_data.items():\n",
    "        # Assume ddf indexed by datetime with a column 'divergence' that can be 'Bullish' or 'Bearish' or None\n",
    "        # We'll create an array aligned with df5 index\n",
    "        arr = np.zeros(len(df5), dtype=np.float32)\n",
    "        # For each event in ddf, mark the corresponding timestamps\n",
    "        # This is a simplistic approach: if end_datetime in ddf matches a df5 timestamp,\n",
    "        # set arr[idx] = 1 for bullish, -1 for bearish.\n",
    "        # You may need a more sophisticated approach depending on your data.\n",
    "        for start_dt, event in ddf.iterrows():\n",
    "            end_dt = event['end_datetime']\n",
    "        \n",
    "            for time_delta in range(0, int((end_dt - start_dt).total_seconds()) + 1, 5*60):\n",
    "                t = start_dt + pd.Timedelta(seconds=time_delta)\n",
    "                if t in df5.index:\n",
    "                    idx = df5.index.get_loc(t)\n",
    "                    if event['divergence'] == 'Bullish Divergence':\n",
    "                        arr[idx] = 1\n",
    "                    elif event['divergence'] == 'Bearish Divergence':\n",
    "                        arr[idx] = -1\n",
    "        \n",
    "        divergence_arrays[tf] = arr\n",
    "    \n",
    "    # Combine into a single DataFrame (or array)\n",
    "    # For simplicity, just stack them column-wise\n",
    "    divergence_df = pd.DataFrame(index=df5.index)\n",
    "    for tf, arr in divergence_arrays.items():\n",
    "        df5[f\"div_{tf}\"] = arr\n",
    "    \n",
    "    return df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81.59271025657654 taken.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ns/shg9ht0545sbbr41sk4q_zgr0000gn/T/ipykernel_44174/1955272509.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df5[f\"div_{tf}\"] = arr\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "t = time.time()\n",
    "df_revised = create_divergence_sequence(df_5m, divergence_data)\n",
    "print(f\"{time.time() - t} taken.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "div_5m\n",
       " 0.0    320334\n",
       "-1.0    233313\n",
       " 1.0    213451\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_revised.div_5m.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.lstm_dataset import LSTMDivergenceDataset, create_divergence_sequence\n",
    "from src.model.lstm_mixed import MixedLSTMModel\n",
    "from src.training.train_lstm import train_model, evaluate_model, plot_results, model_naming\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(PROJECT_PATH, 'data', 'processed_data', 'training_data.pickle'))\n",
    "divergence_data = pd.read_pickle(os.path.join(PROJECT_PATH, 'data', 'processed_data', 'divergence_data.pickle'))\n",
    "\n",
    "# Filter 5-minute timeframe data\n",
    "# price_df = df[df['timeframe'] == '5m'].copy()\n",
    "price_df = pd.read_pickle(f\"{PROJECT_PATH}/data/training_data/price_df.pickle\")\n",
    "divergence_df = divergence_data['15m'].copy()  # Assuming '5m' key exists\n",
    "\n",
    "# Split divergence_df into train/val/test\n",
    "total_events = len(divergence_df)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_count = int(total_events * train_ratio)\n",
    "val_count = int(total_events * val_ratio)\n",
    "test_count = total_events - train_count - val_count\n",
    "\n",
    "divergence_df_train = df_divergence.iloc[:train_count]\n",
    "divergence_df_val = df_divergence.iloc[train_count:train_count+val_count]\n",
    "divergence_df_test = df_divergence.iloc[train_count+val_count:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 23:11:37,710 INFO Train events: 5812, Validation events: 1660, Test events: 831\n",
      "2024-12-11 23:11:37,711 INFO Initializing datasets...\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logger.info(f\"Train events: {len(divergence_df_train)}, \"\n",
    "            f\"Validation events: {len(divergence_df_val)}, \"\n",
    "            f\"Test events: {len(divergence_df_test)}\")\n",
    "\n",
    "# Prepare divergence_data for multiple timeframes (if applicable)\n",
    "# Assuming divergence_data contains multiple timeframes\n",
    "# Example: divergence_data = {'5m': ddf_5m, '15m': ddf_15m, '1h': ddf_1h, ...}\n",
    "# For simplicity, using only '5m' here\n",
    "divergence_data_subset = {'15m': divergence_df_train}\n",
    "\n",
    "# Initialize Dataset\n",
    "logger.info(\"Initializing datasets...\")\n",
    "train_dataset = LSTMDivergenceDataset(divergence_df=divergence_df_train, \n",
    "                                        price_df=price_df, \n",
    "                                        divergence_data=divergence_data, \n",
    "                                        seq_length=288)  # 288 * 5min = 24 hours\n",
    "# Use the same scaler for validation and test\n",
    "scaler = train_dataset.scaler\n",
    "val_dataset = LSTMDivergenceDataset(divergence_df=divergence_df_val, \n",
    "                                    price_df=price_df, \n",
    "                                    divergence_data=divergence_data, \n",
    "                                    seq_length=288, \n",
    "                                    scaler=scaler)\n",
    "\n",
    "test_dataset = LSTMDivergenceDataset(divergence_df=divergence_df_test, \n",
    "                                    price_df=price_df, \n",
    "                                    divergence_data=divergence_data, \n",
    "                                    seq_length=288, \n",
    "                                    scaler=scaler)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.to_pickle(train_dataset, f\"{PROJECT_PATH}/data/training_data/train_dataset.pickle\")\n",
    "pd.to_pickle(val_dataset, f\"{PROJECT_PATH}/data/training_data/val_dataset.pickle\")\n",
    "pd.to_pickle(test_dataset, f\"{PROJECT_PATH}/data/training_data/test_dataset.pickle\")\n",
    "\n",
    "pd.to_pickle(train_loader, f\"{PROJECT_PATH}/data/training_data/test_loader.pickle\")\n",
    "pd.to_pickle(val_loader, f\"{PROJECT_PATH}/data/training_data/test_loader.pickle\")\n",
    "pd.to_pickle(test_loader, f\"{PROJECT_PATH}/data/training_data/test_loader.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-11 23:11:39,906 INFO Using device: cpu\n",
      "2024-12-11 23:11:39,909 INFO Model initialized with args: {'seq_input_dim': 29, 'seq_hidden_dim': 128, 'seq_num_layers': 3, 'nonseq_input_dim': 6, 'mlp_hidden_dim': 256, 'num_classes': 2, 'dropout': 0.3}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize Model\n",
    "model_args = {\n",
    "    \"seq_input_dim\": len(train_dataset.ts_cols),\n",
    "    \"seq_hidden_dim\": 128,\n",
    "    \"seq_num_layers\": 3,\n",
    "    \"nonseq_input_dim\": len(train_dataset.nonseq_cols),\n",
    "    \"mlp_hidden_dim\": 256,\n",
    "    \"num_classes\": 2,\n",
    "    \"dropout\": 0.3\n",
    "}\n",
    "model = MixedLSTMModel(**model_args)\n",
    "logger.info(f\"Model initialized with args: {model_args}\")\n",
    "\n",
    "# Generate model name\n",
    "model_name = model_naming(**model_args)\n",
    "model_save_path = os.path.join(PROJECT_PATH, 'model_data', 'mixed_lstm', model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 10:13:07,456 INFO Starting training...\n",
      "2024-12-12 10:13:11,729 INFO Epoch [1/500], Batch [10/90], Loss: 6.7485\n",
      "2024-12-12 10:13:16,305 INFO Epoch [1/500], Batch [20/90], Loss: 1.5336\n",
      "2024-12-12 10:13:20,931 INFO Epoch [1/500], Batch [30/90], Loss: 2.4667\n",
      "2024-12-12 10:13:25,404 INFO Epoch [1/500], Batch [40/90], Loss: 1.4954\n",
      "2024-12-12 10:13:29,928 INFO Epoch [1/500], Batch [50/90], Loss: 0.6979\n",
      "2024-12-12 10:13:34,466 INFO Epoch [1/500], Batch [60/90], Loss: 0.9215\n",
      "2024-12-12 10:13:38,808 INFO Epoch [1/500], Batch [70/90], Loss: 0.6387\n",
      "2024-12-12 10:13:43,508 INFO Epoch [1/500], Batch [80/90], Loss: 0.9307\n",
      "2024-12-12 10:13:48,326 INFO Epoch [1/500], Batch [90/90], Loss: 0.8169\n",
      "2024-12-12 10:13:53,157 INFO Epoch [1/500] - Train Loss: 1.9638, Train Acc: 0.6593 - Val Loss: 0.8004, Val Acc: 0.7036\n",
      "2024-12-12 10:13:53,161 INFO Best model saved with Val Acc: 0.7036\n",
      "2024-12-12 10:13:57,656 INFO Epoch [2/500], Batch [10/90], Loss: 0.6017\n",
      "2024-12-12 10:14:01,870 INFO Epoch [2/500], Batch [20/90], Loss: 0.6079\n",
      "2024-12-12 10:14:06,016 INFO Epoch [2/500], Batch [30/90], Loss: 0.6122\n",
      "2024-12-12 10:14:10,383 INFO Epoch [2/500], Batch [40/90], Loss: 0.5420\n",
      "2024-12-12 10:14:15,034 INFO Epoch [2/500], Batch [50/90], Loss: 0.6205\n",
      "2024-12-12 10:14:19,180 INFO Epoch [2/500], Batch [60/90], Loss: 0.6112\n",
      "2024-12-12 10:14:23,272 INFO Epoch [2/500], Batch [70/90], Loss: 0.6996\n",
      "2024-12-12 10:14:27,416 INFO Epoch [2/500], Batch [80/90], Loss: 0.6389\n",
      "2024-12-12 10:14:31,535 INFO Epoch [2/500], Batch [90/90], Loss: 0.5576\n",
      "2024-12-12 10:14:35,840 INFO Epoch [2/500] - Train Loss: 0.6195, Train Acc: 0.7521 - Val Loss: 0.6120, Val Acc: 0.7036\n",
      "2024-12-12 10:14:40,031 INFO Epoch [3/500], Batch [10/90], Loss: 0.6731\n",
      "2024-12-12 10:14:44,262 INFO Epoch [3/500], Batch [20/90], Loss: 0.5654\n",
      "2024-12-12 10:14:48,544 INFO Epoch [3/500], Batch [30/90], Loss: 0.5746\n",
      "2024-12-12 10:14:52,739 INFO Epoch [3/500], Batch [40/90], Loss: 0.6087\n",
      "2024-12-12 10:14:56,867 INFO Epoch [3/500], Batch [50/90], Loss: 0.6595\n",
      "2024-12-12 10:15:00,920 INFO Epoch [3/500], Batch [60/90], Loss: 0.5351\n",
      "2024-12-12 10:15:05,042 INFO Epoch [3/500], Batch [70/90], Loss: 0.5964\n",
      "2024-12-12 10:15:09,228 INFO Epoch [3/500], Batch [80/90], Loss: 0.5884\n",
      "2024-12-12 10:15:13,586 INFO Epoch [3/500], Batch [90/90], Loss: 0.7749\n",
      "2024-12-12 10:15:18,223 INFO Epoch [3/500] - Train Loss: 0.6082, Train Acc: 0.7503 - Val Loss: 0.6131, Val Acc: 0.7036\n",
      "2024-12-12 10:15:22,339 INFO Epoch [4/500], Batch [10/90], Loss: 0.7053\n",
      "2024-12-12 10:15:26,448 INFO Epoch [4/500], Batch [20/90], Loss: 0.5582\n",
      "2024-12-12 10:15:30,548 INFO Epoch [4/500], Batch [30/90], Loss: 0.6054\n",
      "2024-12-12 10:15:34,641 INFO Epoch [4/500], Batch [40/90], Loss: 0.6666\n",
      "2024-12-12 10:15:38,776 INFO Epoch [4/500], Batch [50/90], Loss: 0.5369\n",
      "2024-12-12 10:15:43,001 INFO Epoch [4/500], Batch [60/90], Loss: 0.5806\n",
      "2024-12-12 10:15:47,495 INFO Epoch [4/500], Batch [70/90], Loss: 0.5852\n",
      "2024-12-12 10:15:52,508 INFO Epoch [4/500], Batch [80/90], Loss: 0.6231\n",
      "2024-12-12 10:15:56,717 INFO Epoch [4/500], Batch [90/90], Loss: 0.5789\n",
      "2024-12-12 10:16:01,475 INFO Epoch [4/500] - Train Loss: 0.5878, Train Acc: 0.7540 - Val Loss: 0.6087, Val Acc: 0.7036\n",
      "2024-12-12 10:16:05,542 INFO Epoch [5/500], Batch [10/90], Loss: 0.5475\n",
      "2024-12-12 10:16:09,584 INFO Epoch [5/500], Batch [20/90], Loss: 0.5254\n",
      "2024-12-12 10:16:13,767 INFO Epoch [5/500], Batch [30/90], Loss: 0.6336\n",
      "2024-12-12 10:16:17,871 INFO Epoch [5/500], Batch [40/90], Loss: 0.5990\n",
      "2024-12-12 10:16:21,938 INFO Epoch [5/500], Batch [50/90], Loss: 0.5975\n",
      "2024-12-12 10:16:26,236 INFO Epoch [5/500], Batch [60/90], Loss: 0.6357\n",
      "2024-12-12 10:16:30,336 INFO Epoch [5/500], Batch [70/90], Loss: 0.5461\n",
      "2024-12-12 10:16:34,448 INFO Epoch [5/500], Batch [80/90], Loss: 0.5408\n",
      "2024-12-12 10:16:38,514 INFO Epoch [5/500], Batch [90/90], Loss: 0.5938\n",
      "2024-12-12 10:16:43,123 INFO Epoch [5/500] - Train Loss: 0.5749, Train Acc: 0.7555 - Val Loss: 0.6050, Val Acc: 0.7036\n",
      "2024-12-12 10:16:47,336 INFO Epoch [6/500], Batch [10/90], Loss: 0.5663\n",
      "2024-12-12 10:16:51,445 INFO Epoch [6/500], Batch [20/90], Loss: 0.5496\n",
      "2024-12-12 10:16:55,387 INFO Epoch [6/500], Batch [30/90], Loss: 0.6006\n",
      "2024-12-12 10:16:59,537 INFO Epoch [6/500], Batch [40/90], Loss: 0.6227\n",
      "2024-12-12 10:17:03,561 INFO Epoch [6/500], Batch [50/90], Loss: 0.5778\n",
      "2024-12-12 10:17:07,526 INFO Epoch [6/500], Batch [60/90], Loss: 0.6276\n",
      "2024-12-12 10:17:11,799 INFO Epoch [6/500], Batch [70/90], Loss: 0.6659\n",
      "2024-12-12 10:17:16,094 INFO Epoch [6/500], Batch [80/90], Loss: 0.6092\n",
      "2024-12-12 10:17:20,196 INFO Epoch [6/500], Batch [90/90], Loss: 0.5431\n",
      "2024-12-12 10:17:24,573 INFO Epoch [6/500] - Train Loss: 0.6153, Train Acc: 0.7534 - Val Loss: 0.6200, Val Acc: 0.7036\n",
      "2024-12-12 10:17:28,658 INFO Epoch [7/500], Batch [10/90], Loss: 0.5833\n",
      "2024-12-12 10:17:32,626 INFO Epoch [7/500], Batch [20/90], Loss: 0.5459\n",
      "2024-12-12 10:17:36,667 INFO Epoch [7/500], Batch [30/90], Loss: 0.5628\n",
      "2024-12-12 10:17:40,799 INFO Epoch [7/500], Batch [40/90], Loss: 0.6337\n",
      "2024-12-12 10:17:44,943 INFO Epoch [7/500], Batch [50/90], Loss: 0.5580\n",
      "2024-12-12 10:17:49,227 INFO Epoch [7/500], Batch [60/90], Loss: 0.5459\n",
      "2024-12-12 10:17:53,173 INFO Epoch [7/500], Batch [70/90], Loss: 0.8150\n",
      "2024-12-12 10:17:57,210 INFO Epoch [7/500], Batch [80/90], Loss: 0.6431\n",
      "2024-12-12 10:18:01,271 INFO Epoch [7/500], Batch [90/90], Loss: 0.6012\n",
      "2024-12-12 10:18:05,392 INFO Epoch [7/500] - Train Loss: 0.5676, Train Acc: 0.7538 - Val Loss: 0.6082, Val Acc: 0.7036\n",
      "2024-12-12 10:18:09,415 INFO Epoch [8/500], Batch [10/90], Loss: 0.6866\n",
      "2024-12-12 10:18:13,562 INFO Epoch [8/500], Batch [20/90], Loss: 0.6269\n",
      "2024-12-12 10:18:17,759 INFO Epoch [8/500], Batch [30/90], Loss: 0.5660\n",
      "2024-12-12 10:18:21,827 INFO Epoch [8/500], Batch [40/90], Loss: 0.5778\n",
      "2024-12-12 10:18:25,863 INFO Epoch [8/500], Batch [50/90], Loss: 0.5452\n",
      "2024-12-12 10:18:29,862 INFO Epoch [8/500], Batch [60/90], Loss: 0.5751\n",
      "2024-12-12 10:18:33,865 INFO Epoch [8/500], Batch [70/90], Loss: 0.5883\n",
      "2024-12-12 10:18:37,973 INFO Epoch [8/500], Batch [80/90], Loss: 0.5336\n",
      "2024-12-12 10:18:42,190 INFO Epoch [8/500], Batch [90/90], Loss: 0.4220\n",
      "2024-12-12 10:18:47,395 INFO Epoch [8/500] - Train Loss: 0.5616, Train Acc: 0.7545 - Val Loss: 0.6064, Val Acc: 0.7036\n",
      "2024-12-12 10:18:52,075 INFO Epoch [9/500], Batch [10/90], Loss: 0.5086\n",
      "2024-12-12 10:18:57,237 INFO Epoch [9/500], Batch [20/90], Loss: 0.5062\n",
      "2024-12-12 10:19:01,809 INFO Epoch [9/500], Batch [30/90], Loss: 0.5648\n",
      "2024-12-12 10:19:06,508 INFO Epoch [9/500], Batch [40/90], Loss: 0.6191\n",
      "2024-12-12 10:19:10,759 INFO Epoch [9/500], Batch [50/90], Loss: 0.4854\n",
      "2024-12-12 10:19:15,268 INFO Epoch [9/500], Batch [60/90], Loss: 0.5843\n",
      "2024-12-12 10:19:19,813 INFO Epoch [9/500], Batch [70/90], Loss: 0.5361\n",
      "2024-12-12 10:19:24,511 INFO Epoch [9/500], Batch [80/90], Loss: 0.5255\n",
      "2024-12-12 10:19:29,462 INFO Epoch [9/500], Batch [90/90], Loss: 0.6017\n",
      "2024-12-12 10:19:35,415 INFO Epoch [9/500] - Train Loss: 0.5570, Train Acc: 0.7536 - Val Loss: 0.6088, Val Acc: 0.7036\n",
      "2024-12-12 10:19:40,077 INFO Epoch [10/500], Batch [10/90], Loss: 0.4684\n",
      "2024-12-12 10:19:44,688 INFO Epoch [10/500], Batch [20/90], Loss: 0.5341\n",
      "2024-12-12 10:19:48,834 INFO Epoch [10/500], Batch [30/90], Loss: 0.5068\n",
      "2024-12-12 10:19:52,960 INFO Epoch [10/500], Batch [40/90], Loss: 0.4854\n",
      "2024-12-12 10:19:57,491 INFO Epoch [10/500], Batch [50/90], Loss: 0.6158\n",
      "2024-12-12 10:20:02,137 INFO Epoch [10/500], Batch [60/90], Loss: 0.5920\n",
      "2024-12-12 10:20:06,369 INFO Epoch [10/500], Batch [70/90], Loss: 0.6392\n",
      "2024-12-12 10:20:10,977 INFO Epoch [10/500], Batch [80/90], Loss: 0.4707\n",
      "2024-12-12 10:20:15,654 INFO Epoch [10/500], Batch [90/90], Loss: 0.5583\n",
      "2024-12-12 10:20:20,947 INFO Epoch [10/500] - Train Loss: 0.5516, Train Acc: 0.7543 - Val Loss: 0.6082, Val Acc: 0.7036\n",
      "2024-12-12 10:20:25,053 INFO Epoch [11/500], Batch [10/90], Loss: 0.5695\n",
      "2024-12-12 10:20:29,386 INFO Epoch [11/500], Batch [20/90], Loss: 0.5982\n",
      "2024-12-12 10:20:33,502 INFO Epoch [11/500], Batch [30/90], Loss: 0.5750\n",
      "2024-12-12 10:20:37,678 INFO Epoch [11/500], Batch [40/90], Loss: 0.5412\n",
      "2024-12-12 10:20:42,214 INFO Epoch [11/500], Batch [50/90], Loss: 0.5610\n",
      "2024-12-12 10:20:46,746 INFO Epoch [11/500], Batch [60/90], Loss: 0.5946\n",
      "2024-12-12 10:20:51,234 INFO Epoch [11/500], Batch [70/90], Loss: 0.6380\n",
      "2024-12-12 10:20:55,584 INFO Epoch [11/500], Batch [80/90], Loss: 0.5282\n",
      "2024-12-12 10:20:59,873 INFO Epoch [11/500], Batch [90/90], Loss: 0.4804\n",
      "2024-12-12 10:21:04,974 INFO Epoch [11/500] - Train Loss: 0.5498, Train Acc: 0.7546 - Val Loss: 0.6101, Val Acc: 0.7036\n",
      "2024-12-12 10:21:09,647 INFO Epoch [12/500], Batch [10/90], Loss: 0.6539\n",
      "2024-12-12 10:21:14,240 INFO Epoch [12/500], Batch [20/90], Loss: 0.5498\n",
      "2024-12-12 10:21:18,939 INFO Epoch [12/500], Batch [30/90], Loss: 0.4734\n",
      "2024-12-12 10:21:23,098 INFO Epoch [12/500], Batch [40/90], Loss: 0.5511\n",
      "2024-12-12 10:21:27,376 INFO Epoch [12/500], Batch [50/90], Loss: 0.4687\n",
      "2024-12-12 10:21:31,746 INFO Epoch [12/500], Batch [60/90], Loss: 0.5686\n",
      "2024-12-12 10:21:36,016 INFO Epoch [12/500], Batch [70/90], Loss: 0.5248\n",
      "2024-12-12 10:21:40,212 INFO Epoch [12/500], Batch [80/90], Loss: 0.5555\n",
      "2024-12-12 10:21:44,551 INFO Epoch [12/500], Batch [90/90], Loss: 0.5693\n",
      "2024-12-12 10:21:49,976 INFO Epoch [12/500] - Train Loss: 0.5494, Train Acc: 0.7545 - Val Loss: 0.6087, Val Acc: 0.7036\n",
      "2024-12-12 10:21:54,620 INFO Epoch [13/500], Batch [10/90], Loss: 0.5538\n",
      "2024-12-12 10:21:58,759 INFO Epoch [13/500], Batch [20/90], Loss: 0.4767\n",
      "2024-12-12 10:22:02,957 INFO Epoch [13/500], Batch [30/90], Loss: 0.4537\n",
      "2024-12-12 10:22:07,150 INFO Epoch [13/500], Batch [40/90], Loss: 0.4957\n",
      "2024-12-12 10:22:11,314 INFO Epoch [13/500], Batch [50/90], Loss: 0.5919\n",
      "2024-12-12 10:22:15,954 INFO Epoch [13/500], Batch [60/90], Loss: 0.4735\n",
      "2024-12-12 10:22:20,347 INFO Epoch [13/500], Batch [70/90], Loss: 0.5433\n",
      "2024-12-12 10:22:24,872 INFO Epoch [13/500], Batch [80/90], Loss: 0.5679\n",
      "2024-12-12 10:22:29,273 INFO Epoch [13/500], Batch [90/90], Loss: 0.6220\n",
      "2024-12-12 10:22:33,970 INFO Epoch [13/500] - Train Loss: 0.5502, Train Acc: 0.7546 - Val Loss: 0.6062, Val Acc: 0.7036\n",
      "2024-12-12 10:22:38,445 INFO Epoch [14/500], Batch [10/90], Loss: 0.6992\n",
      "2024-12-12 10:22:42,957 INFO Epoch [14/500], Batch [20/90], Loss: 0.5488\n",
      "2024-12-12 10:22:47,538 INFO Epoch [14/500], Batch [30/90], Loss: 0.5286\n",
      "2024-12-12 10:22:51,759 INFO Epoch [14/500], Batch [40/90], Loss: 0.4751\n",
      "2024-12-12 10:22:57,142 INFO Epoch [14/500], Batch [50/90], Loss: 0.4929\n",
      "2024-12-12 10:23:01,408 INFO Epoch [14/500], Batch [60/90], Loss: 0.6180\n",
      "2024-12-12 10:23:05,681 INFO Epoch [14/500], Batch [70/90], Loss: 0.6032\n",
      "2024-12-12 10:23:09,804 INFO Epoch [14/500], Batch [80/90], Loss: 0.5375\n",
      "2024-12-12 10:23:14,212 INFO Epoch [14/500], Batch [90/90], Loss: 0.5463\n",
      "2024-12-12 10:23:19,502 INFO Epoch [14/500] - Train Loss: 0.5504, Train Acc: 0.7541 - Val Loss: 0.6071, Val Acc: 0.7036\n",
      "2024-12-12 10:23:24,231 INFO Epoch [15/500], Batch [10/90], Loss: 0.5644\n",
      "2024-12-12 10:23:28,926 INFO Epoch [15/500], Batch [20/90], Loss: 0.5973\n",
      "2024-12-12 10:23:33,045 INFO Epoch [15/500], Batch [30/90], Loss: 0.6093\n",
      "2024-12-12 10:23:37,203 INFO Epoch [15/500], Batch [40/90], Loss: 0.5722\n",
      "2024-12-12 10:23:41,335 INFO Epoch [15/500], Batch [50/90], Loss: 0.4973\n",
      "2024-12-12 10:23:45,388 INFO Epoch [15/500], Batch [60/90], Loss: 0.6062\n",
      "2024-12-12 10:23:49,480 INFO Epoch [15/500], Batch [70/90], Loss: 0.5455\n",
      "2024-12-12 10:23:53,758 INFO Epoch [15/500], Batch [80/90], Loss: 0.5072\n",
      "2024-12-12 10:23:58,113 INFO Epoch [15/500], Batch [90/90], Loss: 0.5688\n",
      "2024-12-12 10:24:03,029 INFO Epoch [15/500] - Train Loss: 0.5500, Train Acc: 0.7552 - Val Loss: 0.6076, Val Acc: 0.7036\n",
      "2024-12-12 10:24:07,158 INFO Epoch [16/500], Batch [10/90], Loss: 0.5673\n",
      "2024-12-12 10:24:11,220 INFO Epoch [16/500], Batch [20/90], Loss: 0.5854\n",
      "2024-12-12 10:24:15,273 INFO Epoch [16/500], Batch [30/90], Loss: 0.4597\n",
      "2024-12-12 10:24:19,348 INFO Epoch [16/500], Batch [40/90], Loss: 0.4951\n",
      "2024-12-12 10:24:23,404 INFO Epoch [16/500], Batch [50/90], Loss: 0.4887\n",
      "2024-12-12 10:24:27,614 INFO Epoch [16/500], Batch [60/90], Loss: 0.6109\n",
      "2024-12-12 10:24:31,758 INFO Epoch [16/500], Batch [70/90], Loss: 0.5803\n",
      "2024-12-12 10:24:35,715 INFO Epoch [16/500], Batch [80/90], Loss: 0.6279\n",
      "2024-12-12 10:24:39,771 INFO Epoch [16/500], Batch [90/90], Loss: 0.5838\n",
      "2024-12-12 10:24:45,202 INFO Epoch [16/500] - Train Loss: 0.5501, Train Acc: 0.7541 - Val Loss: 0.6080, Val Acc: 0.7036\n",
      "2024-12-12 10:24:50,284 INFO Epoch [17/500], Batch [10/90], Loss: 0.5841\n",
      "2024-12-12 10:24:54,675 INFO Epoch [17/500], Batch [20/90], Loss: 0.5188\n",
      "2024-12-12 10:24:59,255 INFO Epoch [17/500], Batch [30/90], Loss: 0.5022\n",
      "2024-12-12 10:25:03,722 INFO Epoch [17/500], Batch [40/90], Loss: 0.6189\n",
      "2024-12-12 10:25:08,268 INFO Epoch [17/500], Batch [50/90], Loss: 0.5647\n",
      "2024-12-12 10:25:12,590 INFO Epoch [17/500], Batch [60/90], Loss: 0.5686\n",
      "2024-12-12 10:25:17,386 INFO Epoch [17/500], Batch [70/90], Loss: 0.6094\n",
      "2024-12-12 10:25:21,996 INFO Epoch [17/500], Batch [80/90], Loss: 0.5470\n",
      "2024-12-12 10:25:26,497 INFO Epoch [17/500], Batch [90/90], Loss: 0.4396\n",
      "2024-12-12 10:25:31,802 INFO Epoch [17/500] - Train Loss: 0.5486, Train Acc: 0.7548 - Val Loss: 0.6084, Val Acc: 0.7036\n",
      "2024-12-12 10:25:36,491 INFO Epoch [18/500], Batch [10/90], Loss: 0.5563\n",
      "2024-12-12 10:25:41,335 INFO Epoch [18/500], Batch [20/90], Loss: 0.5311\n",
      "2024-12-12 10:25:46,224 INFO Epoch [18/500], Batch [30/90], Loss: 0.5515\n",
      "2024-12-12 10:25:50,771 INFO Epoch [18/500], Batch [40/90], Loss: 0.6278\n",
      "2024-12-12 10:25:55,500 INFO Epoch [18/500], Batch [50/90], Loss: 0.5423\n",
      "2024-12-12 10:26:00,379 INFO Epoch [18/500], Batch [60/90], Loss: 0.5519\n",
      "2024-12-12 10:26:05,407 INFO Epoch [18/500], Batch [70/90], Loss: 0.5163\n",
      "2024-12-12 10:26:09,677 INFO Epoch [18/500], Batch [80/90], Loss: 0.5519\n",
      "2024-12-12 10:26:13,929 INFO Epoch [18/500], Batch [90/90], Loss: 0.5684\n",
      "2024-12-12 10:26:19,666 INFO Epoch [18/500] - Train Loss: 0.5494, Train Acc: 0.7543 - Val Loss: 0.6084, Val Acc: 0.7036\n",
      "2024-12-12 10:26:24,258 INFO Epoch [19/500], Batch [10/90], Loss: 0.5480\n",
      "2024-12-12 10:26:29,971 INFO Epoch [19/500], Batch [20/90], Loss: 0.5972\n",
      "2024-12-12 10:26:34,620 INFO Epoch [19/500], Batch [30/90], Loss: 0.5676\n",
      "2024-12-12 10:26:39,060 INFO Epoch [19/500], Batch [40/90], Loss: 0.5227\n",
      "2024-12-12 10:26:43,479 INFO Epoch [19/500], Batch [50/90], Loss: 0.5627\n",
      "2024-12-12 10:26:47,813 INFO Epoch [19/500], Batch [60/90], Loss: 0.5549\n",
      "2024-12-12 10:26:52,086 INFO Epoch [19/500], Batch [70/90], Loss: 0.5617\n",
      "2024-12-12 10:26:56,308 INFO Epoch [19/500], Batch [80/90], Loss: 0.5847\n",
      "2024-12-12 10:27:00,525 INFO Epoch [19/500], Batch [90/90], Loss: 0.5039\n",
      "2024-12-12 10:27:05,219 INFO Epoch [19/500] - Train Loss: 0.5492, Train Acc: 0.7541 - Val Loss: 0.6085, Val Acc: 0.7036\n",
      "2024-12-12 10:27:09,775 INFO Epoch [20/500], Batch [10/90], Loss: 0.5744\n",
      "2024-12-12 10:27:14,667 INFO Epoch [20/500], Batch [20/90], Loss: 0.4245\n",
      "2024-12-12 10:27:19,746 INFO Epoch [20/500], Batch [30/90], Loss: 0.6329\n",
      "2024-12-12 10:27:24,812 INFO Epoch [20/500], Batch [40/90], Loss: 0.5737\n",
      "2024-12-12 10:27:29,920 INFO Epoch [20/500], Batch [50/90], Loss: 0.4823\n",
      "2024-12-12 10:27:34,795 INFO Epoch [20/500], Batch [60/90], Loss: 0.5336\n",
      "2024-12-12 10:27:39,850 INFO Epoch [20/500], Batch [70/90], Loss: 0.6195\n",
      "2024-12-12 10:27:45,032 INFO Epoch [20/500], Batch [80/90], Loss: 0.6246\n",
      "2024-12-12 10:27:49,678 INFO Epoch [20/500], Batch [90/90], Loss: 0.5957\n",
      "2024-12-12 10:27:55,084 INFO Epoch [20/500] - Train Loss: 0.5497, Train Acc: 0.7541 - Val Loss: 0.6085, Val Acc: 0.7036\n",
      "2024-12-12 10:27:59,891 INFO Epoch [21/500], Batch [10/90], Loss: 0.5406\n",
      "2024-12-12 10:28:04,732 INFO Epoch [21/500], Batch [20/90], Loss: 0.5416\n",
      "2024-12-12 10:28:09,276 INFO Epoch [21/500], Batch [30/90], Loss: 0.5739\n",
      "2024-12-12 10:28:14,013 INFO Epoch [21/500], Batch [40/90], Loss: 0.3939\n",
      "2024-12-12 10:28:18,722 INFO Epoch [21/500], Batch [50/90], Loss: 0.6588\n",
      "2024-12-12 10:28:23,260 INFO Epoch [21/500], Batch [60/90], Loss: 0.5767\n",
      "2024-12-12 10:28:27,830 INFO Epoch [21/500], Batch [70/90], Loss: 0.5353\n",
      "2024-12-12 10:28:32,247 INFO Epoch [21/500], Batch [80/90], Loss: 0.4702\n",
      "2024-12-12 10:28:36,921 INFO Epoch [21/500], Batch [90/90], Loss: 0.6280\n",
      "2024-12-12 10:28:43,698 INFO Epoch [21/500] - Train Loss: 0.5489, Train Acc: 0.7545 - Val Loss: 0.6085, Val Acc: 0.7036\n",
      "2024-12-12 10:28:48,674 INFO Epoch [22/500], Batch [10/90], Loss: 0.5831\n",
      "2024-12-12 10:28:53,230 INFO Epoch [22/500], Batch [20/90], Loss: 0.5723\n",
      "2024-12-12 10:28:57,913 INFO Epoch [22/500], Batch [30/90], Loss: 0.5769\n",
      "2024-12-12 10:29:02,480 INFO Epoch [22/500], Batch [40/90], Loss: 0.4529\n",
      "2024-12-12 10:29:07,154 INFO Epoch [22/500], Batch [50/90], Loss: 0.6502\n",
      "2024-12-12 10:29:11,714 INFO Epoch [22/500], Batch [60/90], Loss: 0.5295\n",
      "2024-12-12 10:29:16,467 INFO Epoch [22/500], Batch [70/90], Loss: 0.4998\n",
      "2024-12-12 10:29:21,003 INFO Epoch [22/500], Batch [80/90], Loss: 0.5254\n",
      "2024-12-12 10:29:25,554 INFO Epoch [22/500], Batch [90/90], Loss: 0.5488\n",
      "2024-12-12 10:29:30,622 INFO Epoch [22/500] - Train Loss: 0.5495, Train Acc: 0.7541 - Val Loss: 0.6085, Val Acc: 0.7036\n",
      "2024-12-12 10:29:35,199 INFO Epoch [23/500], Batch [10/90], Loss: 0.5578\n",
      "2024-12-12 10:29:39,684 INFO Epoch [23/500], Batch [20/90], Loss: 0.4907\n",
      "2024-12-12 10:29:44,695 INFO Epoch [23/500], Batch [30/90], Loss: 0.4580\n",
      "2024-12-12 10:29:49,370 INFO Epoch [23/500], Batch [40/90], Loss: 0.5544\n",
      "2024-12-12 10:29:53,937 INFO Epoch [23/500], Batch [50/90], Loss: 0.6398\n",
      "2024-12-12 10:29:58,954 INFO Epoch [23/500], Batch [60/90], Loss: 0.5811\n",
      "2024-12-12 10:30:04,157 INFO Epoch [23/500], Batch [70/90], Loss: 0.4793\n",
      "2024-12-12 10:30:09,018 INFO Epoch [23/500], Batch [80/90], Loss: 0.5397\n",
      "2024-12-12 10:30:14,030 INFO Epoch [23/500], Batch [90/90], Loss: 0.6489\n",
      "2024-12-12 10:30:20,804 INFO Epoch [23/500] - Train Loss: 0.5498, Train Acc: 0.7536 - Val Loss: 0.6085, Val Acc: 0.7036\n",
      "2024-12-12 10:30:25,739 INFO Epoch [24/500], Batch [10/90], Loss: 0.5585\n",
      "2024-12-12 10:30:30,280 INFO Epoch [24/500], Batch [20/90], Loss: 0.6128\n",
      "2024-12-12 10:30:34,942 INFO Epoch [24/500], Batch [30/90], Loss: 0.5278\n",
      "2024-12-12 10:30:39,501 INFO Epoch [24/500], Batch [40/90], Loss: 0.5631\n",
      "2024-12-12 10:30:44,119 INFO Epoch [24/500], Batch [50/90], Loss: 0.6223\n",
      "2024-12-12 10:30:48,729 INFO Epoch [24/500], Batch [60/90], Loss: 0.5025\n",
      "2024-12-12 10:30:53,147 INFO Epoch [24/500], Batch [70/90], Loss: 0.6039\n",
      "2024-12-12 10:30:57,525 INFO Epoch [24/500], Batch [80/90], Loss: 0.4791\n",
      "2024-12-12 10:31:02,176 INFO Epoch [24/500], Batch [90/90], Loss: 0.5567\n",
      "2024-12-12 10:31:07,944 INFO Epoch [24/500] - Train Loss: 0.5498, Train Acc: 0.7534 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:31:12,634 INFO Epoch [25/500], Batch [10/90], Loss: 0.5439\n",
      "2024-12-12 10:31:17,380 INFO Epoch [25/500], Batch [20/90], Loss: 0.5027\n",
      "2024-12-12 10:31:22,080 INFO Epoch [25/500], Batch [30/90], Loss: 0.5394\n",
      "2024-12-12 10:31:26,666 INFO Epoch [25/500], Batch [40/90], Loss: 0.4922\n",
      "2024-12-12 10:31:31,190 INFO Epoch [25/500], Batch [50/90], Loss: 0.5581\n",
      "2024-12-12 10:31:35,890 INFO Epoch [25/500], Batch [60/90], Loss: 0.6128\n",
      "2024-12-12 10:31:40,356 INFO Epoch [25/500], Batch [70/90], Loss: 0.4569\n",
      "2024-12-12 10:31:45,352 INFO Epoch [25/500], Batch [80/90], Loss: 0.6208\n",
      "2024-12-12 10:31:53,019 INFO Epoch [25/500], Batch [90/90], Loss: 0.5816\n",
      "2024-12-12 10:31:58,741 INFO Epoch [25/500] - Train Loss: 0.5489, Train Acc: 0.7540 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:32:03,886 INFO Epoch [26/500], Batch [10/90], Loss: 0.5386\n",
      "2024-12-12 10:32:08,462 INFO Epoch [26/500], Batch [20/90], Loss: 0.6521\n",
      "2024-12-12 10:32:13,317 INFO Epoch [26/500], Batch [30/90], Loss: 0.4988\n",
      "2024-12-12 10:32:18,135 INFO Epoch [26/500], Batch [40/90], Loss: 0.5819\n",
      "2024-12-12 10:32:23,177 INFO Epoch [26/500], Batch [50/90], Loss: 0.5665\n",
      "2024-12-12 10:32:27,879 INFO Epoch [26/500], Batch [60/90], Loss: 0.5926\n",
      "2024-12-12 10:32:32,448 INFO Epoch [26/500], Batch [70/90], Loss: 0.5543\n",
      "2024-12-12 10:32:36,938 INFO Epoch [26/500], Batch [80/90], Loss: 0.7204\n",
      "2024-12-12 10:32:41,505 INFO Epoch [26/500], Batch [90/90], Loss: 0.5356\n",
      "2024-12-12 10:32:46,562 INFO Epoch [26/500] - Train Loss: 0.5478, Train Acc: 0.7548 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:32:51,419 INFO Epoch [27/500], Batch [10/90], Loss: 0.5580\n",
      "2024-12-12 10:32:56,526 INFO Epoch [27/500], Batch [20/90], Loss: 0.5225\n",
      "2024-12-12 10:33:01,518 INFO Epoch [27/500], Batch [30/90], Loss: 0.4649\n",
      "2024-12-12 10:33:06,444 INFO Epoch [27/500], Batch [40/90], Loss: 0.5927\n",
      "2024-12-12 10:33:11,200 INFO Epoch [27/500], Batch [50/90], Loss: 0.5201\n",
      "2024-12-12 10:33:15,932 INFO Epoch [27/500], Batch [60/90], Loss: 0.5713\n",
      "2024-12-12 10:33:21,188 INFO Epoch [27/500], Batch [70/90], Loss: 0.4966\n",
      "2024-12-12 10:33:26,192 INFO Epoch [27/500], Batch [80/90], Loss: 0.5815\n",
      "2024-12-12 10:33:31,310 INFO Epoch [27/500], Batch [90/90], Loss: 0.5775\n",
      "2024-12-12 10:33:37,652 INFO Epoch [27/500] - Train Loss: 0.5489, Train Acc: 0.7541 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:33:42,285 INFO Epoch [28/500], Batch [10/90], Loss: 0.6076\n",
      "2024-12-12 10:33:46,746 INFO Epoch [28/500], Batch [20/90], Loss: 0.4919\n",
      "2024-12-12 10:33:51,417 INFO Epoch [28/500], Batch [30/90], Loss: 0.4949\n",
      "2024-12-12 10:33:55,939 INFO Epoch [28/500], Batch [40/90], Loss: 0.4849\n",
      "2024-12-12 10:34:00,215 INFO Epoch [28/500], Batch [50/90], Loss: 0.5597\n",
      "2024-12-12 10:34:04,839 INFO Epoch [28/500], Batch [60/90], Loss: 0.6387\n",
      "2024-12-12 10:34:09,193 INFO Epoch [28/500], Batch [70/90], Loss: 0.6050\n",
      "2024-12-12 10:34:13,629 INFO Epoch [28/500], Batch [80/90], Loss: 0.5237\n",
      "2024-12-12 10:34:18,409 INFO Epoch [28/500], Batch [90/90], Loss: 0.6148\n",
      "2024-12-12 10:34:23,901 INFO Epoch [28/500] - Train Loss: 0.5492, Train Acc: 0.7543 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:34:28,703 INFO Epoch [29/500], Batch [10/90], Loss: 0.5329\n",
      "2024-12-12 10:34:33,099 INFO Epoch [29/500], Batch [20/90], Loss: 0.5118\n",
      "2024-12-12 10:34:37,602 INFO Epoch [29/500], Batch [30/90], Loss: 0.6272\n",
      "2024-12-12 10:34:42,009 INFO Epoch [29/500], Batch [40/90], Loss: 0.5041\n",
      "2024-12-12 10:34:46,630 INFO Epoch [29/500], Batch [50/90], Loss: 0.5107\n",
      "2024-12-12 10:34:51,286 INFO Epoch [29/500], Batch [60/90], Loss: 0.4824\n",
      "2024-12-12 10:34:55,989 INFO Epoch [29/500], Batch [70/90], Loss: 0.5699\n",
      "2024-12-12 10:35:00,864 INFO Epoch [29/500], Batch [80/90], Loss: 0.5141\n",
      "2024-12-12 10:35:05,372 INFO Epoch [29/500], Batch [90/90], Loss: 0.5354\n",
      "2024-12-12 10:35:10,040 INFO Epoch [29/500] - Train Loss: 0.5498, Train Acc: 0.7543 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:35:14,416 INFO Epoch [30/500], Batch [10/90], Loss: 0.6100\n",
      "2024-12-12 10:35:18,847 INFO Epoch [30/500], Batch [20/90], Loss: 0.5345\n",
      "2024-12-12 10:35:23,642 INFO Epoch [30/500], Batch [30/90], Loss: 0.4886\n",
      "2024-12-12 10:35:28,090 INFO Epoch [30/500], Batch [40/90], Loss: 0.5892\n",
      "2024-12-12 10:35:32,582 INFO Epoch [30/500], Batch [50/90], Loss: 0.5522\n",
      "2024-12-12 10:35:37,030 INFO Epoch [30/500], Batch [60/90], Loss: 0.5064\n",
      "2024-12-12 10:35:41,965 INFO Epoch [30/500], Batch [70/90], Loss: 0.4389\n",
      "2024-12-12 10:35:46,445 INFO Epoch [30/500], Batch [80/90], Loss: 0.5991\n",
      "2024-12-12 10:35:51,242 INFO Epoch [30/500], Batch [90/90], Loss: 0.4672\n",
      "2024-12-12 10:35:56,826 INFO Epoch [30/500] - Train Loss: 0.5496, Train Acc: 0.7533 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:36:01,148 INFO Epoch [31/500], Batch [10/90], Loss: 0.5847\n",
      "2024-12-12 10:36:05,715 INFO Epoch [31/500], Batch [20/90], Loss: 0.5444\n",
      "2024-12-12 10:36:10,092 INFO Epoch [31/500], Batch [30/90], Loss: 0.5545\n",
      "2024-12-12 10:36:14,572 INFO Epoch [31/500], Batch [40/90], Loss: 0.5251\n",
      "2024-12-12 10:36:19,116 INFO Epoch [31/500], Batch [50/90], Loss: 0.5173\n",
      "2024-12-12 10:36:23,850 INFO Epoch [31/500], Batch [60/90], Loss: 0.5716\n",
      "2024-12-12 10:36:28,273 INFO Epoch [31/500], Batch [70/90], Loss: 0.5035\n",
      "2024-12-12 10:36:32,850 INFO Epoch [31/500], Batch [80/90], Loss: 0.5191\n",
      "2024-12-12 10:36:37,354 INFO Epoch [31/500], Batch [90/90], Loss: 0.5431\n",
      "2024-12-12 10:36:42,721 INFO Epoch [31/500] - Train Loss: 0.5483, Train Acc: 0.7546 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:36:47,425 INFO Epoch [32/500], Batch [10/90], Loss: 0.5466\n",
      "2024-12-12 10:36:52,184 INFO Epoch [32/500], Batch [20/90], Loss: 0.4704\n",
      "2024-12-12 10:36:57,393 INFO Epoch [32/500], Batch [30/90], Loss: 0.5227\n",
      "2024-12-12 10:37:02,026 INFO Epoch [32/500], Batch [40/90], Loss: 0.5710\n",
      "2024-12-12 10:37:06,561 INFO Epoch [32/500], Batch [50/90], Loss: 0.5626\n",
      "2024-12-12 10:37:10,979 INFO Epoch [32/500], Batch [60/90], Loss: 0.6621\n",
      "2024-12-12 10:37:15,483 INFO Epoch [32/500], Batch [70/90], Loss: 0.5692\n",
      "2024-12-12 10:37:20,045 INFO Epoch [32/500], Batch [80/90], Loss: 0.6052\n",
      "2024-12-12 10:37:24,740 INFO Epoch [32/500], Batch [90/90], Loss: 0.5349\n",
      "2024-12-12 10:37:30,007 INFO Epoch [32/500] - Train Loss: 0.5471, Train Acc: 0.7548 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:37:34,520 INFO Epoch [33/500], Batch [10/90], Loss: 0.5632\n",
      "2024-12-12 10:37:38,894 INFO Epoch [33/500], Batch [20/90], Loss: 0.5850\n",
      "2024-12-12 10:37:43,278 INFO Epoch [33/500], Batch [30/90], Loss: 0.5068\n",
      "2024-12-12 10:37:47,823 INFO Epoch [33/500], Batch [40/90], Loss: 0.6407\n",
      "2024-12-12 10:37:52,489 INFO Epoch [33/500], Batch [50/90], Loss: 0.5743\n",
      "2024-12-12 10:37:57,071 INFO Epoch [33/500], Batch [60/90], Loss: 0.6357\n",
      "2024-12-12 10:38:01,514 INFO Epoch [33/500], Batch [70/90], Loss: 0.5074\n",
      "2024-12-12 10:38:06,004 INFO Epoch [33/500], Batch [80/90], Loss: 0.5999\n",
      "2024-12-12 10:38:10,553 INFO Epoch [33/500], Batch [90/90], Loss: 0.5658\n",
      "2024-12-12 10:38:15,081 INFO Epoch [33/500] - Train Loss: 0.5477, Train Acc: 0.7555 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:38:19,611 INFO Epoch [34/500], Batch [10/90], Loss: 0.5749\n",
      "2024-12-12 10:38:24,275 INFO Epoch [34/500], Batch [20/90], Loss: 0.5503\n",
      "2024-12-12 10:38:28,822 INFO Epoch [34/500], Batch [30/90], Loss: 0.5666\n",
      "2024-12-12 10:38:33,300 INFO Epoch [34/500], Batch [40/90], Loss: 0.5363\n",
      "2024-12-12 10:38:37,946 INFO Epoch [34/500], Batch [50/90], Loss: 0.5400\n",
      "2024-12-12 10:38:42,519 INFO Epoch [34/500], Batch [60/90], Loss: 0.4908\n",
      "2024-12-12 10:38:47,274 INFO Epoch [34/500], Batch [70/90], Loss: 0.5140\n",
      "2024-12-12 10:38:51,901 INFO Epoch [34/500], Batch [80/90], Loss: 0.4740\n",
      "2024-12-12 10:38:57,205 INFO Epoch [34/500], Batch [90/90], Loss: 0.5689\n",
      "2024-12-12 10:39:02,132 INFO Epoch [34/500] - Train Loss: 0.5484, Train Acc: 0.7545 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:39:06,592 INFO Epoch [35/500], Batch [10/90], Loss: 0.6117\n",
      "2024-12-12 10:39:11,124 INFO Epoch [35/500], Batch [20/90], Loss: 0.5591\n",
      "2024-12-12 10:39:15,476 INFO Epoch [35/500], Batch [30/90], Loss: 0.5221\n",
      "2024-12-12 10:39:20,190 INFO Epoch [35/500], Batch [40/90], Loss: 0.5855\n",
      "2024-12-12 10:39:24,971 INFO Epoch [35/500], Batch [50/90], Loss: 0.4500\n",
      "2024-12-12 10:39:29,577 INFO Epoch [35/500], Batch [60/90], Loss: 0.5837\n",
      "2024-12-12 10:39:33,952 INFO Epoch [35/500], Batch [70/90], Loss: 0.5025\n",
      "2024-12-12 10:39:38,309 INFO Epoch [35/500], Batch [80/90], Loss: 0.5426\n",
      "2024-12-12 10:39:42,674 INFO Epoch [35/500], Batch [90/90], Loss: 0.6534\n",
      "2024-12-12 10:39:47,861 INFO Epoch [35/500] - Train Loss: 0.5498, Train Acc: 0.7540 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:39:52,450 INFO Epoch [36/500], Batch [10/90], Loss: 0.6587\n",
      "2024-12-12 10:39:56,902 INFO Epoch [36/500], Batch [20/90], Loss: 0.4934\n",
      "2024-12-12 10:40:01,672 INFO Epoch [36/500], Batch [30/90], Loss: 0.5352\n",
      "2024-12-12 10:40:06,118 INFO Epoch [36/500], Batch [40/90], Loss: 0.6420\n",
      "2024-12-12 10:40:10,406 INFO Epoch [36/500], Batch [50/90], Loss: 0.4904\n",
      "2024-12-12 10:40:14,837 INFO Epoch [36/500], Batch [60/90], Loss: 0.5703\n",
      "2024-12-12 10:40:19,387 INFO Epoch [36/500], Batch [70/90], Loss: 0.6104\n",
      "2024-12-12 10:40:24,167 INFO Epoch [36/500], Batch [80/90], Loss: 0.5420\n",
      "2024-12-12 10:40:28,529 INFO Epoch [36/500], Batch [90/90], Loss: 0.4771\n",
      "2024-12-12 10:40:33,713 INFO Epoch [36/500] - Train Loss: 0.5488, Train Acc: 0.7548 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:40:38,781 INFO Epoch [37/500], Batch [10/90], Loss: 0.6191\n",
      "2024-12-12 10:40:43,394 INFO Epoch [37/500], Batch [20/90], Loss: 0.6349\n",
      "2024-12-12 10:40:47,919 INFO Epoch [37/500], Batch [30/90], Loss: 0.5831\n",
      "2024-12-12 10:40:52,596 INFO Epoch [37/500], Batch [40/90], Loss: 0.5777\n",
      "2024-12-12 10:40:57,208 INFO Epoch [37/500], Batch [50/90], Loss: 0.5507\n",
      "2024-12-12 10:41:01,662 INFO Epoch [37/500], Batch [60/90], Loss: 0.6185\n",
      "2024-12-12 10:41:06,180 INFO Epoch [37/500], Batch [70/90], Loss: 0.5985\n",
      "2024-12-12 10:41:10,719 INFO Epoch [37/500], Batch [80/90], Loss: 0.5741\n",
      "2024-12-12 10:41:15,425 INFO Epoch [37/500], Batch [90/90], Loss: 0.6521\n",
      "2024-12-12 10:41:22,023 INFO Epoch [37/500] - Train Loss: 0.5507, Train Acc: 0.7538 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:41:26,677 INFO Epoch [38/500], Batch [10/90], Loss: 0.5889\n",
      "2024-12-12 10:41:31,259 INFO Epoch [38/500], Batch [20/90], Loss: 0.6394\n",
      "2024-12-12 10:41:35,684 INFO Epoch [38/500], Batch [30/90], Loss: 0.5974\n",
      "2024-12-12 10:41:39,973 INFO Epoch [38/500], Batch [40/90], Loss: 0.4786\n",
      "2024-12-12 10:41:44,625 INFO Epoch [38/500], Batch [50/90], Loss: 0.6645\n",
      "2024-12-12 10:41:49,313 INFO Epoch [38/500], Batch [60/90], Loss: 0.5033\n",
      "2024-12-12 10:41:54,016 INFO Epoch [38/500], Batch [70/90], Loss: 0.6552\n",
      "2024-12-12 10:41:58,454 INFO Epoch [38/500], Batch [80/90], Loss: 0.4912\n",
      "2024-12-12 10:42:02,889 INFO Epoch [38/500], Batch [90/90], Loss: 0.4527\n",
      "2024-12-12 10:42:08,139 INFO Epoch [38/500] - Train Loss: 0.5498, Train Acc: 0.7545 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:42:13,272 INFO Epoch [39/500], Batch [10/90], Loss: 0.5918\n",
      "2024-12-12 10:42:17,969 INFO Epoch [39/500], Batch [20/90], Loss: 0.6578\n",
      "2024-12-12 10:42:22,761 INFO Epoch [39/500], Batch [30/90], Loss: 0.5750\n",
      "2024-12-12 10:42:27,292 INFO Epoch [39/500], Batch [40/90], Loss: 0.6105\n",
      "2024-12-12 10:42:31,825 INFO Epoch [39/500], Batch [50/90], Loss: 0.5898\n",
      "2024-12-12 10:42:36,257 INFO Epoch [39/500], Batch [60/90], Loss: 0.5069\n",
      "2024-12-12 10:42:40,734 INFO Epoch [39/500], Batch [70/90], Loss: 0.5482\n",
      "2024-12-12 10:42:45,238 INFO Epoch [39/500], Batch [80/90], Loss: 0.5416\n",
      "2024-12-12 10:42:49,945 INFO Epoch [39/500], Batch [90/90], Loss: 0.6094\n",
      "2024-12-12 10:42:55,775 INFO Epoch [39/500] - Train Loss: 0.5488, Train Acc: 0.7546 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:43:00,243 INFO Epoch [40/500], Batch [10/90], Loss: 0.5532\n",
      "2024-12-12 10:43:04,877 INFO Epoch [40/500], Batch [20/90], Loss: 0.5525\n",
      "2024-12-12 10:43:09,309 INFO Epoch [40/500], Batch [30/90], Loss: 0.5442\n",
      "2024-12-12 10:43:13,887 INFO Epoch [40/500], Batch [40/90], Loss: 0.5127\n",
      "2024-12-12 10:43:18,360 INFO Epoch [40/500], Batch [50/90], Loss: 0.5806\n",
      "2024-12-12 10:43:23,276 INFO Epoch [40/500], Batch [60/90], Loss: 0.5427\n",
      "2024-12-12 10:43:27,924 INFO Epoch [40/500], Batch [70/90], Loss: 0.6602\n",
      "2024-12-12 10:43:32,500 INFO Epoch [40/500], Batch [80/90], Loss: 0.5148\n",
      "2024-12-12 10:43:37,086 INFO Epoch [40/500], Batch [90/90], Loss: 0.6152\n",
      "2024-12-12 10:43:42,098 INFO Epoch [40/500] - Train Loss: 0.5496, Train Acc: 0.7545 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:43:46,801 INFO Epoch [41/500], Batch [10/90], Loss: 0.5802\n",
      "2024-12-12 10:43:51,732 INFO Epoch [41/500], Batch [20/90], Loss: 0.5228\n",
      "2024-12-12 10:43:56,454 INFO Epoch [41/500], Batch [30/90], Loss: 0.5557\n",
      "2024-12-12 10:44:00,965 INFO Epoch [41/500], Batch [40/90], Loss: 0.5718\n",
      "2024-12-12 10:44:05,625 INFO Epoch [41/500], Batch [50/90], Loss: 0.5425\n",
      "2024-12-12 10:44:10,323 INFO Epoch [41/500], Batch [60/90], Loss: 0.5068\n",
      "2024-12-12 10:44:15,184 INFO Epoch [41/500], Batch [70/90], Loss: 0.5023\n",
      "2024-12-12 10:44:19,837 INFO Epoch [41/500], Batch [80/90], Loss: 0.6202\n",
      "2024-12-12 10:44:24,392 INFO Epoch [41/500], Batch [90/90], Loss: 0.6616\n",
      "2024-12-12 10:44:30,047 INFO Epoch [41/500] - Train Loss: 0.5484, Train Acc: 0.7543 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:44:34,985 INFO Epoch [42/500], Batch [10/90], Loss: 0.5617\n",
      "2024-12-12 10:44:39,603 INFO Epoch [42/500], Batch [20/90], Loss: 0.6157\n",
      "2024-12-12 10:44:43,953 INFO Epoch [42/500], Batch [30/90], Loss: 0.5656\n",
      "2024-12-12 10:44:48,604 INFO Epoch [42/500], Batch [40/90], Loss: 0.6743\n",
      "2024-12-12 10:44:53,134 INFO Epoch [42/500], Batch [50/90], Loss: 0.5263\n",
      "2024-12-12 10:44:57,689 INFO Epoch [42/500], Batch [60/90], Loss: 0.4914\n",
      "2024-12-12 10:45:02,050 INFO Epoch [42/500], Batch [70/90], Loss: 0.4642\n",
      "2024-12-12 10:45:06,492 INFO Epoch [42/500], Batch [80/90], Loss: 0.5143\n",
      "2024-12-12 10:45:10,797 INFO Epoch [42/500], Batch [90/90], Loss: 0.5408\n",
      "2024-12-12 10:45:15,820 INFO Epoch [42/500] - Train Loss: 0.5497, Train Acc: 0.7543 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:45:20,447 INFO Epoch [43/500], Batch [10/90], Loss: 0.5664\n",
      "2024-12-12 10:45:25,028 INFO Epoch [43/500], Batch [20/90], Loss: 0.6131\n",
      "2024-12-12 10:45:29,475 INFO Epoch [43/500], Batch [30/90], Loss: 0.5993\n",
      "2024-12-12 10:45:34,184 INFO Epoch [43/500], Batch [40/90], Loss: 0.5655\n",
      "2024-12-12 10:45:38,510 INFO Epoch [43/500], Batch [50/90], Loss: 0.5763\n",
      "2024-12-12 10:45:42,968 INFO Epoch [43/500], Batch [60/90], Loss: 0.5273\n",
      "2024-12-12 10:45:47,379 INFO Epoch [43/500], Batch [70/90], Loss: 0.5591\n",
      "2024-12-12 10:45:51,916 INFO Epoch [43/500], Batch [80/90], Loss: 0.5790\n",
      "2024-12-12 10:45:56,463 INFO Epoch [43/500], Batch [90/90], Loss: 0.5616\n",
      "2024-12-12 10:46:01,730 INFO Epoch [43/500] - Train Loss: 0.5493, Train Acc: 0.7545 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:46:06,310 INFO Epoch [44/500], Batch [10/90], Loss: 0.5725\n",
      "2024-12-12 10:46:10,835 INFO Epoch [44/500], Batch [20/90], Loss: 0.5696\n",
      "2024-12-12 10:46:15,172 INFO Epoch [44/500], Batch [30/90], Loss: 0.6326\n",
      "2024-12-12 10:46:19,942 INFO Epoch [44/500], Batch [40/90], Loss: 0.5121\n",
      "2024-12-12 10:46:24,592 INFO Epoch [44/500], Batch [50/90], Loss: 0.5363\n",
      "2024-12-12 10:46:29,149 INFO Epoch [44/500], Batch [60/90], Loss: 0.5483\n",
      "2024-12-12 10:46:33,584 INFO Epoch [44/500], Batch [70/90], Loss: 0.5568\n",
      "2024-12-12 10:46:38,018 INFO Epoch [44/500], Batch [80/90], Loss: 0.4737\n",
      "2024-12-12 10:46:42,438 INFO Epoch [44/500], Batch [90/90], Loss: 0.6345\n",
      "2024-12-12 10:46:47,909 INFO Epoch [44/500] - Train Loss: 0.5476, Train Acc: 0.7552 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:46:52,557 INFO Epoch [45/500], Batch [10/90], Loss: 0.5756\n",
      "2024-12-12 10:46:57,157 INFO Epoch [45/500], Batch [20/90], Loss: 0.6045\n",
      "2024-12-12 10:47:01,578 INFO Epoch [45/500], Batch [30/90], Loss: 0.5138\n",
      "2024-12-12 10:47:05,949 INFO Epoch [45/500], Batch [40/90], Loss: 0.5612\n",
      "2024-12-12 10:47:10,257 INFO Epoch [45/500], Batch [50/90], Loss: 0.5895\n",
      "2024-12-12 10:47:14,639 INFO Epoch [45/500], Batch [60/90], Loss: 0.5276\n",
      "2024-12-12 10:47:19,218 INFO Epoch [45/500], Batch [70/90], Loss: 0.5882\n",
      "2024-12-12 10:47:24,008 INFO Epoch [45/500], Batch [80/90], Loss: 0.6075\n",
      "2024-12-12 10:47:28,594 INFO Epoch [45/500], Batch [90/90], Loss: 0.5278\n",
      "2024-12-12 10:47:33,597 INFO Epoch [45/500] - Train Loss: 0.5497, Train Acc: 0.7543 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:47:38,240 INFO Epoch [46/500], Batch [10/90], Loss: 0.4732\n",
      "2024-12-12 10:47:42,620 INFO Epoch [46/500], Batch [20/90], Loss: 0.5218\n",
      "2024-12-12 10:47:47,343 INFO Epoch [46/500], Batch [30/90], Loss: 0.5054\n",
      "2024-12-12 10:47:52,361 INFO Epoch [46/500], Batch [40/90], Loss: 0.5531\n",
      "2024-12-12 10:47:57,109 INFO Epoch [46/500], Batch [50/90], Loss: 0.5131\n",
      "2024-12-12 10:48:02,025 INFO Epoch [46/500], Batch [60/90], Loss: 0.5430\n",
      "2024-12-12 10:48:06,745 INFO Epoch [46/500], Batch [70/90], Loss: 0.4732\n",
      "2024-12-12 10:48:11,082 INFO Epoch [46/500], Batch [80/90], Loss: 0.5861\n",
      "2024-12-12 10:48:15,475 INFO Epoch [46/500], Batch [90/90], Loss: 0.5392\n",
      "2024-12-12 10:48:21,415 INFO Epoch [46/500] - Train Loss: 0.5499, Train Acc: 0.7538 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:48:25,985 INFO Epoch [47/500], Batch [10/90], Loss: 0.6180\n",
      "2024-12-12 10:48:30,703 INFO Epoch [47/500], Batch [20/90], Loss: 0.6646\n",
      "2024-12-12 10:48:35,376 INFO Epoch [47/500], Batch [30/90], Loss: 0.5099\n",
      "2024-12-12 10:48:40,153 INFO Epoch [47/500], Batch [40/90], Loss: 0.6153\n",
      "2024-12-12 10:48:44,626 INFO Epoch [47/500], Batch [50/90], Loss: 0.5778\n",
      "2024-12-12 10:48:49,640 INFO Epoch [47/500], Batch [60/90], Loss: 0.4968\n",
      "2024-12-12 10:48:54,331 INFO Epoch [47/500], Batch [70/90], Loss: 0.4996\n",
      "2024-12-12 10:48:58,812 INFO Epoch [47/500], Batch [80/90], Loss: 0.5899\n",
      "2024-12-12 10:49:03,256 INFO Epoch [47/500], Batch [90/90], Loss: 0.5259\n",
      "2024-12-12 10:49:08,669 INFO Epoch [47/500] - Train Loss: 0.5501, Train Acc: 0.7534 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:49:13,073 INFO Epoch [48/500], Batch [10/90], Loss: 0.5157\n",
      "2024-12-12 10:49:17,588 INFO Epoch [48/500], Batch [20/90], Loss: 0.4708\n",
      "2024-12-12 10:49:22,772 INFO Epoch [48/500], Batch [30/90], Loss: 0.6578\n",
      "2024-12-12 10:49:27,315 INFO Epoch [48/500], Batch [40/90], Loss: 0.6407\n",
      "2024-12-12 10:49:31,858 INFO Epoch [48/500], Batch [50/90], Loss: 0.5547\n",
      "2024-12-12 10:49:36,428 INFO Epoch [48/500], Batch [60/90], Loss: 0.4597\n",
      "2024-12-12 10:49:40,814 INFO Epoch [48/500], Batch [70/90], Loss: 0.5801\n",
      "2024-12-12 10:49:45,300 INFO Epoch [48/500], Batch [80/90], Loss: 0.5676\n",
      "2024-12-12 10:49:50,072 INFO Epoch [48/500], Batch [90/90], Loss: 0.5617\n",
      "2024-12-12 10:49:55,191 INFO Epoch [48/500] - Train Loss: 0.5491, Train Acc: 0.7545 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:49:59,897 INFO Epoch [49/500], Batch [10/90], Loss: 0.5671\n",
      "2024-12-12 10:50:04,778 INFO Epoch [49/500], Batch [20/90], Loss: 0.4846\n",
      "2024-12-12 10:50:09,158 INFO Epoch [49/500], Batch [30/90], Loss: 0.5681\n",
      "2024-12-12 10:50:13,780 INFO Epoch [49/500], Batch [40/90], Loss: 0.5685\n",
      "2024-12-12 10:50:18,225 INFO Epoch [49/500], Batch [50/90], Loss: 0.5877\n",
      "2024-12-12 10:50:22,964 INFO Epoch [49/500], Batch [60/90], Loss: 0.5544\n",
      "2024-12-12 10:50:27,551 INFO Epoch [49/500], Batch [70/90], Loss: 0.5581\n",
      "2024-12-12 10:50:32,136 INFO Epoch [49/500], Batch [80/90], Loss: 0.6114\n",
      "2024-12-12 10:50:36,551 INFO Epoch [49/500], Batch [90/90], Loss: 0.6547\n",
      "2024-12-12 10:50:41,655 INFO Epoch [49/500] - Train Loss: 0.5501, Train Acc: 0.7540 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:50:45,990 INFO Epoch [50/500], Batch [10/90], Loss: 0.5616\n",
      "2024-12-12 10:50:50,654 INFO Epoch [50/500], Batch [20/90], Loss: 0.5996\n",
      "2024-12-12 10:50:55,196 INFO Epoch [50/500], Batch [30/90], Loss: 0.5387\n",
      "2024-12-12 10:50:59,579 INFO Epoch [50/500], Batch [40/90], Loss: 0.4743\n",
      "2024-12-12 10:51:04,102 INFO Epoch [50/500], Batch [50/90], Loss: 0.5536\n",
      "2024-12-12 10:51:08,452 INFO Epoch [50/500], Batch [60/90], Loss: 0.5601\n",
      "2024-12-12 10:51:12,926 INFO Epoch [50/500], Batch [70/90], Loss: 0.5325\n",
      "2024-12-12 10:51:17,357 INFO Epoch [50/500], Batch [80/90], Loss: 0.5047\n",
      "2024-12-12 10:51:22,025 INFO Epoch [50/500], Batch [90/90], Loss: 0.5192\n",
      "2024-12-12 10:51:27,160 INFO Epoch [50/500] - Train Loss: 0.5493, Train Acc: 0.7543 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:51:31,918 INFO Epoch [51/500], Batch [10/90], Loss: 0.5267\n",
      "2024-12-12 10:51:36,497 INFO Epoch [51/500], Batch [20/90], Loss: 0.5064\n",
      "2024-12-12 10:51:41,095 INFO Epoch [51/500], Batch [30/90], Loss: 0.5205\n",
      "2024-12-12 10:51:45,613 INFO Epoch [51/500], Batch [40/90], Loss: 0.5027\n",
      "2024-12-12 10:51:50,321 INFO Epoch [51/500], Batch [50/90], Loss: 0.5983\n",
      "2024-12-12 10:51:55,005 INFO Epoch [51/500], Batch [60/90], Loss: 0.6026\n",
      "2024-12-12 10:51:59,656 INFO Epoch [51/500], Batch [70/90], Loss: 0.5369\n",
      "2024-12-12 10:52:04,201 INFO Epoch [51/500], Batch [80/90], Loss: 0.5356\n",
      "2024-12-12 10:52:08,613 INFO Epoch [51/500], Batch [90/90], Loss: 0.5298\n",
      "2024-12-12 10:52:13,594 INFO Epoch [51/500] - Train Loss: 0.5484, Train Acc: 0.7545 - Val Loss: 0.6086, Val Acc: 0.7036\n",
      "2024-12-12 10:52:13,595 INFO No improvement for 50 epochs. Early stopping.\n",
      "2024-12-12 10:52:13,603 INFO Loading the best model for testing...\n",
      "2024-12-12 10:52:13,608 INFO Evaluating on test set...\n",
      "2024-12-12 10:52:16,634 INFO Test Metrics: {'loss': 1.4628509448036533, 'accuracy': 0.7424789410348978, 'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1_score': np.float64(0.0), 'confusion_matrix': array([[617,   0],\n",
      "       [214,   0]])}\n",
      "2024-12-12 10:52:16,636 INFO Metrics saved to /Users/shawn/Documents/personal/rsi_divergence_detector/model_data/mixed_lstm/metrics.pkl\n",
      "2024-12-12 10:52:16,637 INFO Plotting training results...\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "logger.info(\"Starting training...\")\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=500,\n",
    "    lr=1e-3,\n",
    "    device=device,\n",
    "    log_interval=10,\n",
    "    save_path=model_save_path\n",
    ")\n",
    "\n",
    "# Load the best model\n",
    "logger.info(\"Loading the best model for testing...\")\n",
    "model.load_state_dict(torch.load(model_save_path, map_location=device))\n",
    "\n",
    "# Evaluate on test set\n",
    "logger.info(\"Evaluating on test set...\")\n",
    "    # After evaluating on the test set\n",
    "test_metrics = evaluate_model(model, test_loader, device)\n",
    "logger.info(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "metrics_path = os.path.join(PROJECT_PATH, 'model_data', 'mixed_lstm', 'metrics.pkl')\n",
    "metrics = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'test_loss': test_metrics['loss'],\n",
    "    'test_accuracy': test_metrics['accuracy']\n",
    "}\n",
    "# Save the extended metrics\n",
    "metrics['test_precision'] = test_metrics['precision']\n",
    "metrics['test_recall'] = test_metrics['recall']\n",
    "metrics['test_f1_score'] = test_metrics['f1_score']\n",
    "metrics['test_confusion_matrix'] = test_metrics['confusion_matrix']\n",
    "\n",
    "pd.to_pickle(metrics, metrics_path)\n",
    "logger.info(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "# Plot training and validation metrics\n",
    "logger.info(\"Plotting training results...\")\n",
    "plot_results(train_losses, val_losses, train_accuracies, val_accuracies, outdir=os.path.join(PROJECT_PATH, 'plots'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixedLSTMModel(\n",
       "  (lstm): LSTM(29, 128, num_layers=3, batch_first=True, dropout=0.3)\n",
       "  (fc1): Linear(in_features=134, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc2): Linear(in_features=256, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "False    617\n",
       "True     214\n",
       "Name: count, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "divergence_df_test.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "total_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_seq, X_nonseq, y in test_loader:\n",
    "        X_seq = X_seq.to(device)\n",
    "        X_nonseq = X_nonseq.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = model(X_seq, X_nonseq)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        total_loss += loss.item() * X_seq.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_preds.sort()\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.dataset.lstm_dataset import LSTMDivergenceDataset, create_divergence_sequence\n",
    "from src.model.transformer import TransformerMixedModel\n",
    "from src.training.train_lstm import train_model, evaluate_model, plot_results, model_naming\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(os.path.join(PROJECT_PATH, 'data', 'processed_data', 'training_data.pickle'))\n",
    "divergence_data = pd.read_pickle(os.path.join(PROJECT_PATH, 'data', 'processed_data', 'divergence_data.pickle'))\n",
    "\n",
    "# Filter 5-minute timeframe data\n",
    "# price_df = df[df['timeframe'] == '5m'].copy()\n",
    "price_df = pd.read_pickle(f\"{PROJECT_PATH}/data/training_data/price_df.pickle\")\n",
    "divergence_df = divergence_data['15m'].copy()  # Assuming '5m' key exists\n",
    "\n",
    "# Split divergence_df into train/val/test\n",
    "total_events = len(divergence_df)\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.2\n",
    "test_ratio = 0.1\n",
    "\n",
    "train_count = int(total_events * train_ratio)\n",
    "val_count = int(total_events * val_ratio)\n",
    "test_count = total_events - train_count - val_count\n",
    "\n",
    "divergence_df_train = df_divergence.iloc[:train_count]\n",
    "divergence_df_val = df_divergence.iloc[train_count:train_count+val_count]\n",
    "divergence_df_test = df_divergence.iloc[train_count+val_count:]\n",
    "\n",
    "\n",
    "logger.info(f\"Train events: {len(divergence_df_train)}, \"\n",
    "            f\"Validation events: {len(divergence_df_val)}, \"\n",
    "            f\"Test events: {len(divergence_df_test)}\")\n",
    "\n",
    "# Prepare divergence_data for multiple timeframes (if applicable)\n",
    "# Assuming divergence_data contains multiple timeframes\n",
    "# Example: divergence_data = {'5m': ddf_5m, '15m': ddf_15m, '1h': ddf_1h, ...}\n",
    "# For simplicity, using only '5m' here\n",
    "divergence_data_subset = {'15m': divergence_df_train}\n",
    "\n",
    "# Initialize Dataset\n",
    "logger.info(\"Initializing datasets...\")\n",
    "train_dataset = LSTMDivergenceDataset(divergence_df=divergence_df_train, \n",
    "                                        price_df=price_df, \n",
    "                                        divergence_data=divergence_data, \n",
    "                                        seq_length=288)  # 288 * 5min = 24 hours\n",
    "# Use the same scaler for validation and test\n",
    "scaler = train_dataset.scaler\n",
    "val_dataset = LSTMDivergenceDataset(divergence_df=divergence_df_val, \n",
    "                                    price_df=price_df, \n",
    "                                    divergence_data=divergence_data, \n",
    "                                    seq_length=288, \n",
    "                                    scaler=scaler)\n",
    "\n",
    "test_dataset = LSTMDivergenceDataset(divergence_df=divergence_df_test, \n",
    "                                    price_df=price_df, \n",
    "                                    divergence_data=divergence_data, \n",
    "                                    seq_length=288, \n",
    "                                    scaler=scaler)\n",
    "\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 14:45:19,260 INFO Using device: cpu\n",
      "/Users/shawn/Library/Caches/pypoetry/virtualenvs/rsi-divergence-detector-MNiR8ro_-py3.10/lib/python3.10/site-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "2024-12-12 14:45:19,274 INFO Transformer Model initialized with args: {'seq_input_dim': 29, 'seq_model_dim': 128, 'seq_num_heads': 8, 'seq_num_layers': 3, 'nonseq_input_dim': 6, 'mlp_hidden_dim': 256, 'num_classes': 2, 'dropout': 0.3, 'max_len': 5000}\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize Transformer Model\n",
    "transformer_model_args = {\n",
    "    \"seq_input_dim\": len(train_dataset.ts_cols),\n",
    "    \"seq_model_dim\": 256,          # You can adjust this as needed\n",
    "    \"seq_num_heads\": 8,            # Number of attention heads\n",
    "    \"seq_num_layers\": 6,           # Number of Transformer layers\n",
    "    \"nonseq_input_dim\": len(train_dataset.nonseq_cols),\n",
    "    \"mlp_hidden_dim\": 512,\n",
    "    \"num_classes\": 2,\n",
    "    \"dropout\": 0.3,\n",
    "    \"max_len\": 5000                 # Maximum sequence length\n",
    "}\n",
    "\n",
    "\n",
    "transformer_model = TransformerMixedModel(**transformer_model_args)\n",
    "logger.info(f\"Transformer Model initialized with args: {transformer_model_args}\")\n",
    "\n",
    "# Generate model name\n",
    "transformer_model_name = model_naming(**transformer_model_args)\n",
    "transformer_model_save_dir = os.path.join(PROJECT_PATH, 'model_data', 'transformer_mixed')\n",
    "os.makedirs(transformer_model_save_dir, exist_ok=True)\n",
    "transformer_model_save_path = os.path.join(transformer_model_save_dir, transformer_model_name)\n",
    "\n",
    "# Calculate class weights for imbalanced loss\n",
    "classes = np.unique(divergence_df_train['label'])\n",
    "class_weights = class_weight.compute_class_weight('balanced', classes=classes, y=divergence_df_train['label'])\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "logger.info(f\"Class weights: {class_weights}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 15:58:56,504 INFO Starting training of the Transformer model...\n",
      "2024-12-12 15:59:10,592 INFO Epoch [1/500], Batch [10/90], Loss: 102.2178\n",
      "2024-12-12 15:59:24,129 INFO Epoch [1/500], Batch [20/90], Loss: 64.6928\n",
      "2024-12-12 15:59:37,866 INFO Epoch [1/500], Batch [30/90], Loss: 80.9774\n",
      "2024-12-12 15:59:52,420 INFO Epoch [1/500], Batch [40/90], Loss: 47.0456\n",
      "2024-12-12 16:00:07,321 INFO Epoch [1/500], Batch [50/90], Loss: 35.3128\n",
      "2024-12-12 16:00:21,375 INFO Epoch [1/500], Batch [60/90], Loss: 42.2994\n",
      "2024-12-12 16:00:34,911 INFO Epoch [1/500], Batch [70/90], Loss: 22.5758\n",
      "2024-12-12 16:00:48,721 INFO Epoch [1/500], Batch [80/90], Loss: 24.9593\n",
      "2024-12-12 16:01:03,296 INFO Epoch [1/500], Batch [90/90], Loss: 13.3111\n",
      "2024-12-12 16:01:07,934 INFO Epoch [1/500] - Train Loss: 45.9228, Train Acc: 0.6428 - Val Loss: 40.8713, Val Acc: 0.7036\n",
      "2024-12-12 16:01:07,943 INFO Best model saved with Val Acc: 0.7036\n",
      "2024-12-12 16:01:21,559 INFO Epoch [2/500], Batch [10/90], Loss: 10.7808\n",
      "2024-12-12 16:01:34,977 INFO Epoch [2/500], Batch [20/90], Loss: 11.9363\n",
      "2024-12-12 16:01:48,453 INFO Epoch [2/500], Batch [30/90], Loss: 11.0095\n",
      "2024-12-12 16:02:02,101 INFO Epoch [2/500], Batch [40/90], Loss: 11.1737\n",
      "2024-12-12 16:02:16,144 INFO Epoch [2/500], Batch [50/90], Loss: 10.3549\n",
      "2024-12-12 16:02:30,613 INFO Epoch [2/500], Batch [60/90], Loss: 3.1512\n",
      "2024-12-12 16:02:44,507 INFO Epoch [2/500], Batch [70/90], Loss: 3.0492\n",
      "2024-12-12 16:02:58,373 INFO Epoch [2/500], Batch [80/90], Loss: 3.8021\n",
      "2024-12-12 16:03:12,799 INFO Epoch [2/500], Batch [90/90], Loss: 4.4895\n",
      "2024-12-12 16:03:18,617 INFO Epoch [2/500] - Train Loss: 9.5824, Train Acc: 0.6301 - Val Loss: 17.2472, Val Acc: 0.7036\n",
      "2024-12-12 16:03:32,553 INFO Epoch [3/500], Batch [10/90], Loss: 3.8340\n",
      "2024-12-12 16:03:46,418 INFO Epoch [3/500], Batch [20/90], Loss: 7.1473\n",
      "2024-12-12 16:04:00,235 INFO Epoch [3/500], Batch [30/90], Loss: 6.7345\n",
      "2024-12-12 16:04:14,063 INFO Epoch [3/500], Batch [40/90], Loss: 2.6966\n",
      "2024-12-12 16:04:28,007 INFO Epoch [3/500], Batch [50/90], Loss: 2.6662\n",
      "2024-12-12 16:04:41,881 INFO Epoch [3/500], Batch [60/90], Loss: 2.4723\n",
      "2024-12-12 16:04:55,746 INFO Epoch [3/500], Batch [70/90], Loss: 1.6957\n",
      "2024-12-12 16:05:09,666 INFO Epoch [3/500], Batch [80/90], Loss: 2.1651\n",
      "2024-12-12 16:05:23,635 INFO Epoch [3/500], Batch [90/90], Loss: 3.7331\n",
      "2024-12-12 16:05:28,057 INFO Epoch [3/500] - Train Loss: 4.3675, Train Acc: 0.6311 - Val Loss: 0.8199, Val Acc: 0.4946\n",
      "2024-12-12 16:05:41,880 INFO Epoch [4/500], Batch [10/90], Loss: 1.0460\n",
      "2024-12-12 16:05:55,583 INFO Epoch [4/500], Batch [20/90], Loss: 3.2647\n",
      "2024-12-12 16:06:09,330 INFO Epoch [4/500], Batch [30/90], Loss: 3.2535\n",
      "2024-12-12 16:06:23,311 INFO Epoch [4/500], Batch [40/90], Loss: 0.8870\n",
      "2024-12-12 16:06:37,236 INFO Epoch [4/500], Batch [50/90], Loss: 2.1885\n",
      "2024-12-12 16:06:50,983 INFO Epoch [4/500], Batch [60/90], Loss: 3.0319\n",
      "2024-12-12 16:07:04,653 INFO Epoch [4/500], Batch [70/90], Loss: 1.8047\n",
      "2024-12-12 16:07:18,463 INFO Epoch [4/500], Batch [80/90], Loss: 3.1551\n",
      "2024-12-12 16:07:32,357 INFO Epoch [4/500], Batch [90/90], Loss: 1.0931\n",
      "2024-12-12 16:07:36,650 INFO Epoch [4/500] - Train Loss: 2.2697, Train Acc: 0.6339 - Val Loss: 3.9686, Val Acc: 0.2964\n",
      "2024-12-12 16:07:50,396 INFO Epoch [5/500], Batch [10/90], Loss: 2.9831\n",
      "2024-12-12 16:08:04,155 INFO Epoch [5/500], Batch [20/90], Loss: 2.4806\n",
      "2024-12-12 16:08:17,907 INFO Epoch [5/500], Batch [30/90], Loss: 3.1551\n",
      "2024-12-12 16:08:31,799 INFO Epoch [5/500], Batch [40/90], Loss: 2.3536\n",
      "2024-12-12 16:08:45,647 INFO Epoch [5/500], Batch [50/90], Loss: 1.4210\n",
      "2024-12-12 16:08:59,455 INFO Epoch [5/500], Batch [60/90], Loss: 2.6805\n",
      "2024-12-12 16:09:14,033 INFO Epoch [5/500], Batch [70/90], Loss: 0.9849\n",
      "2024-12-12 16:09:28,157 INFO Epoch [5/500], Batch [80/90], Loss: 1.3773\n",
      "2024-12-12 16:09:41,369 INFO Epoch [5/500], Batch [90/90], Loss: 3.5117\n",
      "2024-12-12 16:09:45,489 INFO Epoch [5/500] - Train Loss: 2.2793, Train Acc: 0.6371 - Val Loss: 4.4183, Val Acc: 0.7036\n",
      "2024-12-12 16:09:58,590 INFO Epoch [6/500], Batch [10/90], Loss: 2.3163\n",
      "2024-12-12 16:10:11,627 INFO Epoch [6/500], Batch [20/90], Loss: 1.6381\n",
      "2024-12-12 16:10:24,731 INFO Epoch [6/500], Batch [30/90], Loss: 0.7743\n",
      "2024-12-12 16:10:37,821 INFO Epoch [6/500], Batch [40/90], Loss: 1.2978\n",
      "2024-12-12 16:10:50,904 INFO Epoch [6/500], Batch [50/90], Loss: 1.2526\n",
      "2024-12-12 16:11:03,880 INFO Epoch [6/500], Batch [60/90], Loss: 1.6970\n",
      "2024-12-12 16:11:16,915 INFO Epoch [6/500], Batch [70/90], Loss: 1.6838\n",
      "2024-12-12 16:11:30,018 INFO Epoch [6/500], Batch [80/90], Loss: 2.6872\n",
      "2024-12-12 16:11:43,008 INFO Epoch [6/500], Batch [90/90], Loss: 5.5257\n",
      "2024-12-12 16:11:46,976 INFO Epoch [6/500] - Train Loss: 1.5352, Train Acc: 0.6449 - Val Loss: 10.0439, Val Acc: 0.7036\n",
      "2024-12-12 16:12:00,046 INFO Epoch [7/500], Batch [10/90], Loss: 3.3631\n",
      "2024-12-12 16:12:13,071 INFO Epoch [7/500], Batch [20/90], Loss: 1.6465\n",
      "2024-12-12 16:12:26,127 INFO Epoch [7/500], Batch [30/90], Loss: 1.3719\n",
      "2024-12-12 16:12:39,635 INFO Epoch [7/500], Batch [40/90], Loss: 0.4545\n",
      "2024-12-12 16:12:52,794 INFO Epoch [7/500], Batch [50/90], Loss: 1.7153\n",
      "2024-12-12 16:13:05,844 INFO Epoch [7/500], Batch [60/90], Loss: 0.6588\n",
      "2024-12-12 16:13:19,054 INFO Epoch [7/500], Batch [70/90], Loss: 0.8147\n",
      "2024-12-12 16:13:32,822 INFO Epoch [7/500], Batch [80/90], Loss: 1.2727\n",
      "2024-12-12 16:13:46,751 INFO Epoch [7/500], Batch [90/90], Loss: 1.0985\n",
      "2024-12-12 16:13:51,396 INFO Epoch [7/500] - Train Loss: 1.3781, Train Acc: 0.6499 - Val Loss: 0.6627, Val Acc: 0.7036\n",
      "2024-12-12 16:14:05,380 INFO Epoch [8/500], Batch [10/90], Loss: 0.9114\n",
      "2024-12-12 16:14:19,253 INFO Epoch [8/500], Batch [20/90], Loss: 1.5996\n",
      "2024-12-12 16:14:33,083 INFO Epoch [8/500], Batch [30/90], Loss: 0.7758\n",
      "2024-12-12 16:14:46,888 INFO Epoch [8/500], Batch [40/90], Loss: 0.4676\n",
      "2024-12-12 16:15:00,876 INFO Epoch [8/500], Batch [50/90], Loss: 0.6045\n",
      "2024-12-12 16:15:14,780 INFO Epoch [8/500], Batch [60/90], Loss: 0.5926\n",
      "2024-12-12 16:15:28,593 INFO Epoch [8/500], Batch [70/90], Loss: 0.8565\n",
      "2024-12-12 16:15:42,734 INFO Epoch [8/500], Batch [80/90], Loss: 0.8249\n",
      "2024-12-12 16:15:56,656 INFO Epoch [8/500], Batch [90/90], Loss: 1.1915\n",
      "2024-12-12 16:16:01,029 INFO Epoch [8/500] - Train Loss: 0.8059, Train Acc: 0.6820 - Val Loss: 1.2125, Val Acc: 0.7036\n",
      "2024-12-12 16:16:14,682 INFO Epoch [9/500], Batch [10/90], Loss: 0.9732\n",
      "2024-12-12 16:16:28,426 INFO Epoch [9/500], Batch [20/90], Loss: 0.9842\n",
      "2024-12-12 16:16:42,323 INFO Epoch [9/500], Batch [30/90], Loss: 2.2365\n",
      "2024-12-12 16:16:56,178 INFO Epoch [9/500], Batch [40/90], Loss: 0.9672\n",
      "2024-12-12 16:17:09,802 INFO Epoch [9/500], Batch [50/90], Loss: 0.4310\n",
      "2024-12-12 16:17:23,315 INFO Epoch [9/500], Batch [60/90], Loss: 0.9008\n",
      "2024-12-12 16:17:37,146 INFO Epoch [9/500], Batch [70/90], Loss: 0.6354\n",
      "2024-12-12 16:17:51,070 INFO Epoch [9/500], Batch [80/90], Loss: 0.6867\n",
      "2024-12-12 16:18:04,897 INFO Epoch [9/500], Batch [90/90], Loss: 1.1756\n",
      "2024-12-12 16:18:09,282 INFO Epoch [9/500] - Train Loss: 0.9258, Train Acc: 0.6939 - Val Loss: 0.6301, Val Acc: 0.7030\n",
      "2024-12-12 16:18:22,819 INFO Epoch [10/500], Batch [10/90], Loss: 0.9936\n",
      "2024-12-12 16:18:36,576 INFO Epoch [10/500], Batch [20/90], Loss: 1.1815\n",
      "2024-12-12 16:18:50,502 INFO Epoch [10/500], Batch [30/90], Loss: 0.6519\n",
      "2024-12-12 16:19:04,434 INFO Epoch [10/500], Batch [40/90], Loss: 0.8125\n",
      "2024-12-12 16:19:18,311 INFO Epoch [10/500], Batch [50/90], Loss: 0.5888\n",
      "2024-12-12 16:19:32,179 INFO Epoch [10/500], Batch [60/90], Loss: 0.6671\n",
      "2024-12-12 16:19:46,099 INFO Epoch [10/500], Batch [70/90], Loss: 0.5681\n",
      "2024-12-12 16:19:59,959 INFO Epoch [10/500], Batch [80/90], Loss: 0.6114\n",
      "2024-12-12 16:20:13,749 INFO Epoch [10/500], Batch [90/90], Loss: 0.9153\n",
      "2024-12-12 16:20:18,232 INFO Epoch [10/500] - Train Loss: 0.7422, Train Acc: 0.7025 - Val Loss: 0.6083, Val Acc: 0.7036\n",
      "2024-12-12 16:20:32,197 INFO Epoch [11/500], Batch [10/90], Loss: 0.7328\n",
      "2024-12-12 16:20:46,143 INFO Epoch [11/500], Batch [20/90], Loss: 0.6251\n",
      "2024-12-12 16:21:00,007 INFO Epoch [11/500], Batch [30/90], Loss: 0.7352\n",
      "2024-12-12 16:21:13,887 INFO Epoch [11/500], Batch [40/90], Loss: 0.6495\n",
      "2024-12-12 16:21:28,126 INFO Epoch [11/500], Batch [50/90], Loss: 0.5187\n",
      "2024-12-12 16:21:42,048 INFO Epoch [11/500], Batch [60/90], Loss: 0.5444\n",
      "2024-12-12 16:21:55,907 INFO Epoch [11/500], Batch [70/90], Loss: 0.5181\n",
      "2024-12-12 16:22:09,852 INFO Epoch [11/500], Batch [80/90], Loss: 0.6220\n",
      "2024-12-12 16:22:23,743 INFO Epoch [11/500], Batch [90/90], Loss: 0.7782\n",
      "2024-12-12 16:22:28,217 INFO Epoch [11/500] - Train Loss: 0.6302, Train Acc: 0.7300 - Val Loss: 0.6144, Val Acc: 0.6946\n",
      "2024-12-12 16:22:42,116 INFO Epoch [12/500], Batch [10/90], Loss: 0.9222\n",
      "2024-12-12 16:22:56,002 INFO Epoch [12/500], Batch [20/90], Loss: 0.6722\n",
      "2024-12-12 16:23:10,011 INFO Epoch [12/500], Batch [30/90], Loss: 0.5823\n",
      "2024-12-12 16:23:23,952 INFO Epoch [12/500], Batch [40/90], Loss: 0.5532\n",
      "2024-12-12 16:23:37,810 INFO Epoch [12/500], Batch [50/90], Loss: 0.6763\n",
      "2024-12-12 16:23:51,900 INFO Epoch [12/500], Batch [60/90], Loss: 0.6318\n",
      "2024-12-12 16:24:05,726 INFO Epoch [12/500], Batch [70/90], Loss: 0.5144\n",
      "2024-12-12 16:24:19,687 INFO Epoch [12/500], Batch [80/90], Loss: 0.5411\n",
      "2024-12-12 16:24:33,543 INFO Epoch [12/500], Batch [90/90], Loss: 0.5926\n",
      "2024-12-12 16:24:38,021 INFO Epoch [12/500] - Train Loss: 0.6371, Train Acc: 0.7233 - Val Loss: 0.6065, Val Acc: 0.7036\n",
      "2024-12-12 16:24:51,926 INFO Epoch [13/500], Batch [10/90], Loss: 0.4319\n",
      "2024-12-12 16:25:05,942 INFO Epoch [13/500], Batch [20/90], Loss: 0.5675\n",
      "2024-12-12 16:25:19,939 INFO Epoch [13/500], Batch [30/90], Loss: 0.4871\n",
      "2024-12-12 16:25:33,851 INFO Epoch [13/500], Batch [40/90], Loss: 0.6303\n",
      "2024-12-12 16:25:47,719 INFO Epoch [13/500], Batch [50/90], Loss: 0.6797\n",
      "2024-12-12 16:26:01,492 INFO Epoch [13/500], Batch [60/90], Loss: 0.5414\n",
      "2024-12-12 16:26:15,312 INFO Epoch [13/500], Batch [70/90], Loss: 0.6947\n",
      "2024-12-12 16:26:29,198 INFO Epoch [13/500], Batch [80/90], Loss: 0.7554\n",
      "2024-12-12 16:26:43,964 INFO Epoch [13/500], Batch [90/90], Loss: 0.6043\n",
      "2024-12-12 16:26:48,920 INFO Epoch [13/500] - Train Loss: 0.6395, Train Acc: 0.7226 - Val Loss: 0.6609, Val Acc: 0.7036\n",
      "2024-12-12 16:27:03,744 INFO Epoch [14/500], Batch [10/90], Loss: 0.5541\n",
      "2024-12-12 16:27:17,944 INFO Epoch [14/500], Batch [20/90], Loss: 0.6834\n",
      "2024-12-12 16:27:31,909 INFO Epoch [14/500], Batch [30/90], Loss: 0.5927\n",
      "2024-12-12 16:27:45,696 INFO Epoch [14/500], Batch [40/90], Loss: 0.6214\n",
      "2024-12-12 16:27:59,648 INFO Epoch [14/500], Batch [50/90], Loss: 0.6682\n",
      "2024-12-12 16:28:13,647 INFO Epoch [14/500], Batch [60/90], Loss: 0.5726\n",
      "2024-12-12 16:28:27,509 INFO Epoch [14/500], Batch [70/90], Loss: 0.6742\n",
      "2024-12-12 16:28:41,333 INFO Epoch [14/500], Batch [80/90], Loss: 0.7418\n",
      "2024-12-12 16:28:55,155 INFO Epoch [14/500], Batch [90/90], Loss: 0.5945\n",
      "2024-12-12 16:28:59,685 INFO Epoch [14/500] - Train Loss: 0.6078, Train Acc: 0.7397 - Val Loss: 0.6571, Val Acc: 0.6729\n",
      "2024-12-12 16:29:13,581 INFO Epoch [15/500], Batch [10/90], Loss: 0.7061\n",
      "2024-12-12 16:29:27,433 INFO Epoch [15/500], Batch [20/90], Loss: 0.5256\n",
      "2024-12-12 16:29:41,293 INFO Epoch [15/500], Batch [30/90], Loss: 0.8608\n",
      "2024-12-12 16:29:55,151 INFO Epoch [15/500], Batch [40/90], Loss: 0.7112\n",
      "2024-12-12 16:30:08,978 INFO Epoch [15/500], Batch [50/90], Loss: 0.5983\n",
      "2024-12-12 16:30:23,035 INFO Epoch [15/500], Batch [60/90], Loss: 0.6807\n",
      "2024-12-12 16:30:37,018 INFO Epoch [15/500], Batch [70/90], Loss: 0.5218\n",
      "2024-12-12 16:30:50,973 INFO Epoch [15/500], Batch [80/90], Loss: 0.5256\n",
      "2024-12-12 16:31:04,902 INFO Epoch [15/500], Batch [90/90], Loss: 0.4968\n",
      "2024-12-12 16:31:09,366 INFO Epoch [15/500] - Train Loss: 0.5951, Train Acc: 0.7429 - Val Loss: 0.6759, Val Acc: 0.7036\n",
      "2024-12-12 16:31:23,255 INFO Epoch [16/500], Batch [10/90], Loss: 0.6314\n",
      "2024-12-12 16:31:37,138 INFO Epoch [16/500], Batch [20/90], Loss: 0.6027\n",
      "2024-12-12 16:31:51,080 INFO Epoch [16/500], Batch [30/90], Loss: 0.5853\n",
      "2024-12-12 16:32:05,320 INFO Epoch [16/500], Batch [40/90], Loss: 0.5448\n",
      "2024-12-12 16:32:19,176 INFO Epoch [16/500], Batch [50/90], Loss: 0.5746\n",
      "2024-12-12 16:32:33,123 INFO Epoch [16/500], Batch [60/90], Loss: 0.6191\n",
      "2024-12-12 16:32:46,952 INFO Epoch [16/500], Batch [70/90], Loss: 0.6841\n",
      "2024-12-12 16:33:01,349 INFO Epoch [16/500], Batch [80/90], Loss: 0.7758\n",
      "2024-12-12 16:33:15,463 INFO Epoch [16/500], Batch [90/90], Loss: 0.5795\n",
      "2024-12-12 16:33:20,468 INFO Epoch [16/500] - Train Loss: 0.5868, Train Acc: 0.7440 - Val Loss: 0.6364, Val Acc: 0.7036\n",
      "2024-12-12 16:33:34,669 INFO Epoch [17/500], Batch [10/90], Loss: 0.5581\n",
      "2024-12-12 16:33:48,797 INFO Epoch [17/500], Batch [20/90], Loss: 0.5563\n",
      "2024-12-12 16:34:02,912 INFO Epoch [17/500], Batch [30/90], Loss: 0.5675\n",
      "2024-12-12 16:34:17,330 INFO Epoch [17/500], Batch [40/90], Loss: 0.5561\n",
      "2024-12-12 16:34:31,542 INFO Epoch [17/500], Batch [50/90], Loss: 0.5555\n",
      "2024-12-12 16:34:45,928 INFO Epoch [17/500], Batch [60/90], Loss: 0.5326\n",
      "2024-12-12 16:34:59,991 INFO Epoch [17/500], Batch [70/90], Loss: 0.4388\n",
      "2024-12-12 16:35:13,937 INFO Epoch [17/500], Batch [80/90], Loss: 0.5167\n",
      "2024-12-12 16:35:28,076 INFO Epoch [17/500], Batch [90/90], Loss: 0.5042\n",
      "2024-12-12 16:35:33,077 INFO Epoch [17/500] - Train Loss: 0.5558, Train Acc: 0.7529 - Val Loss: 0.5967, Val Acc: 0.7036\n",
      "2024-12-12 16:35:47,502 INFO Epoch [18/500], Batch [10/90], Loss: 0.5796\n",
      "2024-12-12 16:36:01,981 INFO Epoch [18/500], Batch [20/90], Loss: 0.5773\n",
      "2024-12-12 16:36:15,873 INFO Epoch [18/500], Batch [30/90], Loss: 0.5136\n",
      "2024-12-12 16:36:30,088 INFO Epoch [18/500], Batch [40/90], Loss: 0.5046\n",
      "2024-12-12 16:36:46,649 INFO Epoch [18/500], Batch [50/90], Loss: 0.5181\n",
      "2024-12-12 16:37:01,099 INFO Epoch [18/500], Batch [60/90], Loss: 0.4900\n",
      "2024-12-12 16:37:15,150 INFO Epoch [18/500], Batch [70/90], Loss: 0.6436\n",
      "2024-12-12 16:37:28,901 INFO Epoch [18/500], Batch [80/90], Loss: 0.4576\n",
      "2024-12-12 16:37:42,687 INFO Epoch [18/500], Batch [90/90], Loss: 0.5611\n",
      "2024-12-12 16:37:47,047 INFO Epoch [18/500] - Train Loss: 0.5485, Train Acc: 0.7546 - Val Loss: 0.6186, Val Acc: 0.7036\n",
      "2024-12-12 16:38:00,877 INFO Epoch [19/500], Batch [10/90], Loss: 0.4812\n",
      "2024-12-12 16:38:14,636 INFO Epoch [19/500], Batch [20/90], Loss: 0.5609\n",
      "2024-12-12 16:38:28,388 INFO Epoch [19/500], Batch [30/90], Loss: 0.5703\n",
      "2024-12-12 16:38:42,116 INFO Epoch [19/500], Batch [40/90], Loss: 0.4962\n",
      "2024-12-12 16:38:56,199 INFO Epoch [19/500], Batch [50/90], Loss: 0.5907\n",
      "2024-12-12 16:39:10,128 INFO Epoch [19/500], Batch [60/90], Loss: 0.5705\n",
      "2024-12-12 16:39:23,966 INFO Epoch [19/500], Batch [70/90], Loss: 0.5687\n",
      "2024-12-12 16:39:37,742 INFO Epoch [19/500], Batch [80/90], Loss: 0.5023\n",
      "2024-12-12 16:39:51,478 INFO Epoch [19/500], Batch [90/90], Loss: 0.5397\n",
      "2024-12-12 16:39:55,961 INFO Epoch [19/500] - Train Loss: 0.5504, Train Acc: 0.7543 - Val Loss: 0.5944, Val Acc: 0.7036\n",
      "2024-12-12 16:40:09,763 INFO Epoch [20/500], Batch [10/90], Loss: 0.5906\n",
      "2024-12-12 16:40:23,551 INFO Epoch [20/500], Batch [20/90], Loss: 0.5525\n",
      "2024-12-12 16:40:37,340 INFO Epoch [20/500], Batch [30/90], Loss: 0.5781\n",
      "2024-12-12 16:40:51,172 INFO Epoch [20/500], Batch [40/90], Loss: 0.6637\n",
      "2024-12-12 16:41:04,950 INFO Epoch [20/500], Batch [50/90], Loss: 0.5221\n",
      "2024-12-12 16:41:18,765 INFO Epoch [20/500], Batch [60/90], Loss: 0.6759\n",
      "2024-12-12 16:41:32,470 INFO Epoch [20/500], Batch [70/90], Loss: 0.5733\n",
      "2024-12-12 16:41:46,173 INFO Epoch [20/500], Batch [80/90], Loss: 0.5926\n",
      "2024-12-12 16:42:01,032 INFO Epoch [20/500], Batch [90/90], Loss: 0.5225\n",
      "2024-12-12 16:42:06,069 INFO Epoch [20/500] - Train Loss: 0.5486, Train Acc: 0.7541 - Val Loss: 0.5943, Val Acc: 0.7036\n",
      "2024-12-12 16:42:20,109 INFO Epoch [21/500], Batch [10/90], Loss: 0.5074\n",
      "2024-12-12 16:42:34,694 INFO Epoch [21/500], Batch [20/90], Loss: 0.5709\n",
      "2024-12-12 16:42:48,942 INFO Epoch [21/500], Batch [30/90], Loss: 0.5934\n",
      "2024-12-12 16:43:03,653 INFO Epoch [21/500], Batch [40/90], Loss: 0.5127\n",
      "2024-12-12 16:43:18,039 INFO Epoch [21/500], Batch [50/90], Loss: 0.5636\n",
      "2024-12-12 16:43:32,323 INFO Epoch [21/500], Batch [60/90], Loss: 0.4943\n",
      "2024-12-12 16:43:46,821 INFO Epoch [21/500], Batch [70/90], Loss: 0.5361\n",
      "2024-12-12 16:44:00,837 INFO Epoch [21/500], Batch [80/90], Loss: 0.6126\n",
      "2024-12-12 16:44:14,938 INFO Epoch [21/500], Batch [90/90], Loss: 0.5690\n",
      "2024-12-12 16:44:19,612 INFO Epoch [21/500] - Train Loss: 0.5453, Train Acc: 0.7545 - Val Loss: 0.5955, Val Acc: 0.7036\n",
      "2024-12-12 16:44:33,737 INFO Epoch [22/500], Batch [10/90], Loss: 0.6046\n",
      "2024-12-12 16:44:47,725 INFO Epoch [22/500], Batch [20/90], Loss: 0.5351\n",
      "2024-12-12 16:45:01,956 INFO Epoch [22/500], Batch [30/90], Loss: 0.5846\n",
      "2024-12-12 16:45:16,552 INFO Epoch [22/500], Batch [40/90], Loss: 0.5837\n",
      "2024-12-12 16:45:30,800 INFO Epoch [22/500], Batch [50/90], Loss: 0.5193\n",
      "2024-12-12 16:45:44,653 INFO Epoch [22/500], Batch [60/90], Loss: 0.5486\n",
      "2024-12-12 16:45:58,734 INFO Epoch [22/500], Batch [70/90], Loss: 0.5259\n",
      "2024-12-12 16:46:13,180 INFO Epoch [22/500], Batch [80/90], Loss: 0.5606\n",
      "2024-12-12 16:46:27,120 INFO Epoch [22/500], Batch [90/90], Loss: 0.5621\n",
      "2024-12-12 16:46:32,058 INFO Epoch [22/500] - Train Loss: 0.5500, Train Acc: 0.7538 - Val Loss: 0.5944, Val Acc: 0.7036\n",
      "2024-12-12 16:46:46,463 INFO Epoch [23/500], Batch [10/90], Loss: 0.4836\n",
      "2024-12-12 16:47:00,338 INFO Epoch [23/500], Batch [20/90], Loss: 0.5520\n",
      "2024-12-12 16:47:14,060 INFO Epoch [23/500], Batch [30/90], Loss: 0.7596\n",
      "2024-12-12 16:47:27,551 INFO Epoch [23/500], Batch [40/90], Loss: 0.5163\n",
      "2024-12-12 16:47:41,034 INFO Epoch [23/500], Batch [50/90], Loss: 0.4932\n",
      "2024-12-12 16:47:54,742 INFO Epoch [23/500], Batch [60/90], Loss: 0.5009\n",
      "2024-12-12 16:48:08,332 INFO Epoch [23/500], Batch [70/90], Loss: 0.5240\n",
      "2024-12-12 16:48:21,954 INFO Epoch [23/500], Batch [80/90], Loss: 0.5699\n",
      "2024-12-12 16:48:35,385 INFO Epoch [23/500], Batch [90/90], Loss: 0.6095\n",
      "2024-12-12 16:48:39,925 INFO Epoch [23/500] - Train Loss: 0.5464, Train Acc: 0.7536 - Val Loss: 0.5938, Val Acc: 0.7036\n",
      "2024-12-12 16:48:53,583 INFO Epoch [24/500], Batch [10/90], Loss: 0.5216\n",
      "2024-12-12 16:49:07,213 INFO Epoch [24/500], Batch [20/90], Loss: 0.5611\n",
      "2024-12-12 16:49:21,189 INFO Epoch [24/500], Batch [30/90], Loss: 0.6436\n",
      "2024-12-12 16:49:36,973 INFO Epoch [24/500], Batch [40/90], Loss: 0.5645\n",
      "2024-12-12 16:49:51,343 INFO Epoch [24/500], Batch [50/90], Loss: 0.5749\n",
      "2024-12-12 16:50:05,477 INFO Epoch [24/500], Batch [60/90], Loss: 0.5870\n",
      "2024-12-12 16:50:19,246 INFO Epoch [24/500], Batch [70/90], Loss: 0.5084\n",
      "2024-12-12 16:50:33,068 INFO Epoch [24/500], Batch [80/90], Loss: 0.6360\n",
      "2024-12-12 16:50:46,849 INFO Epoch [24/500], Batch [90/90], Loss: 0.6663\n",
      "2024-12-12 16:50:52,185 INFO Epoch [24/500] - Train Loss: 0.5466, Train Acc: 0.7543 - Val Loss: 0.5933, Val Acc: 0.7036\n",
      "2024-12-12 16:51:06,019 INFO Epoch [25/500], Batch [10/90], Loss: 0.5020\n",
      "2024-12-12 16:51:19,778 INFO Epoch [25/500], Batch [20/90], Loss: 0.4830\n",
      "2024-12-12 16:51:33,359 INFO Epoch [25/500], Batch [30/90], Loss: 0.4640\n",
      "2024-12-12 16:51:47,079 INFO Epoch [25/500], Batch [40/90], Loss: 0.5269\n",
      "2024-12-12 16:52:00,934 INFO Epoch [25/500], Batch [50/90], Loss: 0.4758\n",
      "2024-12-12 16:52:14,590 INFO Epoch [25/500], Batch [60/90], Loss: 0.4749\n",
      "2024-12-12 16:52:28,389 INFO Epoch [25/500], Batch [70/90], Loss: 0.5419\n",
      "2024-12-12 16:52:42,501 INFO Epoch [25/500], Batch [80/90], Loss: 0.5576\n",
      "2024-12-12 16:52:56,443 INFO Epoch [25/500], Batch [90/90], Loss: 0.4668\n",
      "2024-12-12 16:53:01,522 INFO Epoch [25/500] - Train Loss: 0.5469, Train Acc: 0.7534 - Val Loss: 0.5961, Val Acc: 0.7036\n",
      "2024-12-12 16:53:15,746 INFO Epoch [26/500], Batch [10/90], Loss: 0.4603\n",
      "2024-12-12 16:53:29,306 INFO Epoch [26/500], Batch [20/90], Loss: 0.5068\n",
      "2024-12-12 16:53:42,743 INFO Epoch [26/500], Batch [30/90], Loss: 0.6164\n",
      "2024-12-12 16:53:56,149 INFO Epoch [26/500], Batch [40/90], Loss: 0.5515\n",
      "2024-12-12 16:54:09,468 INFO Epoch [26/500], Batch [50/90], Loss: 0.6317\n",
      "2024-12-12 16:54:22,865 INFO Epoch [26/500], Batch [60/90], Loss: 0.4758\n",
      "2024-12-12 16:54:36,184 INFO Epoch [26/500], Batch [70/90], Loss: 0.4988\n",
      "2024-12-12 16:54:49,573 INFO Epoch [26/500], Batch [80/90], Loss: 0.5666\n",
      "2024-12-12 16:55:03,258 INFO Epoch [26/500], Batch [90/90], Loss: 0.5039\n",
      "2024-12-12 16:55:07,768 INFO Epoch [26/500] - Train Loss: 0.5453, Train Acc: 0.7550 - Val Loss: 0.5954, Val Acc: 0.7036\n",
      "2024-12-12 16:55:21,029 INFO Epoch [27/500], Batch [10/90], Loss: 0.6995\n",
      "2024-12-12 16:55:34,795 INFO Epoch [27/500], Batch [20/90], Loss: 0.5469\n",
      "2024-12-12 16:55:48,269 INFO Epoch [27/500], Batch [30/90], Loss: 0.5811\n",
      "2024-12-12 16:56:02,104 INFO Epoch [27/500], Batch [40/90], Loss: 0.5260\n",
      "2024-12-12 16:56:15,822 INFO Epoch [27/500], Batch [50/90], Loss: 0.5552\n",
      "2024-12-12 16:56:29,881 INFO Epoch [27/500], Batch [60/90], Loss: 0.5435\n",
      "2024-12-12 16:56:43,955 INFO Epoch [27/500], Batch [70/90], Loss: 0.4756\n",
      "2024-12-12 16:56:57,430 INFO Epoch [27/500], Batch [80/90], Loss: 0.5100\n",
      "2024-12-12 16:57:11,106 INFO Epoch [27/500], Batch [90/90], Loss: 0.6073\n",
      "2024-12-12 16:57:15,655 INFO Epoch [27/500] - Train Loss: 0.5427, Train Acc: 0.7540 - Val Loss: 0.5997, Val Acc: 0.7036\n",
      "2024-12-12 16:57:29,271 INFO Epoch [28/500], Batch [10/90], Loss: 0.5688\n",
      "2024-12-12 16:57:42,904 INFO Epoch [28/500], Batch [20/90], Loss: 0.5966\n",
      "2024-12-12 16:57:56,824 INFO Epoch [28/500], Batch [30/90], Loss: 0.5426\n",
      "2024-12-12 16:58:10,270 INFO Epoch [28/500], Batch [40/90], Loss: 0.4481\n",
      "2024-12-12 16:58:23,910 INFO Epoch [28/500], Batch [50/90], Loss: 0.5129\n",
      "2024-12-12 16:58:37,676 INFO Epoch [28/500], Batch [60/90], Loss: 0.5460\n",
      "2024-12-12 16:58:51,640 INFO Epoch [28/500], Batch [70/90], Loss: 0.5599\n",
      "2024-12-12 16:59:05,455 INFO Epoch [28/500], Batch [80/90], Loss: 0.5955\n",
      "2024-12-12 16:59:19,211 INFO Epoch [28/500], Batch [90/90], Loss: 0.4879\n",
      "2024-12-12 16:59:24,502 INFO Epoch [28/500] - Train Loss: 0.5484, Train Acc: 0.7536 - Val Loss: 0.5980, Val Acc: 0.7036\n",
      "2024-12-12 16:59:38,287 INFO Epoch [29/500], Batch [10/90], Loss: 0.5125\n",
      "2024-12-12 16:59:52,107 INFO Epoch [29/500], Batch [20/90], Loss: 0.4678\n",
      "2024-12-12 17:00:05,867 INFO Epoch [29/500], Batch [30/90], Loss: 0.4914\n",
      "2024-12-12 17:00:19,943 INFO Epoch [29/500], Batch [40/90], Loss: 0.5283\n",
      "2024-12-12 17:00:34,001 INFO Epoch [29/500], Batch [50/90], Loss: 0.5515\n",
      "2024-12-12 17:00:47,903 INFO Epoch [29/500], Batch [60/90], Loss: 0.5151\n",
      "2024-12-12 17:01:01,790 INFO Epoch [29/500], Batch [70/90], Loss: 0.5663\n",
      "2024-12-12 17:01:15,664 INFO Epoch [29/500], Batch [80/90], Loss: 0.5191\n",
      "2024-12-12 17:01:29,750 INFO Epoch [29/500], Batch [90/90], Loss: 0.4986\n",
      "2024-12-12 17:01:34,462 INFO Epoch [29/500] - Train Loss: 0.5427, Train Acc: 0.7541 - Val Loss: 0.5937, Val Acc: 0.7036\n",
      "2024-12-12 17:01:48,356 INFO Epoch [30/500], Batch [10/90], Loss: 0.5966\n",
      "2024-12-12 17:02:03,240 INFO Epoch [30/500], Batch [20/90], Loss: 0.6002\n",
      "2024-12-12 17:02:17,348 INFO Epoch [30/500], Batch [30/90], Loss: 0.5349\n",
      "2024-12-12 17:02:31,143 INFO Epoch [30/500], Batch [40/90], Loss: 0.5292\n",
      "2024-12-12 17:02:44,835 INFO Epoch [30/500], Batch [50/90], Loss: 0.5263\n",
      "2024-12-12 17:02:58,585 INFO Epoch [30/500], Batch [60/90], Loss: 0.5077\n",
      "2024-12-12 17:03:12,214 INFO Epoch [30/500], Batch [70/90], Loss: 0.6302\n",
      "2024-12-12 17:03:26,075 INFO Epoch [30/500], Batch [80/90], Loss: 0.4687\n",
      "2024-12-12 17:03:39,885 INFO Epoch [30/500], Batch [90/90], Loss: 0.5089\n",
      "2024-12-12 17:03:44,474 INFO Epoch [30/500] - Train Loss: 0.5426, Train Acc: 0.7538 - Val Loss: 0.5943, Val Acc: 0.7036\n",
      "2024-12-12 17:03:58,214 INFO Epoch [31/500], Batch [10/90], Loss: 0.5127\n",
      "2024-12-12 17:04:12,053 INFO Epoch [31/500], Batch [20/90], Loss: 0.5449\n",
      "2024-12-12 17:04:25,792 INFO Epoch [31/500], Batch [30/90], Loss: 0.5872\n",
      "2024-12-12 17:04:39,838 INFO Epoch [31/500], Batch [40/90], Loss: 0.5165\n",
      "2024-12-12 17:04:53,496 INFO Epoch [31/500], Batch [50/90], Loss: 0.4386\n",
      "2024-12-12 17:05:07,166 INFO Epoch [31/500], Batch [60/90], Loss: 0.6972\n",
      "2024-12-12 17:05:21,142 INFO Epoch [31/500], Batch [70/90], Loss: 0.6037\n",
      "2024-12-12 17:05:34,894 INFO Epoch [31/500], Batch [80/90], Loss: 0.5603\n",
      "2024-12-12 17:05:48,861 INFO Epoch [31/500], Batch [90/90], Loss: 0.6345\n",
      "2024-12-12 17:05:53,385 INFO Epoch [31/500] - Train Loss: 0.5454, Train Acc: 0.7536 - Val Loss: 0.5985, Val Acc: 0.7036\n",
      "2024-12-12 17:06:07,048 INFO Epoch [32/500], Batch [10/90], Loss: 0.5525\n",
      "2024-12-12 17:06:20,796 INFO Epoch [32/500], Batch [20/90], Loss: 0.6273\n",
      "2024-12-12 17:06:34,470 INFO Epoch [32/500], Batch [30/90], Loss: 0.6389\n",
      "2024-12-12 17:06:48,224 INFO Epoch [32/500], Batch [40/90], Loss: 0.4507\n",
      "2024-12-12 17:07:01,917 INFO Epoch [32/500], Batch [50/90], Loss: 0.5055\n",
      "2024-12-12 17:07:15,997 INFO Epoch [32/500], Batch [60/90], Loss: 0.5699\n",
      "2024-12-12 17:07:29,693 INFO Epoch [32/500], Batch [70/90], Loss: 0.5659\n",
      "2024-12-12 17:07:43,643 INFO Epoch [32/500], Batch [80/90], Loss: 0.4689\n",
      "2024-12-12 17:07:58,058 INFO Epoch [32/500], Batch [90/90], Loss: 0.4964\n",
      "2024-12-12 17:08:02,783 INFO Epoch [32/500] - Train Loss: 0.5416, Train Acc: 0.7548 - Val Loss: 0.5946, Val Acc: 0.7036\n",
      "2024-12-12 17:08:16,824 INFO Epoch [33/500], Batch [10/90], Loss: 0.6083\n",
      "2024-12-12 17:08:30,669 INFO Epoch [33/500], Batch [20/90], Loss: 0.4952\n",
      "2024-12-12 17:08:44,576 INFO Epoch [33/500], Batch [30/90], Loss: 0.4923\n",
      "2024-12-12 17:08:58,826 INFO Epoch [33/500], Batch [40/90], Loss: 0.4367\n",
      "2024-12-12 17:09:12,766 INFO Epoch [33/500], Batch [50/90], Loss: 0.4616\n",
      "2024-12-12 17:09:26,589 INFO Epoch [33/500], Batch [60/90], Loss: 0.4491\n",
      "2024-12-12 17:09:40,323 INFO Epoch [33/500], Batch [70/90], Loss: 0.6316\n",
      "2024-12-12 17:09:54,320 INFO Epoch [33/500], Batch [80/90], Loss: 0.5420\n",
      "2024-12-12 17:10:08,129 INFO Epoch [33/500], Batch [90/90], Loss: 0.6251\n",
      "2024-12-12 17:10:12,684 INFO Epoch [33/500] - Train Loss: 0.5442, Train Acc: 0.7541 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:10:26,437 INFO Epoch [34/500], Batch [10/90], Loss: 0.5289\n",
      "2024-12-12 17:10:40,293 INFO Epoch [34/500], Batch [20/90], Loss: 0.5783\n",
      "2024-12-12 17:10:54,083 INFO Epoch [34/500], Batch [30/90], Loss: 0.5065\n",
      "2024-12-12 17:11:08,001 INFO Epoch [34/500], Batch [40/90], Loss: 0.5451\n",
      "2024-12-12 17:11:22,243 INFO Epoch [34/500], Batch [50/90], Loss: 0.5741\n",
      "2024-12-12 17:11:35,960 INFO Epoch [34/500], Batch [60/90], Loss: 0.5717\n",
      "2024-12-12 17:11:49,716 INFO Epoch [34/500], Batch [70/90], Loss: 0.6425\n",
      "2024-12-12 17:12:03,462 INFO Epoch [34/500], Batch [80/90], Loss: 0.5601\n",
      "2024-12-12 17:12:17,239 INFO Epoch [34/500], Batch [90/90], Loss: 0.5001\n",
      "2024-12-12 17:12:21,754 INFO Epoch [34/500] - Train Loss: 0.5425, Train Acc: 0.7541 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:12:35,676 INFO Epoch [35/500], Batch [10/90], Loss: 0.4880\n",
      "2024-12-12 17:12:49,458 INFO Epoch [35/500], Batch [20/90], Loss: 0.5871\n",
      "2024-12-12 17:13:03,275 INFO Epoch [35/500], Batch [30/90], Loss: 0.5352\n",
      "2024-12-12 17:13:17,208 INFO Epoch [35/500], Batch [40/90], Loss: 0.5959\n",
      "2024-12-12 17:13:31,235 INFO Epoch [35/500], Batch [50/90], Loss: 0.6155\n",
      "2024-12-12 17:13:45,199 INFO Epoch [35/500], Batch [60/90], Loss: 0.5205\n",
      "2024-12-12 17:13:59,215 INFO Epoch [35/500], Batch [70/90], Loss: 0.5109\n",
      "2024-12-12 17:14:13,145 INFO Epoch [35/500], Batch [80/90], Loss: 0.5240\n",
      "2024-12-12 17:14:27,000 INFO Epoch [35/500], Batch [90/90], Loss: 0.5363\n",
      "2024-12-12 17:14:31,560 INFO Epoch [35/500] - Train Loss: 0.5415, Train Acc: 0.7540 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:14:45,270 INFO Epoch [36/500], Batch [10/90], Loss: 0.6133\n",
      "2024-12-12 17:14:59,029 INFO Epoch [36/500], Batch [20/90], Loss: 0.5608\n",
      "2024-12-12 17:15:12,906 INFO Epoch [36/500], Batch [30/90], Loss: 0.6112\n",
      "2024-12-12 17:15:26,887 INFO Epoch [36/500], Batch [40/90], Loss: 0.7151\n",
      "2024-12-12 17:15:40,729 INFO Epoch [36/500], Batch [50/90], Loss: 0.5671\n",
      "2024-12-12 17:15:54,623 INFO Epoch [36/500], Batch [60/90], Loss: 0.6141\n",
      "2024-12-12 17:16:08,457 INFO Epoch [36/500], Batch [70/90], Loss: 0.5606\n",
      "2024-12-12 17:16:22,213 INFO Epoch [36/500], Batch [80/90], Loss: 0.6300\n",
      "2024-12-12 17:16:35,999 INFO Epoch [36/500], Batch [90/90], Loss: 0.4622\n",
      "2024-12-12 17:16:40,574 INFO Epoch [36/500] - Train Loss: 0.5419, Train Acc: 0.7546 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:16:54,381 INFO Epoch [37/500], Batch [10/90], Loss: 0.5388\n",
      "2024-12-12 17:17:08,636 INFO Epoch [37/500], Batch [20/90], Loss: 0.5573\n",
      "2024-12-12 17:17:22,472 INFO Epoch [37/500], Batch [30/90], Loss: 0.5169\n",
      "2024-12-12 17:17:36,297 INFO Epoch [37/500], Batch [40/90], Loss: 0.5397\n",
      "2024-12-12 17:17:50,047 INFO Epoch [37/500], Batch [50/90], Loss: 0.6494\n",
      "2024-12-12 17:18:03,762 INFO Epoch [37/500], Batch [60/90], Loss: 0.4800\n",
      "2024-12-12 17:18:17,423 INFO Epoch [37/500], Batch [70/90], Loss: 0.5434\n",
      "2024-12-12 17:18:31,212 INFO Epoch [37/500], Batch [80/90], Loss: 0.4635\n",
      "2024-12-12 17:18:44,946 INFO Epoch [37/500], Batch [90/90], Loss: 0.5211\n",
      "2024-12-12 17:18:49,631 INFO Epoch [37/500] - Train Loss: 0.5415, Train Acc: 0.7543 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:19:03,537 INFO Epoch [38/500], Batch [10/90], Loss: 0.5110\n",
      "2024-12-12 17:19:17,273 INFO Epoch [38/500], Batch [20/90], Loss: 0.5891\n",
      "2024-12-12 17:19:31,024 INFO Epoch [38/500], Batch [30/90], Loss: 0.5737\n",
      "2024-12-12 17:19:44,786 INFO Epoch [38/500], Batch [40/90], Loss: 0.6066\n",
      "2024-12-12 17:19:58,636 INFO Epoch [38/500], Batch [50/90], Loss: 0.6396\n",
      "2024-12-12 17:20:12,417 INFO Epoch [38/500], Batch [60/90], Loss: 0.6218\n",
      "2024-12-12 17:20:26,164 INFO Epoch [38/500], Batch [70/90], Loss: 0.4436\n",
      "2024-12-12 17:20:39,950 INFO Epoch [38/500], Batch [80/90], Loss: 0.5555\n",
      "2024-12-12 17:20:53,706 INFO Epoch [38/500], Batch [90/90], Loss: 0.5763\n",
      "2024-12-12 17:20:58,198 INFO Epoch [38/500] - Train Loss: 0.5427, Train Acc: 0.7545 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:21:12,268 INFO Epoch [39/500], Batch [10/90], Loss: 0.5317\n",
      "2024-12-12 17:21:27,161 INFO Epoch [39/500], Batch [20/90], Loss: 0.5734\n",
      "2024-12-12 17:21:41,562 INFO Epoch [39/500], Batch [30/90], Loss: 0.5473\n",
      "2024-12-12 17:21:55,836 INFO Epoch [39/500], Batch [40/90], Loss: 0.5325\n",
      "2024-12-12 17:22:09,722 INFO Epoch [39/500], Batch [50/90], Loss: 0.5763\n",
      "2024-12-12 17:22:23,154 INFO Epoch [39/500], Batch [60/90], Loss: 0.5853\n",
      "2024-12-12 17:22:37,290 INFO Epoch [39/500], Batch [70/90], Loss: 0.5956\n",
      "2024-12-12 17:22:51,142 INFO Epoch [39/500], Batch [80/90], Loss: 0.5609\n",
      "2024-12-12 17:23:04,738 INFO Epoch [39/500], Batch [90/90], Loss: 0.5269\n",
      "2024-12-12 17:23:09,203 INFO Epoch [39/500] - Train Loss: 0.5413, Train Acc: 0.7536 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:23:22,632 INFO Epoch [40/500], Batch [10/90], Loss: 0.4152\n",
      "2024-12-12 17:23:35,974 INFO Epoch [40/500], Batch [20/90], Loss: 0.5941\n",
      "2024-12-12 17:23:49,561 INFO Epoch [40/500], Batch [30/90], Loss: 0.6056\n",
      "2024-12-12 17:24:02,836 INFO Epoch [40/500], Batch [40/90], Loss: 0.5200\n",
      "2024-12-12 17:24:16,293 INFO Epoch [40/500], Batch [50/90], Loss: 0.4799\n",
      "2024-12-12 17:24:29,619 INFO Epoch [40/500], Batch [60/90], Loss: 0.5046\n",
      "2024-12-12 17:24:43,014 INFO Epoch [40/500], Batch [70/90], Loss: 0.5898\n",
      "2024-12-12 17:24:56,357 INFO Epoch [40/500], Batch [80/90], Loss: 0.4768\n",
      "2024-12-12 17:25:09,712 INFO Epoch [40/500], Batch [90/90], Loss: 0.6190\n",
      "2024-12-12 17:25:14,205 INFO Epoch [40/500] - Train Loss: 0.5399, Train Acc: 0.7541 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:25:27,682 INFO Epoch [41/500], Batch [10/90], Loss: 0.6191\n",
      "2024-12-12 17:25:41,218 INFO Epoch [41/500], Batch [20/90], Loss: 0.6150\n",
      "2024-12-12 17:25:54,628 INFO Epoch [41/500], Batch [30/90], Loss: 0.4650\n",
      "2024-12-12 17:26:07,991 INFO Epoch [41/500], Batch [40/90], Loss: 0.5072\n",
      "2024-12-12 17:26:21,481 INFO Epoch [41/500], Batch [50/90], Loss: 0.5658\n",
      "2024-12-12 17:26:34,903 INFO Epoch [41/500], Batch [60/90], Loss: 0.4846\n",
      "2024-12-12 17:26:48,387 INFO Epoch [41/500], Batch [70/90], Loss: 0.5290\n",
      "2024-12-12 17:27:01,859 INFO Epoch [41/500], Batch [80/90], Loss: 0.5904\n",
      "2024-12-12 17:27:15,205 INFO Epoch [41/500], Batch [90/90], Loss: 0.6502\n",
      "2024-12-12 17:27:19,671 INFO Epoch [41/500] - Train Loss: 0.5421, Train Acc: 0.7540 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:27:32,988 INFO Epoch [42/500], Batch [10/90], Loss: 0.5104\n",
      "2024-12-12 17:27:46,275 INFO Epoch [42/500], Batch [20/90], Loss: 0.5455\n",
      "2024-12-12 17:27:59,612 INFO Epoch [42/500], Batch [30/90], Loss: 0.5554\n",
      "2024-12-12 17:28:13,020 INFO Epoch [42/500], Batch [40/90], Loss: 0.4427\n",
      "2024-12-12 17:28:26,587 INFO Epoch [42/500], Batch [50/90], Loss: 0.4684\n",
      "2024-12-12 17:28:40,453 INFO Epoch [42/500], Batch [60/90], Loss: 0.6140\n",
      "2024-12-12 17:28:53,875 INFO Epoch [42/500], Batch [70/90], Loss: 0.5177\n",
      "2024-12-12 17:29:07,307 INFO Epoch [42/500], Batch [80/90], Loss: 0.5204\n",
      "2024-12-12 17:29:20,643 INFO Epoch [42/500], Batch [90/90], Loss: 0.4886\n",
      "2024-12-12 17:29:24,974 INFO Epoch [42/500] - Train Loss: 0.5420, Train Acc: 0.7534 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:29:38,564 INFO Epoch [43/500], Batch [10/90], Loss: 0.5844\n",
      "2024-12-12 17:29:51,912 INFO Epoch [43/500], Batch [20/90], Loss: 0.5497\n",
      "2024-12-12 17:30:05,321 INFO Epoch [43/500], Batch [30/90], Loss: 0.5561\n",
      "2024-12-12 17:30:18,756 INFO Epoch [43/500], Batch [40/90], Loss: 0.5692\n",
      "2024-12-12 17:30:32,175 INFO Epoch [43/500], Batch [50/90], Loss: 0.5619\n",
      "2024-12-12 17:30:45,654 INFO Epoch [43/500], Batch [60/90], Loss: 0.6455\n",
      "2024-12-12 17:30:59,100 INFO Epoch [43/500], Batch [70/90], Loss: 0.4903\n",
      "2024-12-12 17:31:12,592 INFO Epoch [43/500], Batch [80/90], Loss: 0.5469\n",
      "2024-12-12 17:31:25,940 INFO Epoch [43/500], Batch [90/90], Loss: 0.6132\n",
      "2024-12-12 17:31:30,350 INFO Epoch [43/500] - Train Loss: 0.5426, Train Acc: 0.7541 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:31:43,802 INFO Epoch [44/500], Batch [10/90], Loss: 0.4526\n",
      "2024-12-12 17:31:57,238 INFO Epoch [44/500], Batch [20/90], Loss: 0.6356\n",
      "2024-12-12 17:32:10,665 INFO Epoch [44/500], Batch [30/90], Loss: 0.6323\n",
      "2024-12-12 17:32:24,218 INFO Epoch [44/500], Batch [40/90], Loss: 0.4791\n",
      "2024-12-12 17:32:37,725 INFO Epoch [44/500], Batch [50/90], Loss: 0.4884\n",
      "2024-12-12 17:32:51,692 INFO Epoch [44/500], Batch [60/90], Loss: 0.5325\n",
      "2024-12-12 17:33:05,457 INFO Epoch [44/500], Batch [70/90], Loss: 0.5684\n",
      "2024-12-12 17:33:19,700 INFO Epoch [44/500], Batch [80/90], Loss: 0.6066\n",
      "2024-12-12 17:33:33,729 INFO Epoch [44/500], Batch [90/90], Loss: 0.5993\n",
      "2024-12-12 17:33:38,534 INFO Epoch [44/500] - Train Loss: 0.5424, Train Acc: 0.7536 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:33:52,267 INFO Epoch [45/500], Batch [10/90], Loss: 0.6263\n",
      "2024-12-12 17:34:06,055 INFO Epoch [45/500], Batch [20/90], Loss: 0.5211\n",
      "2024-12-12 17:34:20,242 INFO Epoch [45/500], Batch [30/90], Loss: 0.5614\n",
      "2024-12-12 17:34:34,114 INFO Epoch [45/500], Batch [40/90], Loss: 0.5377\n",
      "2024-12-12 17:34:48,030 INFO Epoch [45/500], Batch [50/90], Loss: 0.6597\n",
      "2024-12-12 17:35:01,689 INFO Epoch [45/500], Batch [60/90], Loss: 0.5646\n",
      "2024-12-12 17:35:15,576 INFO Epoch [45/500], Batch [70/90], Loss: 0.5808\n",
      "2024-12-12 17:35:29,337 INFO Epoch [45/500], Batch [80/90], Loss: 0.5165\n",
      "2024-12-12 17:35:43,359 INFO Epoch [45/500], Batch [90/90], Loss: 0.6066\n",
      "2024-12-12 17:35:48,289 INFO Epoch [45/500] - Train Loss: 0.5421, Train Acc: 0.7538 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:36:02,362 INFO Epoch [46/500], Batch [10/90], Loss: 0.6560\n",
      "2024-12-12 17:36:16,210 INFO Epoch [46/500], Batch [20/90], Loss: 0.6046\n",
      "2024-12-12 17:36:29,777 INFO Epoch [46/500], Batch [30/90], Loss: 0.5182\n",
      "2024-12-12 17:36:43,338 INFO Epoch [46/500], Batch [40/90], Loss: 0.6008\n",
      "2024-12-12 17:36:57,055 INFO Epoch [46/500], Batch [50/90], Loss: 0.5398\n",
      "2024-12-12 17:37:10,588 INFO Epoch [46/500], Batch [60/90], Loss: 0.4718\n",
      "2024-12-12 17:37:24,091 INFO Epoch [46/500], Batch [70/90], Loss: 0.5115\n",
      "2024-12-12 17:37:37,700 INFO Epoch [46/500], Batch [80/90], Loss: 0.5254\n",
      "2024-12-12 17:37:51,383 INFO Epoch [46/500], Batch [90/90], Loss: 0.4253\n",
      "2024-12-12 17:37:55,791 INFO Epoch [46/500] - Train Loss: 0.5404, Train Acc: 0.7546 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:38:09,450 INFO Epoch [47/500], Batch [10/90], Loss: 0.5148\n",
      "2024-12-12 17:38:23,116 INFO Epoch [47/500], Batch [20/90], Loss: 0.5266\n",
      "2024-12-12 17:38:36,812 INFO Epoch [47/500], Batch [30/90], Loss: 0.6022\n",
      "2024-12-12 17:38:50,380 INFO Epoch [47/500], Batch [40/90], Loss: 0.5154\n",
      "2024-12-12 17:39:04,002 INFO Epoch [47/500], Batch [50/90], Loss: 0.5645\n",
      "2024-12-12 17:39:18,001 INFO Epoch [47/500], Batch [60/90], Loss: 0.7730\n",
      "2024-12-12 17:39:31,601 INFO Epoch [47/500], Batch [70/90], Loss: 0.6555\n",
      "2024-12-12 17:39:45,123 INFO Epoch [47/500], Batch [80/90], Loss: 0.5260\n",
      "2024-12-12 17:39:58,654 INFO Epoch [47/500], Batch [90/90], Loss: 0.4937\n",
      "2024-12-12 17:40:03,123 INFO Epoch [47/500] - Train Loss: 0.5428, Train Acc: 0.7552 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:40:16,634 INFO Epoch [48/500], Batch [10/90], Loss: 0.6201\n",
      "2024-12-12 17:40:30,255 INFO Epoch [48/500], Batch [20/90], Loss: 0.6090\n",
      "2024-12-12 17:40:43,776 INFO Epoch [48/500], Batch [30/90], Loss: 0.5114\n",
      "2024-12-12 17:40:57,313 INFO Epoch [48/500], Batch [40/90], Loss: 0.5013\n",
      "2024-12-12 17:41:11,066 INFO Epoch [48/500], Batch [50/90], Loss: 0.5110\n",
      "2024-12-12 17:41:24,727 INFO Epoch [48/500], Batch [60/90], Loss: 0.5734\n",
      "2024-12-12 17:41:38,304 INFO Epoch [48/500], Batch [70/90], Loss: 0.4542\n",
      "2024-12-12 17:41:51,861 INFO Epoch [48/500], Batch [80/90], Loss: 0.6376\n",
      "2024-12-12 17:42:05,493 INFO Epoch [48/500], Batch [90/90], Loss: 0.6362\n",
      "2024-12-12 17:42:10,019 INFO Epoch [48/500] - Train Loss: 0.5450, Train Acc: 0.7538 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:42:23,720 INFO Epoch [49/500], Batch [10/90], Loss: 0.5221\n",
      "2024-12-12 17:42:37,126 INFO Epoch [49/500], Batch [20/90], Loss: 0.5542\n",
      "2024-12-12 17:42:50,698 INFO Epoch [49/500], Batch [30/90], Loss: 0.5456\n",
      "2024-12-12 17:43:04,231 INFO Epoch [49/500], Batch [40/90], Loss: 0.5366\n",
      "2024-12-12 17:43:18,438 INFO Epoch [49/500], Batch [50/90], Loss: 0.5358\n",
      "2024-12-12 17:43:32,083 INFO Epoch [49/500], Batch [60/90], Loss: 0.6751\n",
      "2024-12-12 17:43:45,557 INFO Epoch [49/500], Batch [70/90], Loss: 0.5492\n",
      "2024-12-12 17:43:59,652 INFO Epoch [49/500], Batch [80/90], Loss: 0.4533\n",
      "2024-12-12 17:44:13,266 INFO Epoch [49/500], Batch [90/90], Loss: 0.5191\n",
      "2024-12-12 17:44:17,716 INFO Epoch [49/500] - Train Loss: 0.5397, Train Acc: 0.7550 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:44:31,286 INFO Epoch [50/500], Batch [10/90], Loss: 0.5075\n",
      "2024-12-12 17:44:44,794 INFO Epoch [50/500], Batch [20/90], Loss: 0.5448\n",
      "2024-12-12 17:44:58,418 INFO Epoch [50/500], Batch [30/90], Loss: 0.5020\n",
      "2024-12-12 17:45:12,040 INFO Epoch [50/500], Batch [40/90], Loss: 0.5425\n",
      "2024-12-12 17:45:25,629 INFO Epoch [50/500], Batch [50/90], Loss: 0.5432\n",
      "2024-12-12 17:45:39,239 INFO Epoch [50/500], Batch [60/90], Loss: 0.5257\n",
      "2024-12-12 17:45:52,967 INFO Epoch [50/500], Batch [70/90], Loss: 0.6729\n",
      "2024-12-12 17:46:06,625 INFO Epoch [50/500], Batch [80/90], Loss: 0.6019\n",
      "2024-12-12 17:46:20,263 INFO Epoch [50/500], Batch [90/90], Loss: 0.6486\n",
      "2024-12-12 17:46:24,664 INFO Epoch [50/500] - Train Loss: 0.5414, Train Acc: 0.7548 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:46:38,261 INFO Epoch [51/500], Batch [10/90], Loss: 0.4631\n",
      "2024-12-12 17:46:51,801 INFO Epoch [51/500], Batch [20/90], Loss: 0.5085\n",
      "2024-12-12 17:47:05,177 INFO Epoch [51/500], Batch [30/90], Loss: 0.4836\n",
      "2024-12-12 17:47:18,702 INFO Epoch [51/500], Batch [40/90], Loss: 0.4985\n",
      "2024-12-12 17:47:32,255 INFO Epoch [51/500], Batch [50/90], Loss: 0.6048\n",
      "2024-12-12 17:47:45,703 INFO Epoch [51/500], Batch [60/90], Loss: 0.4863\n",
      "2024-12-12 17:47:59,175 INFO Epoch [51/500], Batch [70/90], Loss: 0.4709\n",
      "2024-12-12 17:48:12,615 INFO Epoch [51/500], Batch [80/90], Loss: 0.6237\n",
      "2024-12-12 17:48:25,865 INFO Epoch [51/500], Batch [90/90], Loss: 0.4487\n",
      "2024-12-12 17:48:30,232 INFO Epoch [51/500] - Train Loss: 0.5432, Train Acc: 0.7541 - Val Loss: 0.5950, Val Acc: 0.7036\n",
      "2024-12-12 17:48:30,232 INFO No improvement for 50 epochs. Early stopping.\n",
      "2024-12-12 17:48:30,233 INFO Loading the best Transformer model for testing...\n",
      "2024-12-12 17:48:30,242 INFO Evaluating the Transformer model on the test set...\n",
      "2024-12-12 17:48:32,412 INFO Test Metrics: {'loss': 87.81505947365514, 'accuracy': 0.7424789410348978, 'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1_score': np.float64(0.0), 'confusion_matrix': array([[617,   0],\n",
      "       [214,   0]])}\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'metrics' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_metrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Plot training and validation metrics\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[43mmetrics\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_precision\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecision\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     26\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_recall\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrecall\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     27\u001b[0m metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_f1_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_metrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'metrics' is not defined"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.AdamW(transformer_model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=2)\n",
    "\n",
    "# Train the Transformer model\n",
    "logger.info(\"Starting training of the Transformer model...\")\n",
    "train_losses, val_losses, train_accuracies, val_accuracies = train_model(\n",
    "    model=transformer_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    criterion=criterion,\n",
    "    optimizer=optimizer,\n",
    "    scheduler=scheduler\n",
    "    epochs=50,\n",
    "    lr=1e-4,\n",
    "    device=device,\n",
    "    log_interval=10,\n",
    "    save_path=transformer_model_save_path,\n",
    "    patience=5\n",
    ")\n",
    "\n",
    "# Load the best Transformer model\n",
    "logger.info(\"Loading the best Transformer model for testing...\")\n",
    "transformer_model.load_state_dict(torch.load(transformer_model_save_path, map_location=device))\n",
    "\n",
    "# Evaluate the Transformer model on the test set\n",
    "logger.info(\"Evaluating on test set...\")\n",
    "test_metrics = evaluate_model(transformer_model, test_loader, device)\n",
    "logger.info(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "metrics = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'test_loss': test_metrics['loss'],\n",
    "    'test_accuracy': test_metrics['accuracy'],\n",
    "    'test_precision': test_metrics['precision'],\n",
    "    'test_recall': test_metrics['recall'],\n",
    "    'test_f1_score': test_metrics['f1_score'],\n",
    "    'test_confusion_matrix': test_metrics['confusion_matrix']\n",
    "}\n",
    "metrics_path = os.path.join(transformer_model_save_dir, 'metrics.pkl')\n",
    "pd.to_pickle(metrics, metrics_path)\n",
    "logger.info(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "# Plot training and validation metrics\n",
    "logger.info(\"Plotting training results for the Transformer model...\")\n",
    "plot_results(train_losses, val_losses, train_accuracies, val_accuracies, outdir=os.path.join(PROJECT_PATH, 'plots'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-12 17:53:24,494 INFO Evaluating on test set...\n",
      "2024-12-12 17:53:26,839 INFO Test Metrics: {'loss': 87.81505947365514, 'accuracy': 0.7424789410348978, 'precision': np.float64(0.0), 'recall': np.float64(0.0), 'f1_score': np.float64(0.0), 'confusion_matrix': array([[617,   0],\n",
      "       [214,   0]])}\n",
      "2024-12-12 17:53:26,841 INFO Metrics saved to /Users/shawn/Documents/personal/rsi_divergence_detector/model_data/transformer_mixed/metrics.pkl\n",
      "2024-12-12 17:53:26,841 INFO Plotting training results...\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Transformer model on the test set\n",
    "logger.info(\"Evaluating on test set...\")\n",
    "    # After evaluating on the test set\n",
    "test_metrics = evaluate_model(transformer_model, test_loader, device)\n",
    "logger.info(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "metrics = {\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'train_accuracies': train_accuracies,\n",
    "    'val_accuracies': val_accuracies,\n",
    "    'test_loss': test_metrics['loss'],\n",
    "    'test_accuracy': test_metrics['accuracy']\n",
    "}\n",
    "# Save the extended metrics\n",
    "metrics['test_precision'] = test_metrics['precision']\n",
    "metrics['test_recall'] = test_metrics['recall']\n",
    "metrics['test_f1_score'] = test_metrics['f1_score']\n",
    "metrics['test_confusion_matrix'] = test_metrics['confusion_matrix']\n",
    "\n",
    "metrics_path = os.path.join(transformer_model_save_dir, 'metrics.pkl')\n",
    "pd.to_pickle(metrics, metrics_path)\n",
    "logger.info(f\"Metrics saved to {metrics_path}\")\n",
    "\n",
    "\n",
    "# Plot training and validation metrics\n",
    "logger.info(\"Plotting training results...\")\n",
    "plot_results(train_losses, val_losses, train_accuracies, val_accuracies, outdir=os.path.join(PROJECT_PATH, 'plots'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0kAAAHWCAYAAACi1sL/AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABm10lEQVR4nO3deXhU5f3+8fvMJJmskw1Iwr7Ivqlsxq0ou4qg+NVaVHCprQUrpf601gVwX2pLFUXbWqm1aMUWRatAQMUNFEGQXUUElITIkhWSTGbO74/JjBkSyEIy5yR5v64rVzJnzjPnM5NH5OZZjmGapikAAAAAgCTJYXUBAAAAAGAnhCQAAAAAqISQBAAAAACVEJIAAAAAoBJCEgAAAABUQkgCAAAAgEoISQAAAABQCSEJAAAAACohJAEAAABAJYQkAACasKlTpyo+Pt7qMgCgWSEkAUALtGDBAhmGoc8++8zqUmxv6tSpMgyj2q/o6GirywMANIIIqwsAAMDuXC6X/va3v1U57nQ6LagGANDYCEkAgBbNNE2VlJQoJibmuOdEREToqquuCmNVAAArMd0OAHBcn3/+ucaNGye32634+HiNGDFCa9asCTnH4/Fozpw56t69u6Kjo5Wamqqzzz5bWVlZwXNycnJ07bXXqn379nK5XMrIyNCECRP07bffnvD6gfU233zzjcaMGaO4uDi1bdtW9957r0zTDDnX5/Np7ty56tu3r6Kjo5WWlqZf/OIXOnz4cMh5nTt31kUXXaRly5Zp8ODBiomJ0bPPPntyH5R+nML4/vvv6xe/+IVSU1Pldrt1zTXXVKlBkp5++mn17dtXLpdLbdu21bRp05SXl1flvE8++UQXXHCBkpOTFRcXpwEDBujPf/5zlfO+//57TZw4UfHx8WrdurVuvfVWeb3ek35fANASMZIEAKjWli1bdM4558jtduu2225TZGSknn32WQ0fPlyrVq3SsGHDJEmzZ8/WQw89pBtuuEFDhw5VQUGBPvvsM61fv16jRo2SJE2aNElbtmzRzTffrM6dOys3N1dZWVnas2ePOnfufMI6vF6vxo4dqzPOOEOPPvqoli5dqlmzZqm8vFz33ntv8Lxf/OIXWrBgga699lr9+te/1q5duzRv3jx9/vnn+uijjxQZGRk8d8eOHbryyiv1i1/8Qj//+c/Vs2fPGj+PAwcOVDkWFRUlt9sdcmz69OlKSkrS7NmztWPHDs2fP1+7d+/We++9J8Mwgp/ZnDlzNHLkSN10003B89auXRtSa1ZWli666CJlZGTolltuUXp6urZt26Y333xTt9xyS8hnNGbMGA0bNkx/+MMftGLFCj3++OPq1q2bbrrpphrfGwDgGCYAoMV5/vnnTUnm2rVrj3vOxIkTzaioKHPnzp3BY/v27TMTEhLMc889N3hs4MCB5oUXXnjc1zl8+LApyXzsscfqXOeUKVNMSebNN98cPObz+cwLL7zQjIqKMn/44QfTNE3zgw8+MCWZ//rXv0LaL126tMrxTp06mZLMpUuX1qmG6r7GjBkTPC/wmQ4aNMgsKysLHn/00UdNSebrr79umqZp5ubmmlFRUebo0aNNr9cbPG/evHmmJPPvf/+7aZqmWV5ebnbp0sXs1KmTefjw4ZCafD5flfruvffekHNOO+00c9CgQbV6jwCAUEy3AwBU4fV6tXz5ck2cOFFdu3YNHs/IyNDPfvYzffjhhyooKJAkJSUlacuWLfrqq6+qfa2YmBhFRUXpvffeq3baWW1Mnz49+LNhGJo+fbrKysq0YsUKSdKiRYuUmJioUaNG6cCBA8GvQYMGKT4+Xu+++27I63Xp0kVjxoyp9fWjo6OVlZVV5evhhx+ucu6NN94YMmp10003KSIiQm+99ZYkacWKFSorK9OMGTPkcPz4v+Gf//zncrvd+t///ifJP9Vx165dmjFjhpKSkkKuERiRquyXv/xlyONzzjlH33zzTa3fIwDgR0y3AwBU8cMPP+jIkSPVTkPr3bu3fD6f9u7dq759++ree+/VhAkT1KNHD/Xr109jx47V1VdfrQEDBkjy7wz3yCOP6Le//a3S0tJ0xhln6KKLLtI111yj9PT0GmtxOBwhQU2SevToIUnBNU1fffWV8vPz1aZNm2pfIzc3N+Rxly5darxuZU6nUyNHjqzVud27dw95HB8fr4yMjGCtu3fvlqQqn21UVJS6du0afH7nzp2SpH79+tV4zejoaLVu3TrkWHJycr1DKQC0dIQkAMBJOffcc7Vz5069/vrrWr58uf72t7/pT3/6k5555hndcMMNkqQZM2Zo/Pjxeu2117Rs2TLdfffdeuihh/TOO+/otNNOO+kafD6f2rRpo3/961/VPn9sgDjRTnZNEVuRA0DDYrodAKCK1q1bKzY2Vjt27Kjy3Pbt2+VwONShQ4fgsZSUFF177bV66aWXtHfvXg0YMECzZ88OadetWzf99re/1fLly7V582aVlZXp8ccfr7EWn89XZdrYl19+KUnBTR+6deumgwcP6qyzztLIkSOrfA0cOLCOn0D9HTvtsKioSNnZ2cFaO3XqJElVPtuysjLt2rUr+Hy3bt0kSZs3b27kigEAxyIkAQCqcDqdGj16tF5//fWQbbr379+vhQsX6uyzzw7u6nbw4MGQtvHx8TrllFNUWloqSTpy5IhKSkpCzunWrZsSEhKC59Rk3rx5wZ9N09S8efMUGRmpESNGSJIuv/xyeb1e3XfffVXalpeXV7u1dmP5y1/+Io/HE3w8f/58lZeXa9y4cZKkkSNHKioqSk888UTINubPPfec8vPzdeGFF0qSTj/9dHXp0kVz586tUr95zPbnAICGxXQ7AGjB/v73v2vp0qVVjt9yyy26//77lZWVpbPPPlu/+tWvFBERoWeffValpaV69NFHg+f26dNHw4cP16BBg5SSkqLPPvtMr776anCzhS+//FIjRozQ5Zdfrj59+igiIkKLFy/W/v379dOf/rTGGqOjo7V06VJNmTJFw4YN09tvv63//e9/+v3vfx+cRveTn/xEv/jFL/TQQw9pw4YNGj16tCIjI/XVV19p0aJF+vOf/6zLLrus3p9TeXm5XnzxxWqfu+SSSxQXFxd8XFZWFny/O3bs0NNPP62zzz5bF198sST/KN0dd9yhOXPmaOzYsbr44ouD5w0ZMiR401qHw6H58+dr/PjxOvXUU3XttdcqIyND27dv15YtW7Rs2bJ6vx8AQA0s3l0PAGCBwHbVx/vau3evaZqmuX79enPMmDFmfHy8GRsba5533nnmxx9/HPJa999/vzl06FAzKSnJjImJMXv16mU+8MADwW2wDxw4YE6bNs3s1auXGRcXZyYmJprDhg0zX3nllRrrnDJlihkXF2fu3LnTHD16tBkbG2umpaWZs2bNCtk+O+Avf/mLOWjQIDMmJsZMSEgw+/fvb952223mvn37gud06tTphFuWV1fDiT6rXbt2hXymq1atMm+88UYzOTnZjI+PNydPnmwePHiwyuvOmzfP7NWrlxkZGWmmpaWZN910U5Wtvk3TND/88ENz1KhRZkJCghkXF2cOGDDAfPLJJ6t8RseaNWuWyf/mAaB+DNNkzB4AYE9Tp07Vq6++qqKiIqtLqVHgRrZr167V4MGDrS4HAHASWJMEAAAAAJUQkgAAAACgEkISAAAAAFTCmiQAAAAAqISRJAAAAACohJAEAAAAAJU0+5vJ+nw+7du3TwkJCTIMw+pyAAAAAFjENE0VFhaqbdu2cjiOP17U7EPSvn371KFDB6vLAAAAAGATe/fuVfv27Y/7fLMPSQkJCZL8H4Tb7ba0Fo/Ho+XLl2v06NGKjIy0tBY0DfQZ1BV9BnVFn0Fd0WdQV3bqMwUFBerQoUMwIxxPsw9JgSl2brfbFiEpNjZWbrfb8g6CpoE+g7qiz6Cu6DOoK/oM6sqOfaamZThs3AAAAAAAlRCSAAAAAKASQhIAAAAAVNLs1yQBAADAXkzTVHl5ubxer9WlIAw8Ho8iIiJUUlLS6L9zp9OpiIiIk771DyEJAAAAYVNWVqbs7GwdOXLE6lIQJqZpKj09XXv37g3LfUtjY2OVkZGhqKioer8GIQkAAABh4fP5tGvXLjmdTrVt21ZRUVFh+UszrOXz+VRUVKT4+PgT3sD1ZJmmqbKyMv3www/atWuXunfvXu/rEZIAAAAQFmVlZfL5fOrQoYNiY2OtLgdh4vP5VFZWpujo6EYNSZIUExOjyMhI7d69O3jN+mDjBgAAAIRVY/9FGS1bQ/QveigAAAAAVEJIChOvz9Qnuw5p3QFDn+w6JK/PtLokAAAAANVgTVIYLN2crTlvbFV2fokkp1746jNlJEZr1vg+Gtsvw+ryAAAAmhyvz9Snuw4pt7BEbRKiNbRLipyOprUJROfOnTVjxgzNmDHD6lJwDEJSI1u6OVs3vbhex44b5eSX6KYX12v+VacTlAAAAOog9B+g/RrzH6Br2oFv1qxZmj17dp1fd+3atYqLi6tnVX7Dhw/Xqaeeqrlz557U6yAU0+0akddnas4bW6sEJEnBY3Pe2MrUOwAAgFoK/AN05YAk/fgP0Es3Zzf4NbOzs4Nfc+fOldvtDjl26623Bs8N3Ci3Nlq3bs0ufzZFSGpEn+46VOU/4MpMSdn5Jfp016HwFQUAAGAjpmnqSFl5rb4KSzyatWTLCf8BevaSrSos8dTq9Uyzdv9QnZ6eHvxKTEyUYRjBx9u3b1dCQoLefvttDRo0SC6XSx9++KF27typCRMmKC0tTfHx8RoyZIhWrFgR8rqdO3cOGQEyDEN/+9vfdMkllyg2Nlbdu3fXkiVL6vfBVvjPf/6jvn37yuVyqXPnznr88cdDnn/66afVvXt3RUdHKy0tTZdddlnwuVdffVX9+/dXTEyMUlNTNXLkSBUXF59UPU0F0+0aUW7h8QNSfc4DAABobo56vOpzz7IGeS1TUk5BifrPXl6r87feO0axUQ3z1+Hf/e53+sMf/qCuXbsqOTlZe/fu1QUXXKAHHnhALpdLL7zwgsaPH68dO3aoY8eOx32dOXPm6NFHH9Vjjz2mJ598UpMnT9bu3buVkpJS55rWrVunyy+/XLNnz9YVV1yhjz/+WL/61a+UmpqqqVOn6rPPPtOvf/1r/fOf/9SZZ56pQ4cO6YMPPpDkHz278sor9eijj+qSSy5RYWGhPvjgg1oHy6aOkNSI2iTU7uZVtT0PAAAA9nTvvfdq1KhRwccpKSkaOHBg8PF9992nxYsXa8mSJZo+ffpxX2fq1Km68sorJUkPPvignnjiCX366acaO3ZsnWv64x//qBEjRujuu++WJPXo0UNbt27VY489pqlTp2rPnj2Ki4vTRRddpISEBHXq1EmnnXaaJH9IKi8v16WXXqpOnTpJkvr371/nGpoqQlIjGtolRRmJ0crJL6l2WNiQlJ7o340FAACgJYqJdGrrvWNqde6nuw5p6vNrazxvwbVDavX3q5hIZ62uWxuDBw8OeVxUVKTZs2frf//7XzBwHD16VHv27Dnh6wwYMCD4c1xcnNxut3Jzc+tV07Zt2zRhwoSQY2eddZbmzp0rr9erUaNGqVOnTuratavGjh2rsWPHBqf6DRw4UCNGjFD//v01ZswYjR49WpdddpmSk5PrVUtTw5qkRuR0GJo1vo8kfyCqLPB41vg+TW67SgAAgIZiGIZioyJq9XVO99bKSIyu8veq4GvJv8vdOd1b1+r1atq1ri6O3aXu1ltv1eLFi/Xggw/qgw8+0IYNG9S/f3+VlZWd8HUiIyND35NhyOfzNVidlSUkJGj9+vV66aWXlJGRoXvuuUcDBw5UXl6enE6nsrKy9Pbbb6tPnz568skn1bNnT+3atatRarEbQlIjG9svQ/OvOl3piaFT6tITo9n+GwAAoA6a0j9Af/TRR5o6daouueQS9e/fX+np6fr222/DWkPv3r310UcfVamrR48ecjr9o2gREREaOXKkHn30UX3xxRf69ttv9c4770jyB7SzzjpLc+bM0eeff66oqCgtXrw4rO/BKky3C4Ox/TI0qk+6Jv91tdbsOqzJQzvo3on9bfEfMAAAQFMS+AfoY++TlN6I90mqj+7du+u///2vxo8fL8MwdPfddzfaiNAPP/ygDRs2hBzLyMjQb3/7Ww0ZMkT33XefrrjiCq1evVrz5s3T008/LUl688039c033+jcc89VcnKy3nrrLfl8PvXs2VOffPKJVq5cqdGjR6tNmzb65JNP9MMPP6h3796N8h7shpAUJk6HoU6pcVqz67BaxUcRkAAAAOop8A/Qn+46pNzCErVJ8K/xttPfr/74xz/quuuu05lnnqlWrVrp9ttvV0FBQaNca+HChVq4cGHIsfvuu0933XWXXnnlFd1zzz267777lJGRoXvvvVdTp06VJCUlJem///2vZs+erZKSEnXv3l0vvfSS+vbtq23btun999/X3LlzVVBQoE6dOunxxx/XuHHjGuU92A0hKYziovzDmsVlXosrAQAAaNqcDkOZ3VLDft2pU6cGQ4YkDR8+vNptsTt37hycthYwbdq0kMfHTr+r7nXy8vJOWM977713wucnTZqkSZMmVfvc2Weffdz2vXv31tKlS0/42s0Za5LCKM5VEZJKa3cXZgAAAADhR0gKoziXf+DuCCNJAAAAgG0RksIoruKOzowkAQAAAPZFSAqj4HQ7RpIAAAAA2yIkhVFguh0jSQAAAIB9EZLCKLC7XVEpI0kAAACAXRGSwig+MJJUxkgSAAAAYFeEpDCKDdwniZEkAAAAwLYISWH04xbg5dXeLAwAAACA9QhJYRRYk+QzpRKPz+JqAAAAmjCfV9r1gbTpVf93n/1n6gwfPlwzZswIPu7cubPmzp17wjaGYei111476Ws31Ou0FISkMIqNcsqQfwSpiB3uAAAA6mfrEmluP+kfF0n/ud7/fW4///FGMH78eI0dO7ba5z744AMZhqEvvviizq+7du1a3XjjjSdbXojZs2fr1FNPrXI8Oztb48aNa9BrHWvBggVKSkpq1GuECyEpjAzDUMVgEtuAAwAA1MfWJdIr10gF+0KPF2T7jzdCULr++uuVlZWl7777rspzzz//vAYPHqwBAwbU+XVbt26t2NjYhiixRunp6XK5XGG5VnNASAqz6IpPnJEkAAAASaYplRXX7qukQHr7NknVre2uOLb0dv95tXm9Wq4Rv+iii9S6dWstWLAg5HhRUZEWLVqk66+/XgcPHtSVV16pdu3aKTY2Vv3799dLL710wtc9drrdV199pXPPPVfR0dHq06ePsrKyqrS5/fbb1aNHD8XGxqpr1666++675fF4JPlHcubMmaONGzfKMAwZhhGs+djpdps2bdL555+vmJgYpaam6sYbb1RRUVHw+alTp2rixIn6wx/+oIyMDKWmpmratGnBa9XHnj17NGHCBMXHx8vtduvyyy/X/v37g89v3LhR5513nhISEuR2uzVo0CB99tlnkqTdu3dr/PjxSk5OVlxcnPr27au33nqr3rXUJKLRXhnVcjkleRhJAgAAkCR5jkgPtm2gFzP9I0wPd6jd6b/fJ0XF1XhaRESErrnmGi1YsEB33nmnDMOQJC1atEher1dXXnmlioqKNGjQIN1+++1yu9363//+p6uvvlrdunXT0KFDa7yGz+fTpZdeqrS0NH3yySfKz88PWb8UkJCQoAULFqht27batGmTfv7znyshIUG33XabrrjiCm3evFlLly7VihUrJEmJiYlVXqO4uFhjxoxRZmam1q5dq9zcXN1www2aPn16SBB89913lZGRoXfffVdff/21rrjiCp166qn6+c9/XuP7qe79XXLJJYqPj9eqVatUXl6uadOm6YorrtB7770nSZo8ebJOO+00zZ8/X06nUxs2bFBkZKQkadq0aSorK9P777+vuLg4bd26VfHx8XWuo7YISWHmCky3415JAAAATcZ1112nxx57TKtWrdLw4cMl+afaTZo0SYmJiUpMTNStt94aPP/mm2/WsmXL9Morr9QqJK1YsULbt2/XsmXL1LatPzQ++OCDVdYR3XXXXcGfO3furFtvvVUvv/yybrvtNsXExCg+Pl4RERFKT08/7rUWLlyokpISvfDCC4qL84fEefPmafz48XrkkUeUlpYmSUpOTta8efPkdDrVq1cvXXjhhVq5cmW9QtKqVau0adMm7dq1Sx06+EPsCy+8oL59+2rt2rUaMmSI9uzZo//3//6fevXqJUnq3r17sP2ePXs0adIk9e/fX5LUtWvXOtdQF4SkMHM5TUmGirhXEgAAgBQZ6x/RqY3dH0v/uqzm8ya/KnU6s3bXrqVevXrpzDPP1N///ncNHz5cX3/9tT744APde++9kiSv16sHH3xQr7zyir7//nuVlZWptLS01muOtm3bpg4dOgQDkiRlZmZWOe/f//63nnjiCe3cuVNFRUUqLy+X2+2u9fsIXGvgwIHBgCRJZ511lnw+n3bs2BEMSX379pXT6Qyek5GRoU2bNtXpWgFffvmlOnToEAxIktSnTx8lJSVp27ZtGjJkiGbOnKkbbrhB//znPzVy5Ej93//9n7p16yZJ+vWvf62bbrpJy5cv18iRIzVp0qR6rQOrLdYkhVk0GzcAAAD8yDD8U95q89XtfMndVpJxvBeT3O3859Xm9YzjvU71rr/+ev3nP/9RYWGhnn/+eXXr1k0/+clPJEmPPfaY/vznP+v222/Xu+++qw0bNmjMmDEqKys7uc+nktWrV2vy5Mm64IIL9Oabb+rzzz/XnXfe2aDXqCww1S3AMAz5fI13G5vZs2dry5YtuvDCC/XOO++oT58+Wrx4sSTphhtu0DfffKOrr75amzZt0uDBg/Xkk082Wi2EpDBzEZIAAADqx+GUxj5S8eDYgFPxeOzD/vMaweWXXy6Hw6GFCxfqhRde0HXXXRdcn/TRRx9pwoQJuuqqqzRw4EB17dpVX375Za1fu3fv3tq7d6+ys7ODx9asWRNyzscff6xOnTrpzjvv1ODBg9W9e3ft3r075JyoqCh5vSeesdS7d29t3LhRxcXFwWMfffSRHA6HevbsWeua66JHjx7au3ev9u7dGzy2detW5eXlqU+fPiHn/eY3v9Hy5ct16aWX6vnnnw8+16FDB/3yl7/Uf//7X/32t7/VX//610apVSIkhd2PIYnpdgAAAHXW52Lp8hckd0bocXdb//E+FzfapePj43XFFVfojjvuUHZ2tqZOnRp8rnv37srKytLHH3+sbdu26Re/+EXIzm01GTlypHr06KEpU6Zo48aN+uCDD3TnnXeGnNO9e3ft2bNHL7/8snbu3KknnngiONIS0LlzZ+3atUsbNmzQgQMHVFpaWuVakydPVnR0tKZMmaLNmzfr3Xff1c0336yrr746ONWuvrxerzZs2BDytW3bNg0fPlz9+/fX5MmTtX79en366ae65ppr9JOf/ESDBw/W0aNHNX36dL333nvavXu3PvroI61du1a9e/eWJM2YMUPLli3Trl27tH79er377rvB5xoDISnMAluAs3EDAABAPfW5WJqxWZrypjTpOf/3GZsaNSAFXH/99Tp8+LDGjBkTsn7orrvu0umnn64xY8Zo+PDhSk9P18SJE2v9ug6HQ4sXL9bRo0c1dOhQ3XDDDXrggQdCzrn44ov1m9/8RtOnT9epp56qjz/+WHfffXfIOZMmTdLYsWN13nnnqXXr1tVuQx4bG6tly5bp0KFDGjJkiC677DKNGDFC8+bNq9uHUY2ioiKddtppIV8TJkyQYRhavHixkpOTde6552rkyJHq2rWr/v3vf0uSnE6nDh48qGuuuUY9evTQ5ZdfrnHjxmnOnDmS/OFr2rRp6t27t8aOHasePXro6aefPul6j8cwzVpuEN9EFRQUKDExUfn5+XVe1NbQPB6Pfv3s23r7O6d+NqyjHrykv6X1wP48Ho/eeustXXDBBVXmBQPVoc+grugzqKuT6TMlJSXatWuXunTpoujo6EaqEHbj8/lUUFAgt9sth6Pxx2hO1M9qmw0YSQoz1iQBAAAA9kZICjNCEgAAAGBvhKQwC2wBXkRIAgAAAGyJkBRmgZGkI2XsbgcAAADYESEpzFxO/z4ZjCQBAICWqpnvGwaLNUT/IiSFi88rY/eH6l+8Wmc4tupoSdU96wEAAJqzwG54R44csbgSNGeB/nUyO3ZGNFQxOIGtS6SltyuiYJ9GSxodJeWUpUpb54ZlP38AAAA7cDqdSkpKUm5uriT//XoMw7C4KjQ2n8+nsrIylZSUNOoW4KZp6siRI8rNzVVSUpKcTme9X4uQ1Ni2LpFeuUZS6LBfGx2U+co1Mhr5ztAAAAB2kp6eLknBoITmzzRNHT16VDExMWEJxUlJScF+Vl+EpMbk80pLb9exAUmSHEbF0aW/k3pdKDnqn3QBAACaCsMwlJGRoTZt2sjj8VhdDsLA4/Ho/fff17nnntvoN62OjIw8qRGkAEJSY9r9sVSw77hPGzKlgu/953U5J4yFAQAAWMvpdDbIX2Zhf06nU+Xl5YqOjm70kNRQ2LihMRXtb9jzAAAAADQ6QlJjik9r2PMAAAAANDpCUmPqdKbkbiup+gVqpgzJ3c5/HgAAAABbsE1Ievjhh2UYhmbMmBE8VlJSomnTpik1NVXx8fGaNGmS9u9vQlPTHE5p7CMVD0KDki+wl8PYh9m0AQAAALARW4SktWvX6tlnn9WAAQNCjv/mN7/RG2+8oUWLFmnVqlXat2+fLr30UouqrKc+F0uXvyC5M0IO5yhVa4fOZftvAAAAwGYsD0lFRUWaPHmy/vrXvyo5OTl4PD8/X88995z++Mc/6vzzz9egQYP0/PPP6+OPP9aaNWssrLge+lwszdgsb7/LJUmfx2Tq7NI/6+vU8y0uDAAAAMCxLN8CfNq0abrwwgs1cuRI3X///cHj69atk8fj0ciRI4PHevXqpY4dO2r16tU644wzqn290tJSlZaWBh8XFBRI8u/PbvVe/GZafzk3vyLTGSWfHCo4Wmp5TbC3QP+gn6C26DOoK/oM6oo+g7qyU5+pbQ2WhqSXX35Z69ev19q1a6s8l5OTo6ioKCUlJYUcT0tLU05OznFf86GHHtKcOXOqHF++fLliY2NPuuaT0e7Q9xosyXX0B0nShs3b9Vb+VktrQtOQlZVldQloYugzqCv6DOqKPoO6skOfOXLkSK3Osywk7d27V7fccouysrIUHR3dYK97xx13aObMmcHHBQUF6tChg0aPHi23291g16kP75eR0u5nlBrpH+lq27GLLhjX09KaYG8ej0dZWVkaNWpUk7n5GqxFn0Fd0WdQV/QZ1JWd+kxglllNLAtJ69atU25urk4//fTgMa/Xq/fff1/z5s3TsmXLVFZWpry8vJDRpP379ys9Pf24r+tyueRyuaocj4yMtPyXooQ2kqR4n/+Xc7TctL4mNAm26L9oUugzqCv6DOqKPoO6skOfqe31LQtJI0aM0KZNm0KOXXvtterVq5duv/12dejQQZGRkVq5cqUmTZokSdqxY4f27NmjzMxMK0o+ebGpkqQYT54kU8Wl5ZaWAwAAAKAqy0JSQkKC+vXrF3IsLi5OqampwePXX3+9Zs6cqZSUFLndbt18883KzMw87qYNthfj373PaXoUpxJCEgAAAGBDlu9udyJ/+tOf5HA4NGnSJJWWlmrMmDF6+umnrS6r/iJj5TUi5TQ9SjYKVURIAgAAAGzHViHpvffeC3kcHR2tp556Sk899ZQ1BTU0w1BZRIJiPIeUrCIVlxGSAAAAALux/GayLU1pRLwkKcUoVHGp1+JqAAAAAByLkBRmZREJkqRkMd0OAAAAsCNCUpiVOSuPJBGSAAAAALshJIVZcCTJKNSRMq98PtPiigAAAABURkgKs7KKNUnJKpQkHfGwLgkAAACwE0JSmAVGklIcRZLElDsAAADAZghJYRYISa0qQhKbNwAAAAD2QkgKs8DGDamGf7odI0kAAACAvRCSwiywJimpYk0SI0kAAACAvRCSwqy0Yrqd2yyUZHJDWQAAAMBmCElh5qkYSYpUueJ1lOl2AAAAgM0QksLM63DJjIyV5L9XUnEZIQkAAACwE0KSFWKSJUkpKmQkCQAAALAZQpIVYlIk+UeSiliTBAAAANgKIckCZmyqJClZRYwkAQAAADZDSLJCrH8kKcVguh0AAABgN4QkC5gxFSNJRiH3SQIAAABshpBkBTZuAAAAAGyLkGSF2B9HkorL2LgBAAAAsBNCkgXM2MDudmzcAAAAANgNIckKgTVJTLcDAAAAbIeQZAGz0u523CcJAAAAsBdCkhUCN5NVoYpLPRYXAwAAAKAyQpIVKna3izB8ivQUyuszLS4IAAAAQAAhyQqRMTIj4yRJSUaRistYlwQAAADYBSHJKoF1SWzeAAAAANgKIckiRuV7JbF5AwAAAGAbhCSrMJIEAAAA2BIhySohI0mEJAAAAMAuCElWqRSSighJAAAAgG0QkqwSCEkqZHc7AAAAwEYISVYJrEkyilTExg0AAACAbRCSrBLjD0msSQIAAADshZBklYrpduxuBwAAANgLIckqFSEpifskAQAAALZCSLJKcOOGIh0pKbO4GAAAAAABhCSrVGzcEGH4VF6Sb3ExAAAAAAIISVaJcMnjjJUkOY8etLgYAAAAAAGEJAt5XMmSJGfJYYsrAQAAABBASLJQebR/yl1kaZ61hQAAAAAIIiRZyKy4V5LLw0gSAAAAYBeEJCtV7HAX68mztg4AAAAAQYQkCzni/CNJsV52twMAAADsgpBkoYj4VpKkBF+hyr0+i6sBAAAAIBGSLBWZ0FqSlGIUqrjMa3E1AAAAACRCkqUCI0nJRqGKS8strgYAAACAREiyVsXGDSkiJAEAAAB2QUiyUqx/44Zko1BFhCQAAADAFghJVqoYSUpSkYpLPBYXAwAAAEAiJFmr4mayTsNUadEhi4sBAAAAIBGSrBURpSNGrCTJW3TA4mIAAAAASIQkyxU7EyVJPkISAAAAYAuEJIsdjfSHJPMI0+0AAAAAOyAkWawkMkmSZBwlJAEAAAB2QEiyWFlUsiTJUUJIAgAAAOyAkGSxcpd/h7tIQhIAAABgC4Qki/kqtgF3efKsLQQAAACAJEKS5cyKkBRNSAIAAABsgZBkMUecPyTFlOdbXAkAAAAAiZBkOWd8K0lSvJeQBAAAANgBIcliEQmtJUkJvgKLKwEAAAAgEZIsF1URktwqknxei6sBAAAAQEiyWLQ7VZLkkCkdzbO2GAAAAACEJKvFx8aowIyVJHmKDlhcDQAAAABCksXiXBE6ZCZIkkrycy2uBgAAAAAhyWKRTofyDH9IKitkJAkAAACwmqUhaf78+RowYIDcbrfcbrcyMzP19ttvB58vKSnRtGnTlJqaqvj4eE2aNEn79++3sOLGUWi4JUmewh8srgQAAACApSGpffv2evjhh7Vu3Tp99tlnOv/88zVhwgRt2bJFkvSb3/xGb7zxhhYtWqRVq1Zp3759uvTSS60suVEUOf0hycuaJAAAAMByEVZefPz48SGPH3jgAc2fP19r1qxR+/bt9dxzz2nhwoU6//zzJUnPP/+8evfurTVr1uiMM86wouRGcSQiSfJKvuJDVpcCAAAAtHiWhqTKvF6vFi1apOLiYmVmZmrdunXyeDwaOXJk8JxevXqpY8eOWr169XFDUmlpqUpLS4OPCwr8N2n1eDzyeDyN+yZqELj+sXUcjUiUSiWz+IDlNcJejtdngOOhz6Cu6DOoK/oM6spOfaa2NVgekjZt2qTMzEyVlJQoPj5eixcvVp8+fbRhwwZFRUUpKSkp5Py0tDTl5OQc9/UeeughzZkzp8rx5cuXKzY2tqHLr5esrKyQxwdK/b+G4txv9dZbb1lREmzu2D4D1IQ+g7qiz6Cu6DOoKzv0mSNHjtTqPMtDUs+ePbVhwwbl5+fr1Vdf1ZQpU7Rq1ap6v94dd9yhmTNnBh8XFBSoQ4cOGj16tNxud0OUXG8ej0dZWVkaNWqUIiMjg8ef27dL2i+luLy64IILLKwQdnO8PgMcD30GdUWfQV3RZ1BXduozgVlmNbE8JEVFRemUU06RJA0aNEhr167Vn//8Z11xxRUqKytTXl5eyGjS/v37lZ6eftzXc7lccrlcVY5HRkZa/ksJOLYWX0yyJMlVlmebGmEvduq/aBroM6gr+gzqij6DurJDn6nt9W13nySfz6fS0lINGjRIkZGRWrlyZfC5HTt2aM+ePcrMzLSwwoZnxqRKklyePGsLAQAAAGDtSNIdd9yhcePGqWPHjiosLNTChQv13nvvadmyZUpMTNT111+vmTNnKiUlRW63WzfffLMyMzOb1c52kmTE+UNSdHmh5C2XnJYP8AEAAAAtlqV/G8/NzdU111yj7OxsJSYmasCAAVq2bJlGjRolSfrTn/4kh8OhSZMmqbS0VGPGjNHTTz9tZcmNwhGbIkkyZEoleVJcK2sLAgAAAFowS0PSc889d8Lno6Oj9dRTT+mpp54KU0XWiI1xKc+MU5JRLB05SEgCAAAALGS7NUktUVxUhA6b8f4HRw5aWwwAAADQwhGSbCDOFaHDSvA/OHLI2mIAAACAFo6QZAPxrggdMgMhiZEkAAAAwEqEJBuIczl1mJAEAAAA2AIhyQbiXRE6JEISAAAAYAeEJBuIc0Uoz2RNEgAAAGAHhCQbiKs0kuRjJAkAAACwFCHJBuKinMEtwH1FhCQAAADASoQkG4hwOlTkSJQkmYwkAQAAAJYiJNlEaVSyJMlxlDVJAAAAgJUISTZR5kqSJDnL8iVvubXFAAAAAC0YIckmfK4k+UzD/+DoYWuLAQAAAFowQpJNxEZHKV9x/gesSwIAAAAsQ0iyiThXhA6Z3FAWAAAAsBohySbioiJ0WIQkAAAAwGqEJJuIc/14ryRCEgAAAGAdQpJNxLkidDgw3Y5twAEAAADLEJJsIt4VoUPB6XaEJAAAAMAqhCSbCBlJYrodAAAAYBlCkk3EhYwkEZIAAAAAqxCSbCLe5VQeGzcAAAAAliMk2URcVOX7JLEmCQAAALAKIckm4lyV75NESAIAAACsQkiyiThXpZGk0nzJ67G2IAAAAKCFIiTZRLzLqQLFySvDf4DRJAAAAMAShCSbiHNFyCeH8s04/wFuKAsAAABYgpBkE3GuCEniXkkAAACAxQhJNhEX5Q9J3CsJAAAAsBYhySacDkMxkU5GkgAAAACLEZJsJM7lrHSvJEISAAAAYAVCko3EuSKUF5xud9jaYgAAAIAWipBkI3FRETpkxvsfMJIEAAAAWIKQZCPxrggdZuMGAAAAwFKEJBthTRIAAABgvXqFpL179+q7774LPv700081Y8YM/eUvf2mwwlqiOFcEu9sBAAAAFqtXSPrZz36md999V5KUk5OjUaNG6dNPP9Wdd96pe++9t0ELbElCptsdZeMGAAAAwAr1CkmbN2/W0KFDJUmvvPKK+vXrp48//lj/+te/tGDBgoasr0WJc0X8ON2utEAqL7O2IAAAAKAFqldI8ng8crlckqQVK1bo4osvliT16tVL2dnZDVddCxMX5VSBYuUL/FqOHrK2IAAAAKAFqldI6tu3r5555hl98MEHysrK0tixYyVJ+/btU2pqaoMW2JLEuSJkyqFip9t/gHVJAAAAQNjVKyQ98sgjevbZZzV8+HBdeeWVGjhwoCRpyZIlwWl4qLs4V4QkqchBSAIAAACsElGfRsOHD9eBAwdUUFCg5OTk4PEbb7xRsbGxDVZcSxNfEZLyjQRlSNIRptsBAAAA4VavkaSjR4+qtLQ0GJB2796tuXPnaseOHWrTpk2DFtiSBEaS8sRIEgAAAGCVeoWkCRMm6IUXXpAk5eXladiwYXr88cc1ceJEzZ8/v0ELbEniXE5JqnRDWUaSAAAAgHCrV0hav369zjnnHEnSq6++qrS0NO3evVsvvPCCnnjiiQYtsCUJTLc74Iv3H2AkCQAAAAi7eoWkI0eOKCHBP9qxfPlyXXrppXI4HDrjjDO0e/fuBi2wJQlMt/vBW7Gui5AEAAAAhF29QtIpp5yi1157TXv37tWyZcs0evRoSVJubq7cbneDFtiSxEX5Q9L+8oqRJO6TBAAAAIRdvULSPffco1tvvVWdO3fW0KFDlZmZKck/qnTaaac1aIEtSWBNEtPtAAAAAOvUawvwyy67TGeffbays7OD90iSpBEjRuiSSy5psOJamsBI0uHgxg2EJAAAACDc6hWSJCk9PV3p6en67rvvJEnt27fnRrInyeEwFBvl1CEPu9sBAAAAVqnXdDufz6d7771XiYmJ6tSpkzp16qSkpCTdd9998vl8DV1jixLnitBhs2K6XVmRVF5qbUEAAABAC1OvkaQ777xTzz33nB5++GGdddZZkqQPP/xQs2fPVklJiR544IEGLbIliXdF6EBhrEzDKcP0+keT3BlWlwUAAAC0GPUKSf/4xz/0t7/9TRdffHHw2IABA9SuXTv96le/IiSdhDiXU6Yc8kQlKar0oH9dEiEJAAAACJt6Tbc7dOiQevXqVeV4r169dOgQ62hORmzF5g2lUUn+A2zeAAAAAIRVvULSwIEDNW/evCrH582bpwEDBpx0US1ZfMUNZY9GJvkPEJIAAACAsKrXdLtHH31UF154oVasWBG8R9Lq1au1d+9evfXWWw1aYEsTVxGSjjgT/QcISQAAAEBY1Wsk6Sc/+Ym+/PJLXXLJJcrLy1NeXp4uvfRSbdmyRf/85z8busYWJb7ihrJFgZB09LCF1QAAAAAtT73vk9S2bdsqGzRs3LhRzz33nP7yl7+cdGEtVeCGsgWG23+AkSQAAAAgrOo1koTGE5hul2cEbihLSAIAAADCiZBkM4GNGw6bhCQAAADACoQkmwmMJB004/0HCEkAAABAWNVpTdKll156wufz8vJOphbIfzNZSfrBGwhJbNwAAAAAhFOdQlJiYmKNz19zzTUnVVBLF9i4Ibc8zn+AkSQAAAAgrOoUkp5//vnGqgMVAtPtsj2x/gOeYslzVIqMsbAqAAAAoOVgTZLNBDZuyC11SY6KDHvkkIUVAQAAAC0LIclmAmuSisu8UkyK/yBT7gAAAICwISTZTGAkqbisXGZsqv/gUUaSAAAAgHCxNCQ99NBDGjJkiBISEtSmTRtNnDhRO3bsCDmnpKRE06ZNU2pqquLj4zVp0iTt37/fooobX2BNks+UfIwkAQAAAGFnaUhatWqVpk2bpjVr1igrK0sej0ejR49WcXFx8Jzf/OY3euONN7Ro0SKtWrVK+/btq3Er8qYsJtIpw/D/XO5K9v/AmiQAAAAgbOq0u11DW7p0acjjBQsWqE2bNlq3bp3OPfdc5efn67nnntPChQt1/vnnS/LvsNe7d2+tWbNGZ5xxhhVlNyqHw1BspFPFZV6VRSXLJTGSBAAAAISRpSHpWPn5+ZKklBT/NLN169bJ4/Fo5MiRwXN69eqljh07avXq1dWGpNLSUpWWlgYfFxQUSJI8Ho88Hk9jll+jwPVrqiPOFaHiMq+KnQlKkOQt+kE+i2uHNWrbZ4AA+gzqij6DuqLPoK7s1GdqW4NtQpLP59OMGTN01llnqV+/fpKknJwcRUVFKSkpKeTctLQ05eTkVPs6Dz30kObMmVPl+PLlyxUbG9vgdddHVlbWiU/wOCUZ2vZ9ntIlZe/crHVvvRWO0mBTNfYZ4Bj0GdQVfQZ1RZ9BXdmhzxw5cqRW59kmJE2bNk2bN2/Whx9+eFKvc8cdd2jmzJnBxwUFBerQoYNGjx4tt9t9smWeFI/Ho6ysLI0aNUqRkZHHPe+vu9cod1+BWnUdIB1cqLZJ0Uq74IIwVgq7qG2fAQLoM6gr+gzqij6DurJTnwnMMquJLULS9OnT9eabb+r9999X+/btg8fT09NVVlamvLy8kNGk/fv3Kz09vdrXcrlccrlcVY5HRkZa/ksJqKmW+OiKbcAj/Bs3OI4eksMmtcMaduq/aBroM6gr+gzqij6DurJDn6nt9S3d3c40TU2fPl2LFy/WO++8oy5duoQ8P2jQIEVGRmrlypXBYzt27NCePXuUmZkZ7nLDJnCvpHyjYuSL3e0AAACAsLF0JGnatGlauHChXn/9dSUkJATXGSUmJiomJkaJiYm6/vrrNXPmTKWkpMjtduvmm29WZmZms9zZLiBwr6Q8xfsPsLsdAAAAEDaWhqT58+dLkoYPHx5y/Pnnn9fUqVMlSX/605/kcDg0adIklZaWasyYMXr66afDXGl4xUb5fy2HfAn+A+VHpbIjUpQ9Np4AAAAAmjNLQ5JpmjWeEx0draeeekpPPfVUGCqyh3iXU5J02OuSHJGSzyMdPURIAgAAAMLA0jVJqF5gul1RmVeKTfUfZModAAAAEBaEJBsKbNxQXFpOSAIAAADCjJBkQ3EhISnFf5Ad7gAAAICwICTZUHC6HSEJAAAACDtCkg0FNm4oLmVNEgAAABBuhCQbiqvYAry4jDVJAAAAQLgRkmwojo0bAAAAAMsQkmzox5DklWICa5IISQAAAEA4EJJsKC6wJqmsXGZg44ajbNwAAAAAhAMhyYYC90kyTanEmeA/mP+dtOsDyee1sDIAAACg+SMk2VBMpFMOQxrj+FRR/7naf/DoYekfF0lz+0lbl1hbIAAAANCMEZJsyDAMXRy1TvMj58pRnBv6ZEG29Mo1BCUAAACgkRCS7Mjn1R3GAkmSUeVJ0/9t6e+YegcAAAA0AkKSHe3+WGk6KEfVhFTBlAq+l3Z/HM6qAAAAgBaBkGRHRfsb9jwAAAAAtUZIsqP4tIY9DwAAAECtEZLsqNOZOuRsLZ95vBMMyd1O6nRmOKsCAAAAWgRCkh05nHot/WZJklll64aKx2MflhzO8NYFAAAAtACEJJva2ep83eSZoaKo1qFPuNtKl78g9bnYmsIAAACAZo6QZFPxrggt8w3VE/3/K41+wH/Q3V6asYmABAAAADQiQpJNxbkiJElFHlPqMdZ/8OhhyeBXBgAAADQm/sZtU7FR/vVGxaVe/xQ7SfIUS6UFFlYFAAAANH+EJJuKrxhJKi4tl6JipZhk/xP531tYFQAAAND8EZJsKjjdrrTcf8Ddzv+9YJ9FFQEAAAAtAyHJpoIjSWXHhqTvLKoIAAAAaBkISTYVF5xu5/UfCKxLYiQJAAAAaFSEJJuKc/k3bghOt0usGEliTRIAAADQqAhJNhWycYNUabodIQkAAABoTIQkmwpMtztS5pXPZxKSAAAAgDAhJNlUXFRE8OcjHu+PISn/e8k0LaoKAAAAaP4ISTYVHemQw/D/XFxaHnpD2ZJ86woDAAAAmjlCkk0ZhhF6r6TKN5RlhzsAAACg0RCSbKzq5g3t/d9ZlwQAAAA0GkKSjYWMJEmV7pVESAIAAAAaCyHJxqrcUJZ7JQEAAACNjpBkY/EVN5QtrjKSxJokAAAAoLEQkmwstmIb8OKyY9ckfWdRRQAAAEDzR0iysaobNzCSBAAAADQ2QpKNxVVMtysKrkmqGEnihrIAAABAoyEk2VjcsSNJCRn+79xQFgAAAGg0hCQbi486JiRFxUoxKf6fmXIHAAAANApCko1VuU+SJLkrtgHnXkkAAABAoyAk2ViVjRskbigLAAAANDJCko1VuZmsxA1lAQAAgEZGSLKx2MDNZMuqG0liTRIAAADQGAhJNlb9dDtuKAsAAAA0JkKSjcVFBTZuqDTdjpEkAAAAoFERkmys2pEkbigLAAAANCpCko3FVaxJOurxyuurCETcUBYAAABoVIQkGwvsbidV2rwh5Iay7HAHAAAANDRCko25IhyKcBiSjt28IXBDWdYlAQAAAA2NkGRjhmEoNqpiG/CQdUmBeyWxwx0AAADQ0AhJNhdf3Q1l2eEOAAAAaDSEJJuLq/ZeSYHpdqxJAgAAABoaIcnmAiGpiJAEAAAAhAUhyeaC0+3KqlmTxHQ7AAAAoMERkmwucK+kopA1SYGNG7ihLAAAANDQCEk2V/2apIqNG7ihLAAAANDgCEk2F19dSIqM4YayAAAAQCMhJNlcbFQ1GzdIrEsCAAAAGgkhyebiK9YkHam8JkmqtC6JG8oCAAAADYmQZHPBLcDLjhlJ4oayAAAAQKMgJNlctRs3SNwrCQAAAGgkhCSbq3bjBomQBAAAADQSQpLNBafbHbsmKbHSvZIAAAAANJgIqwvAiQU2bjj+SNI+/w1lDSPMlTUgn1fa/bFUtF+KT5M6nSk5nFZXBQAAgBbK0pGk999/X+PHj1fbtm1lGIZee+21kOdN09Q999yjjIwMxcTEaOTIkfrqq6+sKdYix1+TVPmGsnnhLaohbV0ize0n/eMi6T/X+7/P7ec/DgAAAFjA0pBUXFysgQMH6qmnnqr2+UcffVRPPPGEnnnmGX3yySeKi4vTmDFjVFJSEuZKrRN3vPskhdxQtonucLd1ifTKNVXrL8j2HycoAQAAwAKWhqRx48bp/vvv1yWXXFLlOdM0NXfuXN11112aMGGCBgwYoBdeeEH79u2rMuLUnEVH+qedlZb79OFXP8jrM398simvS/J5paW3SzKrebLi2NLf+c8DAAAAwsi2a5J27dqlnJwcjRw5MngsMTFRw4YN0+rVq/XTn/602nalpaUqLS0NPi4oKJAkeTweeTyexi26BoHr17aOZVv26763tgcfX/Xcp0p3u3TXBb00pm+anPEZcmiTyg/vkWnxe6srY/eHijjhCJgpFXyv8m/el9np7LDVZTd17TMAfQZ1RZ9BXdFnUFd26jO1rcG2ISknJ0eSlJaWFnI8LS0t+Fx1HnroIc2ZM6fK8eXLlys2NrZhi6ynrKysGs/ZeNDQ378MDPT9uClDTkGJpr+8Qdf18OnqI+XqImnn+lXantO6cYptJO0OrdbgWpy34YNl+n5LQaPXY3e16TNAZfQZ1BV9BnVFn0Fd2aHPHDlypFbn2TYk1dcdd9yhmTNnBh8XFBSoQ4cOGj16tNxut4WV+ZNrVlaWRo0apcjIyOOe5/WZeujx9yWVVvOsIUPS2/tjdccZZ0ir3lH3tDh1veCCxiq7URi73dLu+TWed+o5YzSwhY8k1abPAAH0GdQVfQZ1RZ9BXdmpzwRmmdXEtiEpPT1dkrR//35lZGQEj+/fv1+nnnrqcdu5XC65XK4qxyMjIy3/pQTUVMtnOw8qp6C6gORnSsrOL9W35anqLslRlC2HTd5brXU9179D33Gn3BmSu60iup7LduCyV/9F00CfQV3RZ1BX9BnUlR36TG2vb9ubyXbp0kXp6elauXJl8FhBQYE++eQTZWZmWlhZ48strN3ufbmq2N2uKW7c4HBKYx85zpMV0wvHPkxAAgAAQNhZOpJUVFSkr7/+Ovh4165d2rBhg1JSUtSxY0fNmDFD999/v7p3764uXbro7rvvVtu2bTVx4kTrig6DNgnRtTovtlUn/w9N9Yayp4yQHFGSryz0uLutPyD1udiaugAAANCiWRqSPvvsM5133nnBx4G1RFOmTNGCBQt02223qbi4WDfeeKPy8vJ09tlna+nSpYqOrl2IaKqGdklRRmK0cvJLqt0g25CUnhitAX16S6/rxxvKxiSHt9CT9fUKf0BK7OgPRKvnSRmnSj9/hxEkAAAAWMbS6XbDhw+XaZpVvhYsWCBJMgxD9957r3JyclRSUqIVK1aoR48eVpYcFk6HoVnj+0iqvK9dqFnj+8jpipViU/0HmuINZQM3i+07Qep/mf/nvD2SYdtZoAAAAGgB+NuoTY3tl6H5V52u9MTQUbOoCIfmX3W6xvar2MzC3db/vamtS/KUSF8u9f/ce4LUpo/kiJCOHpLyv7O2NgAAALRohCQbG9svQx/efr5e+vkZuuci/8iSp9yn0ztVmlbnbu//XtDEQtI370plRVJCW6ndICnCJbXp7X8ue6O1tQEAAKBFIyTZnNNhKLNbqq47u4tO65gkU9L/vsj+8YTASFJTC0mBqXa9x0uOim6YMdD/nZAEAAAACxGSmpAJA/2B6PUNldYfJbbzf29Ka5K8HmnHW/6fK+9gl3Gq/zshCQAAABYiJDUhFw5oK4chbdibp90Hi/0H3RUhqSmt49n1vn83vrjWUsdK97xiJAkAAAA2QEhqQlonuHTWKa0kSUsCo0nB6XZNaCRpW8VUu14XhW71ndZXkiEV5UiF+y0pDQAAACAkNTEXB6bcbdwn0zR/HEkq+N5/Q1m783ml7f/z/3zszWKj4qRWFVu853wR3roAAACACoSkJmZMv3RFRTj0dW6RtmUX/jiS5Dnin8Jmd3tWS8U/SNFJUudzqj4fnHK3IZxVAQAAAEGEpCbGHR2p83u2kSS9vvF7KTLmxxvKNoV7JW193f+914WSM7Lq86xLAgAAgMUISU3QhFP9o0dvbsyWz2c2nXVJPp+07Q3/z70vrv4cQhIAAAAsRkhqgs7r1Ubxrgh9n3dU6/YcrnRDWZvvcPf9Z1JhthSVIHU7r/pz0vv7v+ftkY4cCl9tAAAAQAVCUhMUHenUmL7pkqTXN3zfdEaSAlPteoyRIlzVnxOTJCV38f/M5g0AAACwACGpiQpMufvfF9nyJjSBkGSaP2793WfCic/NGOD/nk1IAgAAQPgRkpqoM7ulqlV8lA4f8WjHUbf/oJ1vKJu90T+FLjJWOmXkic9lXRIAAAAsREhqoiKcDl3YP0OS9M6+il3i7DySFBhFOmWkFBV74nMJSQAAALAQIakJu/hU/41k3/zW8B+w6w1lTfPH9Ug1TbWTpPSKkHTwa6m0sPHqAgAAAKpBSGrCTu+YpPbJMdpVlug/YNcbyuZu8wceZ5TUfXTN58e3ltztJJlSzuZGLw8AAACojJDUhBmGoYsHtlWpolToqAhKdryhbGCqXbfzpWh37dow5Q4AAAAWISQ1cRMqptzt8Sb7D9hxXdLWipB0vBvIVoeQBAAAAIsQkpq4nukJ6pWeoH2+FP8Bu91Q9uBOKXeL5IiQeo6rfbv0wDbghCQAAACEFyGpGRg/sK2yzVT/A7uNJAU2bOhyrhSbUvt2gZGkH7ZLnpKGrwsAAAA4DkJSM3DxwLbKMf0B5OiBPRZXc4xt9ZhqJ0nutlJsK8n0+keiAAAAgDAhJDUDHVJi5UrtKEk6lL3L4moqydsj7ftcMhxSr4vq1tYwWJcEAAAASxCSmomePXr6fyiw0e52gQ0bOp7p39a7rghJAAAAsAAhqZkYMqCfJCm5/IB2/VBkcTUVAlPt+tRxql0AIQkAAAAWICQ1E6kZXSRJsUapln223eJqJBVkS3s/8f/ce3z9XiMQkvZvkbyehqkLAAAAqAEhqbmIjFZplP9eSZ9t2izTNK2tZ/ub/u/th/o3YaiP5M6SK1Hylvl3uQMAAADCgJDUjEQkt5ckefO+05Z9BdYWE9j6u75T7aSKzRsC90v64uRrAgAAAGqBkNSMOBP9ISnDOKQlGy28X1LxAWn3R/6f6zvVLoB1SQAAAAgzQlJzkthOkpRuHNQbG/fJ57Noyt32/0mmzx9wkjuf3GsRkgAAABBmhKTmpGLtTyfnYWXnl2jtt4esqaO+N5CtTiAk5WySfN6Tfz0AAACgBoSk5sTtn27XJ96/Bfjr4Zxy5/NKuz6Q1i2Qdr7jP9Znwsm/buopUmSs5CmWDu48+dcDAAAAahBhdQFoQBUjSe0c/hGk1zd8r8Edk5WRFKOhXVLkdBiNc92tS6Slt0sFlUKZI0LK3Sa16n5yr+1wSun9/duJZ2+UWvc4udcDAAAAasBIUnNSsSYp6kiOHIap4lKvZi7aqCv/ukZnP/KOlm7Obvhrbl0ivXJNaECSJF+5//jWJSd/jfTADncbTv61AAAAgBoQkpqTBP9IUoT3qBLM4pCncvJLdNOL6xs2KPm8/hEknWCDiKW/O/m1RMF1SWwDDgAAgMZHSGpGvE6XDsktyb8NeGWBGDPnja3ynmjXu8Daok2v+r+fKODs/rjqCNKxVy343n/eyai8w53VN8kFAABAs8eapGbk012HlOBLUYqjQBnGQW03O4Y8b0rKzi/Rp7sOKbNbatUXqG5tkbutNPaR0JvCluRLe9ZIa5+rXWFF++v+Zipr3UtyRvmvm7f75LcVBwAAAE6AkNSM5BaWqMhMVT99W2Uk6djzqgisLTp26lxBtv/4WbdIXo+0+0P/dtymr/aFxafV/tzqRERJbfr41yRlbyQkAQAAoFEx3a4ZaZMQrWwzRZKUYRw84XkhTri2yPR/fTRXWvNUxZQ3n5TSVTp1shSdLOl4u+YZkrud1OnMur+ZY3FTWQAAAIQJI0nNyNAuKfrclSZ5q65JCmgVH6WhXVJCD9a4tqhCj7FS///zh56K7cZ/HIEyFBqyKoLT2If923ifLEISAAAAwoSRpGbE6TB01un+7bIzVP1Iksfr0/6CY6bb1XbNUP//k/pf9mNAkvxrlS5/QXJnhJ7rbus/Xnkt08lg8wYAAACECSNJzczAvn2ltVL7iMOS58fjaW6XHDKUXVCi6xas1Su/zJQ7OtL/ZG3XDB3vvD4XS70u9I9IFe33n9fpzIYZQQpI6ysZTqn4B6kwp2ooAwAAABoIIam5qRjl6eg8rJduGKbcolK1SYjW0C4pys4/qkue/ljbcwo17V/r9fepQxTpdPw4fe64U+4M//MnWlvkcEpdzmn49xMQGSO17inlbvWPJhGSAAAA0EiYbtfcVNxQ1ig/qsy2Dk04tZ0yu6XK6TDUPjlWf58yRDGRTn3w1QH9/r+bZJqmP+CMmHWcF2zgtUUng3VJAAAACANCUnMTGS3FtvL/XPB9laf7t0/UvJ+dJochLVr3nZ5852v/Ezmb/N8dxwwuNvTaopNBSAIAAEAYMN2uOXK3lY4c8E+fS+9f5ekRvdM0Z0I/3f3aZv0x60v1jszRqE+e8T95xb+kqLjGW1t0MghJAAAACANCUnOU2F7K+aLakaSAq8/opO8OHdGz73+jyBV3SY5yqftoqefYMBZaR4HAV/CdVHxAimtlbT0AAABolphu1xwFtujOP35IkqTbx/bSbV13a7hjgzxy6tvBd4WhuJPgSpBSuvl/ZjQJAAAAjYSQ1By52/m/13CDWIfPo1+WPCdJer58rCYvPqjs/KNavfOgXt/wvVbvPCivr3b3JPL6zHq1q7PAlLucLxrn9QEAANDiMd2uOQqGpO9OfN6nz8px6Gv5YltriTFZ3x88qnMfeVeeSgEnIzFas8b30dh+x99ye+nmbM15Y6uy80vq1K5eMgZKW/7LSBIAAAAaDSNJzVFiLUaSinKlVY9KkhwjZ+nys/pKUkhAkqSc/BLd9OJ6Ld2cXe3LLN2crZteXB8SkGrTrt7YvAEAAACNjJGk5qjymiTTlAyj6jkr75VKC6SMU+Ud+DPNf/S9al8qEJnuem2zMhJj5HQY8pmmTFMq95m6c/FmVTexzpT/Dktz3tiqUX3S5XRUU0N9BELSoW+kknwpOrFhXhcAAACoQEhqjipuKKvyo9LRw1JsSujz+z6XPn/R//O4R/Xpt3lVRoKOdaCoTBOe+qhOZZiSsvNL9OmuQ8rsllqntscVmyIldpTy9/jv7dT57IZ5XQAAAKAC0+2aoxPdUNY0pbd/J8mU+v+f1HGYcgtPHJAC3DERykiMVtvEaLVLilFKXGSt2q36Mle+htzIIWOA/ztT7gAAANAIGElqrhLbVX9D2c3/kfaukSJjpZFzJEltEqJr9ZLPXjU4ZERo9c6DuvKva2ps98yqb/S/Tdm6cmhHXT64g1rFu0Ke9/pMfbrrkHILS9QmIVpDu6SceHpexkBp+5uEJAAAADQKQlJz5W7nDxH5lXa4KyuWlt/t//nsmcENHoZ2SVFGYrRy8kuqXV9kSEpP9IeXympqJ0lxUU45DGnvoaN6dOkO/SnrS43tl6HJwzpqWJcULduSU/ed8SrWJZnZX2jNzoO1D1cAAABALRCSmqvq7pX04VypcJ+U1FE6c3rwsNNhaNb4PrrpxfUypJDAE4gcs8b3qRJAatPu8csH6ic92ujNL/bpxU/2aOPePL2xcZ/e2LhP6e5o5RRUneoX2Blv/lWnVx+UKkKS74cduvavq1Qi/8hUrbcd93ml3R9LRful+DSp05mSw3niNgAAAGgxWJPUXAV2uAusSTq8W/r4Cf/Pox+QImNCTh/bL0Pzrzpd6YmhU+/SE6OPH1Zq2S4myqn/G9xBr087S2/efLauHNpRMZGOagOS5A9bpqRZS7aorNxX5fmlu03lmklyyqfexp7g8VptO751iTS3n/SPi6T/XO//Pref/zgAAAAgRpKar8T2/u+BkJR1t1ReInU+R+o9vtomY/tlaFSf9LqtD6pju37tEvXQpf01sncbXf+Pz074uvsLStXzrreVGh+l1DhXxfcordyeq3lmZ53v3KC+jm/1ube7pFpsO751ifTKNdKxkwMLsv3HL39B6nPxceup89opAAAANEmEpOYqPs3/PXe7tPopaevrkuGQxj1S/X2TKjgdRr22665ru6LS8lqdZ8q//fiBojJp/4/HN0d01vnaoH7GrirnV7vtuM8rLb1dVQJSsJUhLf2d1OvCaqfeLd2cXfe1UwAAAGiSCEnN0dYl0lu3+n8uzpWW/d7/c9fzpLS+1tVVSW131Js/+XR1So3TweJSHSwq03s7cvXahn3a4usiSRrm2KaLHR8rV0n61NdLvooZpAs+2qV4V4T6tXPLMAz/GqTK67OqMP2jbrs/lrqcE/LM0s3ZuunF9VXiVY1rpyqEewSqvtfzlpdr+yfLdPTw94pJbqdew8bIGVGLPyLqu8Yr3O0AAABqiZDU3BxvSpkk7XzH//wJppSFS2131BvdN3TaXJo7Wq9t2KdUI1+S1MWxX09EzZMk7TNTNMdzjZb5hmrZ1v1atjVHZyfkamqr7TqzaKlia1PYtx+E/KXb6zM1542tMiU55NNQx3a1UV4wlJlyHH96n/wB674lm9ShaGOw3d74gbr74v41jkB5y8u1fc3bKtu9RtvXGOqTeUGNoaW+1/t82T/UdvUc9dXB4LH9WanalzlLp42ZcvwLbl0ic+ntMioFUNPdVsbYR07cz8LdrkJ9g2CTalfHPtPk3l8TaNeUaqXPNO12ltVKn2lx7U76mvXoM1YzTNNswLt82k9BQYESExOVn58vt9ttaS0ej0dvvfWWLrjgAkVG1u5GrHXi8/o3ITjuiInh39BhxiZb/Mt7YIRGqn5nvOpGaLw+U3c++KAe9DwqQ6EzBwP3q/2LcZl6JHrVI+9DtTd+qHNdpa5W+i7tPH2ZPFwrSnroPxt/0BjHp5oV+YLaGoeC51UOZbPG99H5vdqoTUK0YqKcwff32sJndE817e71XKOJP/vlcYNLILSkVQ4tOnFoqe/1Pl/2Dw38+NeSJEc1n+fGM5+o/ppbl8h85RqZMkN2gPFJMmTION4ar3C3q/Q+6/qZ0o52dW3XlGqlXdNu15RqpV3TbmfVNRtLbbMBISmMGj0k7frAv1tbTaa8WWVKmVXqvNbH59XRx/rIdSRH1c0gM83Q4OR1ROnL2NP1n6I++rn5X7VW3nHbHVWUyuWU2zgaPF5gxmqbr6OGOrb7R5OqCRE3eWZomW9o8HhCdITaJLjU+/AqPeH8o3ScdndE/D/deevtSoiO8E8JrFCf0FI5PB6v3e8jb9MDv/99yIiXt7xcB+7vodbmwWo/F58p5Rqpan3Xl6H/6lPD78FnSqWx6Yr5f1tDA3m421WobxCkHe3q9A8HTahW2jXtdk2pVto17XZWXbMxEZIqtKiQtOlV/7bWNZn0nNT/soa/fj3VaQ1NbYPgKaOkwddKXYdLUXFa/Pn3WrroL5ofOVfS8cPO5rgzNCbua53jXaNBRz6W23uoyktXZppSoWL1D+elKio3VOo15JVDpqRbIxYpUcXV7pNhmlKe4vSAZ7IcDoeiIx2KjXQqJsLUlOLnT9guX3H6oMstkuGs2C7dUF5xqS7eP09JJ2hXqFi9l3SpklxShFkmp69MscXfq3/J2ho/zu9d3eSLTvZ/boZDLk+BWhVtr7FdUUI3eSNiZMiUYXoV4SlSTPF3NbY7ktRD5dGpkjNCpiNKjrICJeyvuc7iflfJbN1TzgiXjIhIOSKiZBhOFS+5VYlm0fE/UyNBCZP+LKdM/4is6ZWvvExFb/5eCebxP9MCI14JE/8ghzNSckRIDqe8PkOFi36pRLOw2nY+U8ozEpV49T/ldP4YPL3echX88yolmQUnbnfVC3I6naHtXrymhnbuinYRwX9F8JaXq+DFq2u+Xn3rvPrFSsHakNdbrvwXfqbkWl/PrKjTU7v3N/kfIUHeW16ugn9NOWG7fMMt98+er/g8zR/bvXSdkk7w+8s33HL/9K9Vpot4vT4VvHR9zdecvEDOiMD/Awx5vd5a/i5erPq7r/F34Vbi5AU/tjNN//UWXlu7zyYiQoHxfa/XW+NnGuyjx/4uavP+aFerdk2pVto17XaNfc1q/xE2DAhJFVpUSGqCI0l1Vs8guHrnQV351zXHmTaXqjmeq7XMN1Qv/fyMH3fF8/nk/fSvci69raHfBQAAQIu3ZdRC9T3rwrBes7bZwP6rpiQ99dRTeuyxx5STk6OBAwfqySef1NChQ2tu2NJ0OtO/5qggW9VvdV2xJqnTmeGurOEEtjav43mBjSKW5w9VVungajdgyEj0j2IFORxyxtVyW/MOmVJS++AoRGHON0o4tKnGZoVJvRSV1E4enymvz9TRg98r/eiXNbbbE9lVntg2/nVZpk9RJT+oXdmuGtvtiDtdRe6e8jqi5HVESYXZysx7s8Z2SxKvUlHiKfL5TPl8plyHv9QVR1+usd1fIydrf8wp8sohrxxqc+RrTS//R43tnjEu1/7I9nKqXBG+crUv/1ZX6a0a263y9leeEhSpckXKq0iVK904qF6Omkevvva1Va6ZJK8c8smhVka++jp219huu6+9DptuOQyfIuRVqvLV2ZFbY7v9viQV6cebOsfrqNIceTW2y/Ylq6jSNiTxOqIMx+E6t0tQsdJrcb361lnfdjm+pGCdpoxav799vhQVKlZGxZ99CbVs970vRQWKU2C0JEFH1N5xoMZ2e32tlK/44GNDptwqVodatK1cq6Ha/w5Pps8UKK7iav7ffTvHiUfJJf9nU6i44GfqVnG9+lp9+yjtqm/XlGqlXdNuF45rHj38fY3nWMX2Ienf//63Zs6cqWeeeUbDhg3T3LlzNWbMGO3YsUNt2rSxujx7cTilsY9U7G5nqNrtEMY+bItNG+qtnkHQ6TA0a3yfiq28HVrj61O5hSRp1vg+Vaf51TaUnX9nyOhc7DfvSy9Uf9PeymIvfkzOrufKVfH4u4/+p/Ssn9XYrnD4/SH/8uKt5fVOmTRHzq7n/tiuvFz7a7Em6cKb/xwyHL76q1zte3G50nXouO1ylKp+l8/Wz7u3OabdGzW2Gzj5AWVWabemxnZR1/xX47u1ltf0h06P16cda96S3rumxs/m+zPvV8Zpo2WakilT+zZkqe+a62pulzlH6QNHyajoSd9szFLnT2putyXzj0obODL4+NuNWUr7pOZR0s3D/qA2A35st+uLFcpYW7t2aQNH+je5MKTdG7OU/mnN7Tad8bjSB46S5J9iuGtjltJqcb2Nwx4PqXPnFys0uhbtNh3T7ttav7/H1GbASAUmR3y7aYUy1t5Q8/WGPqJW/fzXMyXt3rRC7df9vMZ2W4Y+pNb9R8kwVLGJjKHtX6xQh1rV+qha9R8pf6mmdm5aoYzPar7musGPKbXfiODjHZtX6oL1NbfbOMR/vcCUl283rVC7Wlxv05BHlNq/4rMxpW82r1BGLT6bzwf7rxews5btNg55NHg9Sfp6U+2vl9pvRPD/CF9uXqkLa/G5rK9oFxDOdoakrzavVEYt2h37/up6zeS+5/sfmNKXW97RhZ/Xvl3gz7UvtzTuZ/P5Me3q+tmEq936wY8qpW+l91fPz6W21wv8tyv5Z0rX9s+KDUN+rNNUHfrLoEr9peLPiy+3vFPn36FhGPq6lv/dxyS3q/Ecq9h+ut2wYcM0ZMgQzZvn3+bZ5/OpQ4cOuvnmm/W73/2uxvYtarpdwNYl/hunVt7lzt3OH5BssP33SQtucy5VGwRPsMtZfTaK8O8YWEMoO3bHwHpuNBD2jRQU5o0iwtxOqv9nSjva1em/wSZUK+2adrumVCvtmnY7q67Z2GqbDRzHfcYGysrKtG7dOo0c+eO/LDkcDo0cOVKrV6+utk1paakKCgpCviR/QLHDV1hq6T5Onmmfq/yq11Q+8VmVX/WaPNPW+4/b4DNoiPdXPul5mQmhocZ0t1X5pOdP+D5H9Gyld2eeoxevG6w//l9/vXjdYL078xyN6Nmq+jZen8pHPRjcICHkehWTUMpHPSCP11elXeQFD8swDPmO6aM++f+VJfKCh6u085mm9g6923/eMZks8Pi7oXfLZ5oNcj2Px6N+5/9M64fN1Q9G6NTCXCNV64fNVb/zf1aljc9brrMvmqJfeWYoRykh7XKUql95Zujsi6bI5y23tN3JfKa0o12d/htsQrXSrmm3a0q10q5pt7PqmuH4qg1bjyTt27dP7dq108cff6zMzMzg8dtuu02rVq3SJ598UqXN7NmzNWfOnCrHFy5cqNjYWt1OFE2F6VNq0Q5Fe/JUEpmkg/E9JaNxcn9G3lr1/+5fivH8OI//SGSKNrefrOykISds1++7fym2ju1K936mc394UemVNpjINlP0Qeur5OowuMGvJ/lHactyv5SjNE8+V5Ki2vSQw3Hiz3PjQUOvfSv1KN8RXOP1ZURPTewsDUw9/h8t4W4n1f8zpR3t6tKuKdVKu6bdrinVSrum3c6qazaWI0eO6Gc/+1nT3t2uPiGptLRUpaWlwccFBQXq0KGDDhw4YIvpdllZWRo1alTjTrdD4/B5ZexdLRXtl+LTZHbIrN36rnq289/Zeqm++uJTdR8wVL2Gja3dkHR966wnr8/UZ7sPK7ewVG0SXBrcKfn4W7hb2E7yf6ZffZalo4f3KSa5rboPHlXru4U3lXb16TNN6f01hXZNqVb6TNNuZ1Wt9JmW1+5kr1mvv880koKCArVq1apph6SysjLFxsbq1Vdf1cSJE4PHp0yZory8PL3++us1vkaLXJOEZoM+g7qiz6Cu6DOoK/oM6spOfaZZrEmKiorSoEGDtHLlyuAxn8+nlStXhowsAQAAAEBDsf0W4DNnztSUKVM0ePBgDR06VHPnzlVxcbGuvfZaq0sDAAAA0AzZPiRdccUV+uGHH3TPPfcoJydHp556qpYuXaq0tFrevwYAAAAA6sD2IUmSpk+frunTp1tdBgAAAIAWwNZrkgAAAAAg3AhJAAAAAFAJIQkAAAAAKiEkAQAAAEAlhCQAAAAAqISQBAAAAACVNIktwE+GaZqSpIKCAosrkTwej44cOaKCggJFRkZaXQ6aAPoM6oo+g7qiz6Cu6DOoKzv1mUAmCGSE42n2IamwsFCS1KFDB4srAQAAAGAHhYWFSkxMPO7zhllTjGrifD6f9u3bp4SEBBmGYWktBQUF6tChg/bu3Su3221pLWga6DOoK/oM6oo+g7qiz6Cu7NRnTNNUYWGh2rZtK4fj+CuPmv1IksPhUPv27a0uI4Tb7ba8g6Bpoc+grugzqCv6DOqKPoO6skufOdEIUgAbNwAAAABAJYQkAAAAAKiEkBRGLpdLs2bNksvlsroUNBH0GdQVfQZ1RZ9BXdFnUFdNsc80+40bAAAAAKAuGEkCAAAAgEoISQAAAABQCSEJAAAAACohJAEAAABAJYSkMHrqqafUuXNnRUdHa9iwYfr000+tLgk28f7772v8+PFq27atDMPQa6+9FvK8aZq65557lJGRoZiYGI0cOVJfffWVNcXCcg899JCGDBmihIQEtWnTRhMnTtSOHTtCzikpKdG0adOUmpqq+Ph4TZo0Sfv377eoYlht/vz5GjBgQPBGjpmZmXr77beDz9NfUJOHH35YhmFoxowZwWP0G1Q2e/ZsGYYR8tWrV6/g802tvxCSwuTf//63Zs6cqVmzZmn9+vUaOHCgxowZo9zcXKtLgw0UFxdr4MCBeuqpp6p9/tFHH9UTTzyhZ555Rp988oni4uI0ZswYlZSUhLlS2MGqVas0bdo0rVmzRllZWfJ4PBo9erSKi4uD5/zmN7/RG2+8oUWLFmnVqlXat2+fLr30UgurhpXat2+vhx9+WOvWrdNnn32m888/XxMmTNCWLVsk0V9wYmvXrtWzzz6rAQMGhByn3+BYffv2VXZ2dvDrww8/DD7X5PqLibAYOnSoOW3atOBjr9drtm3b1nzooYcsrAp2JMlcvHhx8LHP5zPT09PNxx57LHgsLy/PdLlc5ksvvWRBhbCb3NxcU5K5atUq0zT9/SMyMtJctGhR8Jxt27aZkszVq1dbVSZsJjk52fzb3/5Gf8EJFRYWmt27dzezsrLMn/zkJ+Ytt9ximiZ/zqCqWbNmmQMHDqz2uabYXxhJCoOysjKtW7dOI0eODB5zOBwaOXKkVq9ebWFlaAp27dqlnJyckP6TmJioYcOG0X8gScrPz5ckpaSkSJLWrVsnj8cT0md69eqljh070mcgr9erl19+WcXFxcrMzKS/4ISmTZumCy+8MKR/SPw5g+p99dVXatu2rbp27arJkydrz549kppmf4mwuoCW4MCBA/J6vUpLSws5npaWpu3bt1tUFZqKnJwcSaq2/wSeQ8vl8/k0Y8YMnXXWWerXr58kf5+JiopSUlJSyLn0mZZt06ZNyszMVElJieLj47V48WL16dNHGzZsoL+gWi+//LLWr1+vtWvXVnmOP2dwrGHDhmnBggXq2bOnsrOzNWfOHJ1zzjnavHlzk+wvhCQAaMKmTZumzZs3h8z7BqrTs2dPbdiwQfn5+Xr11Vc1ZcoUrVq1yuqyYFN79+7VLbfcoqysLEVHR1tdDpqAcePGBX8eMGCAhg0bpk6dOumVV15RTEyMhZXVD9PtwqBVq1ZyOp1VdvDYv3+/0tPTLaoKTUWgj9B/cKzp06frzTff1Lvvvqv27dsHj6enp6usrEx5eXkh59NnWraoqCidcsopGjRokB566CENHDhQf/7zn+kvqNa6deuUm5ur008/XREREYqIiNCqVav0xBNPKCIiQmlpafQbnFBSUpJ69Oihr7/+ukn+OUNICoOoqCgNGjRIK1euDB7z+XxauXKlMjMzLawMTUGXLl2Unp4e0n8KCgr0ySef0H9aKNM0NX36dC1evFjvvPOOunTpEvL8oEGDFBkZGdJnduzYoT179tBnEOTz+VRaWkp/QbVGjBihTZs2acOGDcGvwYMHa/LkycGf6Tc4kaKiIu3cuVMZGRlN8s8ZptuFycyZMzVlyhQNHjxYQ4cO1dy5c1VcXKxrr73W6tJgA0VFRfr666+Dj3ft2qUNGzYoJSVFHTt21IwZM3T//fere/fu6tKli+6++261bdtWEydOtK5oWGbatGlauHChXn/9dSUkJATncycmJiomJkaJiYm6/vrrNXPmTKWkpMjtduvmm29WZmamzjjjDIurhxXuuOMOjRs3Th07dlRhYaEWLlyo9957T8uWLaO/oFoJCQnBdY4BcXFxSk1NDR6n36CyW2+9VePHj1enTp20b98+zZo1S06nU1deeWXT/HPG6u31WpInn3zS7NixoxkVFWUOHTrUXLNmjdUlwSbeffddU1KVrylTppim6d8G/O677zbT0tJMl8tljhgxwtyxY4e1RcMy1fUVSebzzz8fPOfo0aPmr371KzM5OdmMjY01L7nkEjM7O9u6omGp6667zuzUqZMZFRVltm7d2hwxYoS5fPny4PP0F9RG5S3ATZN+g1BXXHGFmZGRYUZFRZnt2rUzr7jiCvPrr78OPt/U+othmqZpUT4DAAAAANthTRIAAAAAVEJIAgAAAIBKCEkAAAAAUAkhCQAAAAAqISQBAAAAQCWEJAAAAACohJAEAAAAAJUQkgAAAACgEkISAAAnYBiGXnvtNavLAACEESEJAGBbU6dOlWEYVb7Gjh1rdWkAgGYswuoCAAA4kbFjx+r5558POeZyuSyqBgDQEjCSBACwNZfLpfT09JCv5ORkSf6pcPPnz9e4ceMUExOjrl276tVXXw1pv2nTJp1//vmKiYlRamqqbrzxRhUVFYWc8/e//119+/aVy+VSRkaGpk+fHvL8gQMHdMkllyg2Nlbdu3fXkiVLGvdNAwAsRUgCADRpd999tyZNmqSNGzdq8uTJ+ulPf6pt27ZJkoqLizVmzBglJydr7dq1WrRokVasWBESgubPn69p06bpxhtv1KZNm7RkyRKdcsopIdeYM2eOLr/8cn3xxRe64IILNHnyZB06dCis7xMAED6GaZqm1UUAAFCdqVOn6sUXX1R0dHTI8d///vf6/e9/L8Mw9Mtf/lLz588PPnfGGWfo9NNP19NPP62//vWvuv3227V3717FxcVJkt566y2NHz9e+/btU1pamtq1a6drr71W999/f7U1GIahu+66S/fdd58kf/CKj4/X22+/zdooAGimWJMEALC18847LyQESVJKSkrw58zMzJDnMjMztWHDBknStm3bNHDgwGBAkqSzzjpLPp9PO3bskGEY2rdvn0aMGHHCGgYMGBD8OS4uTm63W7m5ufV9SwAAmyMkAQBsLS4ursr0t4YSExNTq/MiIyNDHhuGIZ/P1xglAQBsgDVJAIAmbc2aNVUe9+7dW5LUu3dvbdy4UcXFxcHnP/roIzkcDvXs2VMJCQnq3LmzVq5cGdaaAQD2xkgSAMDWSktLlZOTE3IsIiJCrVq1kiQtWrRIgwcP1tlnn61//etf+vTTT/Xcc89JkiZPnqxZs2ZpypQpmj17tn744QfdfPPNuvrqq5WWliZJmj17tn75y1+qTZs2GjdunAoLC/XRRx/p5ptvDu8bBQDYBiEJAGBrS5cuVUZGRsixnj17avv27ZL8O8+9/PLL+tWvfqWMjAy99NJL6tOnjyQpNjZWy5Yt0y233KIhQ4YoNjZWkyZN0h//+Mfga02ZMkUlJSX605/+pFtvvVWtWrXSZZddFr43CACwHXa3AwA0WYZhaPHixZo4caLVpQAAmhHWJAEAAABAJYQkAAAAAKiENUkAgCaLGeMAgMbASBIAAAAAVEJIAgAAAIBKCEkAAAAAUAkhCQAAAAAqISQBAAAAQCWEJAAAAACohJAEAAAAAJUQkgAAAACgkv8PuCaAZH0CGBEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_results(train_losses, val_losses, train_accuracies, val_accuracies, outdir='plots'):\n",
    "    \"\"\"\n",
    "    Plot training and validation loss and accuracy.\n",
    "    \n",
    "    Parameters:\n",
    "    - train_losses: List of training losses per epoch.\n",
    "    - val_losses: List of validation losses per epoch.\n",
    "    - train_accuracies: List of training accuracies per epoch.\n",
    "    - val_accuracies: List of validation accuracies per epoch.\n",
    "    - outdir: Directory to save the plots.\n",
    "    \"\"\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "\n",
    "    # Plot Loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_losses, label='Train Loss', marker='o')\n",
    "    plt.plot(val_losses, label='Validation Loss', marker='o')\n",
    "    plt.title('Loss per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(outdir, 'loss_plot.png'))\n",
    "\n",
    "    # Plot Accuracy\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(train_accuracies, label='Train Accuracy', marker='o')\n",
    "    plt.plot(val_accuracies, label='Validation Accuracy', marker='o')\n",
    "    plt.title('Accuracy per Epoch')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(os.path.join(outdir, 'accuracy_plot.png'))\n",
    "\n",
    "plot_results(train_losses, val_losses, train_accuracies, val_accuracies, outdir=os.path.join(PROJECT_PATH, 'plots'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "total_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_seq, X_nonseq, y in test_loader:\n",
    "        X_seq = X_seq.to(device)\n",
    "        X_nonseq = X_nonseq.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        outputs = model(X_seq, X_nonseq)\n",
    "        loss = criterion(outputs, y)\n",
    "\n",
    "        total_loss += loss.item() * X_seq.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0)]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_preds.sort()\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rsi-divergence-detector-MNiR8ro_-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
